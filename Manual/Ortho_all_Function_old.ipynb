{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `        -------------------->     Ortho Prediction Software Developing by krish:     <----------------------------`\n",
    "                   Check all Function Working better or Not\n",
    "                   main / Global Variable use in this program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os                  # For system operation\n",
    "import glob                #list the file unix system\n",
    "import subprocess          #to run blastp from OS\n",
    "import time                # to display time\n",
    "from alive_progress import alive_bar #For Progress Bar\n",
    "import multiprocessing  #For Parallel Processing\n",
    "import queue            # For Multiprocessing\n",
    "import numpy.matlib     # Matrix operation \n",
    "import numpy as np      # For Mathmatical (Algebra) Operation\n",
    "import copy             # To copy file\n",
    "import operator   #to concate the list we will remove this later \n",
    "import argparse\n",
    "import datetime         #to display current time & calculate difference\n",
    "from Bio import SeqIO  # For Bio Python Sequence Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Blastp = /usr/bin/blastp\n",
    "Blastp = \"blastp\"\n",
    "Blastp_data =  './blastp_data/'\n",
    "cpu_count = 1 # Default value\n",
    "blastp_matrix = \"BLOSUM62\" \n",
    "genome = \"\"\n",
    "infinite_loop = 70\n",
    "inflation_factor = 1.4\n",
    "#mode = None\n",
    "mode = None\n",
    "Cluster_out = \"./cluster_out/\"\n",
    "#threshold_score : 0\n",
    "threshold_score =0      #5 is given in Remark Server / \n",
    "Save_raw_blastp_score = False\n",
    "Score_file = \"./score_file\"\n",
    "Species = \"./species/\"\n",
    "species = \"./species/\"\n",
    "verbose = False\n",
    "Log_file_name = \"krish_ortho_jp\"\n",
    "\n",
    "queryV = None # Only for test Purpose # Always Preferred AAE \n",
    "#queryV is created by One way Thr..... Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1. GetMatrixNumber()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Following matrix files must be in Folder\n",
    "import os\n",
    "def GetMatrixNumber():\n",
    "    \"This Function will Return the Matrix name choosed by User\"\n",
    "    import sys\n",
    "    print (\"\\n1. BLOSUM45\\n2. BLOSUM62\\n3. BLOSUM82\\n4. Quit\")\n",
    "\n",
    "    metrix_num = input(\"\\nEnter a matrix number: \")\n",
    "    if metrix_num not in (\"1\",\"2\",'3'):\n",
    "        print(\"Wrong input *%s*  Sorry not in the list\\n\"%metrix_num,\"*\"*20,\"Good Bye\",\"*\"*20,\"\\n\")\n",
    "        sys.exit(1)\n",
    "    if metrix_num ==\"1\":\n",
    "        return \"BLOSUM45\"\n",
    "    if metrix_num == \"2\":\n",
    "        return \"BLOSUM62\"\n",
    "    if metrix_num == \"3\":\n",
    "        return \"BLOSUM82\"\n",
    "#GetMatrixNumber()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2. GetQuerySequence(genome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This Function is Developed By Krishna\n",
    "def get_query_seq(genome):\n",
    "    try:\n",
    "        return [str((seq_record.id+seq_record.seq)) for seq_record in SeqIO.parse(genome,\"fasta\")]\n",
    "    except IOError as err:\n",
    "        print(str(err))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is Old Function\n",
    "def GetQuerySequence(genome):\n",
    "    \"\"\"This Function Read Fastaq Files and return as a list Format with gene Position as a index\n",
    "    Later I will reduce code Lengths and Speed\"\"\"\n",
    "    global species\n",
    "    gene_sequence=\"\"\n",
    "    gene_sequence_list= []\n",
    "    try:\n",
    "        with open(species+genome) as gene:\n",
    "            for each_line in gene:\n",
    "                if \">\" in each_line:\n",
    "                    if gene_sequence != \"\":                        \n",
    "                        gene_sequence_list.append(gene_sequence)\n",
    "                        gene_sequence = \"\"\n",
    "                gene_sequence = gene_sequence+each_line\n",
    "            gene_sequence_list.append(gene_sequence)                   \n",
    "        return gene_sequence_list  \n",
    "    except IOError as err:\n",
    "        print (\"IOError occurred in GetQuerySequence function : \" + str(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3.Write Query(query, parallel_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WriteQuery(query, parallel_num):\n",
    "    # This will write query as a sequence and parallel_num is for file name like query_parallel_num\n",
    "    try:\n",
    "        with open(\"./query\"+\"_\"+str(parallel_num), \"w\") as write_query:\n",
    "            write_query.write(query)\n",
    "    except IOError as err:\n",
    "        print (\"IOError occurred in WriteQuery function : \" + str(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 4. RunBlast( Subject , parallel_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "def RunBlast(subject, parallel_num):\n",
    "    \"\"\"By this Function it will create a Pipe line to run Blastp in Computer by input Parameter\n",
    "     subject is a small size gene passed not a whole\n",
    "     parallel_nu is the CPU selected by user\"\"\"\n",
    "    subject = Species+subject \n",
    "    Blastp = \"blastp\"\n",
    "    cmd = [Blastp, \"-query\",\"./query\"+\"_\"+str(parallel_num), \"-subject\", subject,\"-matrix\", blastp_matrix,\n",
    "           \"-outfmt\", \"10 qseqid sseqid score length\"]\n",
    "    \n",
    "    run_blastp =subprocess.Popen(cmd,stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    #qseqid = query (e.g. gene sequence id) , sseqid = subject(e.g. reference genome)\n",
    "    #sequence id score = Rawscore length = Alignment length\n",
    "    \n",
    "    run_blastp_stream = run_blastp.communicate()\n",
    "    run_blastp_output_stream = run_blastp_stream[0]\n",
    "    #run_blastp_error_stream = run_blastp_stream[1]  \n",
    "    \n",
    "    #display(run_blastp)\n",
    "    #display(run_blastp_error_stream)\n",
    "    return run_blastp_output_stream\n",
    "#RunBlast(\"AAE\",\"AAE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run Manually \n",
    "#!blastp -query query_1 -subject \"AAE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 5. Get_Same_Species_Forward_Best_Hit(blastp_score):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Same_Species_Forward_Best_Hit(blastp_score):\n",
    "    \"\"\" If the User specified the Same input like 1(\"AAE\") & 1 ('AAE') i.e if species_of_query == species_of_subject\n",
    "    It will Save Best Score without reversing RunBlast\n",
    "    Search the forward best hit among the blastp scores of same species.\n",
    "    Because there are an duplicated genes in a same genome.\n",
    "    When the blastp score compare with blastp score of duplicate gene, if score and length are same, blastp score of duplicated\n",
    "    gene i sadded to a second best score.\"\"\"\n",
    "    #blastp_score is Output File by doing blast\n",
    "    blastp_score_split_list  = []\n",
    "    temp_best_score = [\"-1\",'-1','-1']\n",
    "    second_temp_best_score = []\n",
    "    best_score = []\n",
    "    second_best_score = []\n",
    "    blastp_score_split = blastp_score.split('\\n')\n",
    "    del blastp_score_split[-1] #delete of [''] in last index \n",
    "    for i in blastp_score_split:\n",
    "        blastp_score_element = i.split(\",\")\n",
    "        blastp_score_split_list.append(blastp_score_element)\n",
    "    for k in blastp_score_split_list: #ex) k in ['gi|15605613|ref|NP_212986.1|', 'gi|15605613|ref|NP_212986.1|', '3702', '699']\n",
    "        if k[0] == k[1]:\n",
    "            best_score.append(k)\n",
    "        elif k[0] !=k[1]:\n",
    "            print(k[2], temp_best_score[2])\n",
    "            if int(k[2]) >int(temp_best_score[2]): #Compare the Score , Greater than\n",
    "                temp_best_score = k\n",
    "            elif int(k[2]) == int(temp_best_score[2]): # Is equal\n",
    "                if int(k[3])> int(temp_best_score[3]) : #Compare the length\n",
    "                    temp_best_score = k\n",
    "                elif int(k[3]) == int(temp_best_score[3]):\n",
    "                    second_temp_best_score.append(k)\n",
    "        #print( \"################ temp Best Score #####################\", temp_best_score)\n",
    "        second_best_score.append(temp_best_score)\n",
    "        for j in second_temp_best_score:\n",
    "            if j[2] == temp_best_score[2] and j[3] == temp_best_score[3]:\n",
    "                second_best_score.append(j)\n",
    "        for m in second_best_score:\n",
    "            if (best_score[0][2] == m[2] and int(best_score[0][3])<= int(m[3])) or int(best_score[0][2])< int(m[2]): \n",
    "                #'104'<\"23\" is True because of string. So the int Function is Used.\n",
    "                best_score.append(m)\n",
    "    return best_score     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 6. GetForwardBestHit(blastp_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetForwardBestHit(blastp_score):\n",
    "    \"Search the forward best hit among blastp scores of same species\"\n",
    "    #print(\"Running Get Forward Best Hit\")\n",
    "    blastp_score_split_list = []\n",
    "    temp_best_score = ['-1','-1','-1']\n",
    "    second_temp_best_score = []\n",
    "    best_score = []\n",
    "    blastp_score_split = blastp_score.split(\"\\n\")\n",
    "    del blastp_score_split[-1] #Remove last index of \"\\n\"\n",
    "    for i in blastp_score_split:\n",
    "        blastp_score_element = i.split(\",\")\n",
    "        blastp_score_split_list.append(blastp_score_element)\n",
    "    for k in blastp_score_split_list:\n",
    "        # ex) k is [\"gi|15605613|ref|NP_212986.1|\",'gi|15605613|ref|NP_212986.1|', '3702', '699']\n",
    "        if int(k[2])>int(temp_best_score[2]): # Compare Score\n",
    "            temp_best_score = k\n",
    "        elif int(k[2]) == int(temp_best_score[2]):\n",
    "            if int(k[3])> int(temp_best_score[3]): #Comapare the Length\n",
    "                temp_best_score = k\n",
    "            elif int(k[3]) == int(temp_best_score[3]):\n",
    "                second_temp_best_score.append(k)\n",
    "    #print(\"## temp best Score -->\",temp_best_score)\n",
    "    best_score.append(temp_best_score)\n",
    "    for j in second_temp_best_score:\n",
    "        if j[2] == temp_best_score[2] and j[3] == temp_best_score[3]:\n",
    "            best_score.append(j)\n",
    "    return best_score , blastp_score_split_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 7. DivisionParallelQuery(queryV,query_division_value, cpu_count , queryV_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DivisionParallelQuery( queryV ,query_division_value,cpu_count,queryV_len):\n",
    "    \"\"\" Running Division parallel Query,input is species_of_query , species_of_subject, queryV, parallel_num\"\"\"\n",
    "    parallel_query = []\n",
    "    parallel_query_start = 0\n",
    "    if queryV_len % cpu_count == 0: #Perfect division by CPU Count\n",
    "        for i in range(cpu_count):\n",
    "            i+=1\n",
    "            if parallel_query_start ==0:\n",
    "                parallel_query.append(queryV[int(parallel_query_start):i*int(query_division_value)])\n",
    "                parallel_query_start +=1\n",
    "            else:\n",
    "                parallel_query.append(queryV[int(parallel_query_start)*int(query_division_value):i*int(query_division_value)])\n",
    "                parallel_query_start +=1\n",
    "    else: # If the division is imperfect\n",
    "        for i in range(cpu_count):\n",
    "            i += 1\n",
    "            if parallel_query_start == 0:\n",
    "                parallel_query.append(queryV[int(parallel_query_start):i*int(query_division_value)])\n",
    "                parallel_query_start +=1\n",
    "            elif i<cpu_count:\n",
    "                parallel_query.append(queryV[int(parallel_query_start)*int(query_division_value):])\n",
    "                parallel_query_start +=1\n",
    "        return parallel_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 8. Run Parallel Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RunParallelQuery(species_of_query, species_of_subject, queryV, parallel_num):\n",
    "    #from progress import bar\n",
    "    #i --> Species_of_query , k --> Species_of_subject , queryV is Genome list whole file (i , k are part of this)\n",
    "    \"\"\"This Function helps to run operation on Several Process\n",
    "    Run the Following Functions. WriteQuery , RunBlast , Get_Same_Species_Forward_Best_Hit, GetForwardBestHit\"\"\"\n",
    "    #print(\"Parallel Query Running\") \n",
    "    #bar = Bar(\"processing\"+str(cpu_count),max = len(queryV))\n",
    "    #bar = bar(\"processing\"+str(cpu_count),max = len(queryV))  #python 3\n",
    "    for j in queryV:\n",
    "        #bar.next()\n",
    "        WriteQuery(j, cpu_count)\n",
    "        #WriteQuery() Function Only write file of 1 Gene inside \n",
    "        #This Function write the File in same directory with name query_ +(cpu_count). This will later use in blast Purpose\n",
    "        \n",
    "        blastp_score= RunBlast(selected_species_dic[species_of_subject],parallel_num)\n",
    "        #In this Function 2 Parameter slected_species_dic_of_subject[spe..] is whole Genome and\n",
    "        # parallel_num is for query_\"file\" like query_1 if cpu-count i.e parallel_num is 1\n",
    "        \n",
    "        if blastp_score != \"\":\n",
    "            best_score, blastp_score_split_list = GetForwardBestHit(blastp_score.decode())\n",
    "            #This Function will return the best score from blast Seq and blastp_score_split_list\n",
    "            #decode() will change to str Format\n",
    "            if species_of_query == species_of_subject:\n",
    "                # if Same Input Passed 1 1 , We are testing the Code in this Mode\n",
    "                # e.g. AAE == AAE It will save best_score without reversing RunBlast.\n",
    "                same_species_forward_best_score = Get_Same_Species_Forward_Best_Hit(blastp_score.decode())\n",
    "                \n",
    "                for best_score_element in same_species_forward_best_score:\n",
    "                    if best_score_element[0] == best_score_element[1]: #ex [A1 of AAE, A! of AAE , 30 ]\n",
    "                        with open(Score_file+selected_species_dic[species_of_query]+\"_\" + selected_species_dic[species_of_subject]+\"_oneway_threshold_best_hit_S\"+str(threshold_score),\"a\") as oneway_threshold_best_hit:\n",
    "                            save_best_score= selected_species_dic[species_of_query]+\"_\"+best_score_element[0].split(\"\\s\")[0]+\" \"+selected_species_dic[species_of_subject]+ \"_\"+best_score_element[1].split(\"\\s\")[0]+\" \"+best_score_element[2]+\"\\n\"\n",
    "                            oneway_threshold_best_hit.write(save_best_score)\n",
    "                                 #best_score_element[0].split(\"|\" ==> [\"gi\", '15642790','ref', \"NP_227831.1\",\"\"])\n",
    "                    else:\n",
    "                        # ex ) [ A1 of AAE , A2 of AAE, 30]\n",
    "                        with open (Score_file+selected_species_dic[species_of_query]+\"_\"+selected_species_dic[species_of_subject]+\n",
    "                                  \"_second_oneway_threshold_best_hit_S\"+ str(threshold_score),\"a\") as second_onewaythreshold_best_hit:\n",
    "                                  second_save_best_score = selected_species_dic[species_of_query]+\"_\"+best_score_element[0].split(\"\\s\")[0].split(\"\\s\")[0]+\\\n",
    "                                  selected_species_dic[species_of_subject]+\"_\"+best_score_element[1].split(\"\\s\")[0]+\" \"+best_score_element[2]+\" \"+best_score_element[3]+\"\\n\"\n",
    "                                  second_oneway_threshold_best_hit.write(second_save_best_score)\n",
    "            \n",
    "                else: # If species_of_query not equal with species_of_subject , run reversing RunBlast\n",
    "                    for best_score_element in best_score:\n",
    "                        if not \"-1\" in best_score_element :\n",
    "                            with open(Score_file+selected_species_dic[species_of_query]+\"_\"+selected_species_dic[species_of_subject]+\"_S\"+str(threshold_score)+\"_\"+str(parallel_num),\"a\")\\\n",
    "                            as save_best_hit:\n",
    "                                best_score_save = selected_species_dic[species_of_query]+\"_\"+best_score_element[0].split(\"\\s\")[0]+\" \"+ selected_species_dic[species_of_subject]+\"_\"+\\\n",
    "                                \"_\"+best_score_element[1].split(\"\\s\")[0]+\" \" +best_score_element[2]+\" \"+best_score_element[3]+\"\\n\"\n",
    "                                save_best_hit.write(best_score_save)\n",
    "                    for blastp_score_split_list_element in blastp_score_split_list :\n",
    "                        with open(Score_file+selected_species_dic[species_of_query]+\"_\"+selected_species_dic[species_of_subject]+\"_\"+\"blastp_score_split_list_S\"+str(threshold_score)+\"_\"+str(parallel_num), \"a\") as save_blastp_score_split_list:\n",
    "                            blastp_score_split_list_save = selected_species_dic[species_of_query]+\"_\"+blastp_score_split_list_element[0].split(\"\\s\")[0]+\" \"+selected_species_dic[species_of_subject]+\"_\"+blastp_score_split_list_element[1].split(\"\\s\")[0]+\" \"+blastp_score_split_list_element[2]+\" \"+blastp_score_split_list_element[3]+\"\\n\"\n",
    "                            save_blastp_score_split_list.write(blastp_score_split_list_save)\n",
    "                \n",
    "            if save_raw_blastp_score:\n",
    "                with open(Score_file+selected_species_dic[species_of_query]+\"_\"+selected_species_dic[species_of_subject]+\"_S\"+\\\n",
    "                         str(threshold_score)+\"_\"+str(parallel_num), \"a\") as save_blastp:\n",
    "                    save_blastp.write(blastp_score)\n",
    "                    \n",
    "            #bar.finish()\n",
    "            return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 9.Oneway_Threshold_Best_Hit(mode)\n",
    "modes are 1. Blastp 2. BLASTP using precalculated data 3. Clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Oneway_Threshold_Best_Hit(\"1\")\n",
    "#backward_best_hit_work_list.append((1,3,5000))\n",
    "#max(backward_best_hit_work_list)\n",
    "#backward_best_hit_work_list\n",
    "user_selected_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "backward_best_hit_work_list = [] #For Testing make this Global Variable\n",
    "def Oneway_Threshold_Best_Hit(mode):\n",
    "    global queryV,user_selected_number, cpu_count\n",
    "    \"This Function accept the mode and Run Program and return backward_best_hit_work_list\"\n",
    "    process_list = []\n",
    "    #backward_best_hit_work_list = [] # Container to save backward best hit work # After Testing make this local Variable\n",
    "    if \"1\" in mode:\n",
    "        #print(\"Blastp Mode is Selected\")\n",
    "        for i in user_selected_number: # user_selected_number is the list of Species 1,2 ..\n",
    "            queryV =  GetQuerySequence(selected_species_dic[i])\n",
    "            # Convert to list format with Genome Seq and Genome Id \n",
    "            for k in user_selected_number: # Select of Subject\n",
    "                if k <i: #One Way Blast Method\n",
    "                    #gene --> query 1->1 1->2 1->3 Forward Checking will skip the same or less\n",
    "                    print(k)\n",
    "                    continue\n",
    "                else:\n",
    "                    #print(\"Doing the blastp & Forward best hit searches between %s genome and %s genome\"%\\\n",
    "                    #      (selected_species_dic[i],selected_species_dic[k]))\n",
    "                    queryV_len = len(queryV) #Repeating Time\n",
    "     \n",
    "                    if cpu_count ==1:\n",
    "                        blastp_time_start = time.time() #start Time\n",
    "                        #print(\"Running Parallel Query with the Following parameter i,k,cpu_count\", i,k,cpu_count)\n",
    "                        RunParallelQuery(i , k , queryV , cpu_count)\n",
    "                        blastp_time_end = time.time()\n",
    "                        #print(\"The Blastp & Forward best hit searches took %.2f minutes\"%((blastp_time_end- blastp_time_start)/60))\n",
    "                    else:\n",
    "                        if queryV_len < cpu_count : \n",
    "                            # If the number of queryV_len is less than cpu_count, Remark will select the number of queryV_len\n",
    "                            blastp_time_start = time.time()\n",
    "                            parallel_query = DivisionParallelQuery(queryV, 1, queryV_len, queryV_len)\n",
    "                            #1 is query_division_value. Because queryV_len / queryV_len( = cpu_count) is 1\n",
    "                            for m in range(queryV_len):\n",
    "                                process = multiprocessing.Process(target = RunParallelQuery, args = (i,k, parallel_query[m],m+1))\n",
    "                                #args(i => species of query, k = > species of subject , m+1 = > cpu_count ex) 1,2 ...)\n",
    "                                process_list.append(process)\n",
    "                                process.start()\n",
    "                            for n in process_list:\n",
    "                                n.join()\n",
    "                            blastp_time_end = time.time()\n",
    "                            #print(\"The blastp & forward best hit searches took %.2f minutes\"%((blastp_time_end - blastp_time_start)/60))\n",
    "                        else:\n",
    "                            blastp_time_start = time.time()\n",
    "                            query_division_value = queryV_len / cpu_count\n",
    "                            parallel_query = DivisionParallelQuery(queryV, query_division_value , cpu_count, queryV_len)\n",
    "                            \n",
    "                            for m in range(cpu_count):\n",
    "                                process = multiprocessing.Process(target=RunParallelQuery, args= (i,k,parallel_query[m],m+1))\n",
    "                                #args(i =>species of query , k = > species of subject , m+1 => cpu_count ex)1,2 ...\n",
    "                                process_list.append(process)\n",
    "                                process.start()\n",
    "                            for n in process_list:\n",
    "                                n.join()\n",
    "                            blastp_time_end = time.time()\n",
    "                            #print(\"The blastp & forward best hit searches took %.2f minutes\"%((blastp_time_end - blastp_time_start)/60))\n",
    "                            \n",
    "                    if not i == k:\n",
    "                        backward_best_hit_work_list.append((i,k,queryV_len))\n",
    "    elif \"2\" in mode:\n",
    "        print(\"BLASTP using precalculated data mode Running\")\n",
    "        for i in user_selected_number: #Select species to write query\n",
    "            queryV = GetQuerySequence(selected_species_dic[i])\n",
    "            for k in user_selected_number: #Select of subject\n",
    "                if Blastp_data + selected_species_dic[i]+\"_\"+selected_species_dic[k] + \"_oneway_threshold_best_hit_S\"+\\\n",
    "                str(threshold_score) in precalculated_data_list:\n",
    "                    used_precalculated_data_list.append(selected_species_dic[i]+\"_\"+selected_species_dic[k])\n",
    "                    continue\n",
    "                else:\n",
    "                    if k<i: # gene ===> query 1 -> 1 , 1 ->2  , 1 ->3 ,2 ->2 , 2->3\n",
    "                        continue\n",
    "                    else:\n",
    "                        #print(\"Doing the blastp & Forward best hit searches between %s genome and %s genome\"%(selected_species_dic[i],selected_species_dic[k]))\n",
    "                        queryV_len = len(queryV)\n",
    "                                    \n",
    "                        if cpu_count ==1:\n",
    "                            blastp_time_start = time.time()\n",
    "                            RunParallelQuery(i,k,queryV, c)\n",
    "                            blastp_time_end = time.time()\n",
    "                            #print(\"The blastp & forward best hit searches took %.2f minutes\"%\\\n",
    "                            #      ((blastp_time_end-blastp_time_start)/60)) \n",
    "                        else:\n",
    "                            if queryV_len<cpu_count:\n",
    "                                #if the number of queryV_len is less than cpu_count, Remark will select the number of queryV_len.\n",
    "                                blastp_time_start = time.time()\n",
    "                                parallel_query = DivisionParallelQuery(queryV,1,queryV_len, queryV_len)\n",
    "                                #1 is query division_value. Because queryV_len /queryV_len( = cpu_count) is 1\n",
    "                                for m in range(queryV_len):\n",
    "                                    process = multiprocessing.Process(target= RunParallelQuery, args = (i,k,parallel_query[m],m+1))\n",
    "                                    #args (i => species of query, k = > species of subject, m+1 = > cpu_count e.x) 1,2,3...)\n",
    "                                    process_list.append(process)\n",
    "                                    process.start()\n",
    "                                    for n in process_list:\n",
    "                                        n.join()\n",
    "                                    blastp_time_end = time.time()\n",
    "                                    #print(\"The blastp & forward best hit searches took %.2f minutes\"%\\\n",
    "                                    #      ((blsatp_time_end- blastp_time_start)/60))\n",
    "                                else:\n",
    "                                    blastp_time_start = time.time()\n",
    "                                    query_division_value = queryV_len / cpu_count\n",
    "                                    print(\"you selected cpu more than1 so query division valueis\",\\\n",
    "                                         query_division_value)\n",
    "                                    parallel_query = DivisionParallelQuery(queryV , query_division_value, cpu_count , queryV_len)\n",
    "                                    \n",
    "                                    for m in range(cpu_count):\n",
    "                                        process = multiprocessing.Process(target = RunParallelQuery, args = (i,k,parallel_query[m],m+1))\n",
    "                                        #args (i = > species of query, k = > species of subject, m+1 = > cpu_count ex ) 1,2,...\n",
    "                                        process_list.append(process)\n",
    "                                        process.start()\n",
    "                                    for n in process_list:\n",
    "                                        n.join()\n",
    "                                    blastp_time_end = time.time()\n",
    "                                    print(\"The blastp & forward best hit searches took %.2f minutes\"%((blastp_time_end - blastp_time_start)/60))\n",
    "                            new_calculated_data_list.append(selected_species_dic[i]+\"_\"+selected_species_dic[k])\n",
    "                            if not i ==k :\n",
    "                                backward_best_hit_work_list.append(i,k,queryV_len)\n",
    "    return backward_best_hit_work_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384 -1\n",
      "213 384\n",
      "97 384\n",
      "227 384\n",
      "220 384\n",
      "200 384\n",
      "174 384\n",
      "87 384\n",
      "78 384\n",
      "78 384\n",
      "77 384\n",
      "73 384\n",
      "75 384\n",
      "74 384\n",
      "72 384\n",
      "70 384\n",
      "71 384\n",
      "69 384\n",
      "1\n",
      "202 -1\n",
      "74 202\n",
      "60 202\n",
      "56 202\n",
      "55 202\n",
      "54 202\n",
      "53 202\n",
      "52 202\n",
      "53 202\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "3\n",
      "59 -1\n",
      "54 59\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "85 -1\n",
      "79 85\n",
      "74 85\n",
      "71 85\n",
      "72 85\n",
      "69 85\n",
      "69 85\n",
      "67 85\n",
      "66 85\n"
     ]
    }
   ],
   "source": [
    "#cpu_count = 1#cpu shortage cause broken pipe error if no sufficient core availiable\n",
    "result = Oneway_Threshold_Best_Hit(\"1\")\n",
    "#results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 5, 157)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 10. Backward_Best_Hit(args):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Backward_Best_Hit(args):\n",
    "    species_of_query, species_of_subject, queryV_len = args\n",
    "    #Received 3 types of input argument\n",
    "    start_time_BBH = time.time()\n",
    "    forward_best_hit_score_list = []\n",
    "    blastp_score_split_list = []\n",
    "    print(\"Start to run the backward best hit between %s genome %genome\"%(selected_species_dic[species_of_query],selected_species_dic[species_of_subject]))\n",
    "    if queryV_len <cpu_count:\n",
    "        #if the number of queryV_len is less than cpu_count, the cpu_count is changed to queryV_len\n",
    "        for parallel_num in range(queryV_len):\n",
    "            parallel_num +=1\n",
    "            with open(Score_file+selected_species_dic[species_of_query]+\"_\"+selected_species_dic[species_of_subject]+\"_\"+\"best_score_S\"+\\\n",
    "                     str(threshold_score)+\"_\"+str(parallel_num),\"r\") as best_hit_score:\n",
    "                for each_line in best_hit_score:\n",
    "                    split_each_line = each_line.split(\" \")\n",
    "                    split_each_line[2] = int(split_each_line[2])\n",
    "                    split_each_line[3] = int(split_each_line[3])\n",
    "                    print(split_each_line)  #to check\n",
    "                    forward_best-hit_score_list.append(split_each_line)\n",
    "            with open(Score_file+selected_species_dic[species_of_query]+\"_\"+selected_species_dic[species_of_subject]+\"_\"+\"blastp_score_split_list_S\"+\\\n",
    "                     str(threshold_score)+\"_\"+str(parallel_num),\"r\") as blastp_score:\n",
    "                for each_line in blastp_score:\n",
    "                    split_each_line = each_line.split(\" \")\n",
    "                    split_each_line[2] = int(split_each_line[2])\n",
    "                    split_each_line[3] = int(split_each_line[3])\n",
    "                    print(split_each_line)\n",
    "                    blastp_score_split_list.append(split_each_line)\n",
    "    else:\n",
    "        for parallel_num in range(cpu_count):\n",
    "            parallel_num += 1\n",
    "            with open(Score_file+selected_species_dic[species_of_query]+\"_\"+selected_species_dic[species_of_subject]+\"_\"+\"best_score_S\"+\\\n",
    "                     str(threshold_score)+\"_\"+str(parallel_num),\"r\") as best_hit_score:\n",
    "                for each_line in best_hit_score:\n",
    "                    split_each_line = each_line.split(\" \")\n",
    "                    split_each_line[2] = int(split_each_line[2])\n",
    "                    split_each_line[3] = int(split_each_line[3])\n",
    "                    print(split_each_line)\n",
    "                    \n",
    "                    forward_best_hit_score_list.append(split_each_line)\n",
    "            with open(Score_file+selected_species_dic[species_of_query]+\"_\"+selected_species_dic[species_of_subject]+\"_\"+\"blastp_score_split_list_S\"+\\\n",
    "                     str(threshold_score)+\"_\"+str(parallel_num),\"r\") as balstp_score:\n",
    "                for each_line in blastp_score:\n",
    "                    split_each_line = each_line.split(\" \")\n",
    "                    split_each_line[2] = int(split_each_line[2])\n",
    "                    split_each_line[3] = int(split_each_line[3])\n",
    "                    print(split_each_line)\n",
    "                    blastp_score_split_list.append(split_each_line)\n",
    "    #bar = Bar(\"Searching: \"+selected_species_dic[species_of_query]+\"_\"+selected_species_dic[species_of_subject],max=len(forward_best_hit_score_list))\n",
    "    \n",
    "    for forward_best_hit_score_element in forward_best_hit_score_list:\n",
    "        matching_list = []\n",
    "        backward_best_score = [\"-1\",\"-1\",'-1']\n",
    "        bar.next()\n",
    "        for element in blastp_score_split:\n",
    "            if element[1] == forward_best_hit_score_element[1]:\n",
    "                matching_list.append(element)\n",
    "            for element in matching_list:\n",
    "                if int(element[2])> int(backward_best_score[2]):\n",
    "                    backward_best_score = element\n",
    "    #        with open('./'+selected_species_dic[species_of_query]+\"_\"+selected_species_dic[species_of_subject]+'_subtraction'+\"_\"+str(threshold_score), 'a') as subtraction :\n",
    "    #            save_data = int(backward_best_score[2]) - int(forward_best_hit_score_element[2])\n",
    "    #            subtraction.write(str(save_data)+\"\\n\")\n",
    "    \n",
    "        if  int(backward_best_score[2])-int(forward_best_hit_score_element[2])<=threshold_score :\n",
    "            #\n",
    "            with open(Score_file+selected_species_dic[species_of_query]+\"_\"+selected_species_dic[species_of_subject]+\"_oneway_threshold_best_hit_S\"+\\\n",
    "                     str(threshold_score),\"a\") as other_oneway_threshold_best_hit:\n",
    "                save_data = forward_best_hit_score_element[0]+\" \"+forward_best_hit_score_element[1]+\" \"+str(int(forward_best_hit_score_element[2]))+\"\\n\"\n",
    "                other_oneway_threshold_best_hit.write(save_data)\n",
    "    #bar.finish()\n",
    "    finish_time_BBH = time.time()\n",
    "    RBH_time = float((finish_time_BBH-start_time_BBH)/60)\n",
    "    print(\"BackwardBestHit of %s -%s took %.2f minutes\"%(selected_species_dic[species_of_query],selected_species_dic[species_of_subject],RBH_time))\n",
    "    return RBH_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 11. Search_Equal_BBH_Data(target_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  Search_Equal_BBH_Data(target_A):\n",
    "    \"\"\" Search the equal backward best hit data. ex --> AAE_AAE_backward_best_hit \"\"\"\n",
    "    put_data = equal_BBH_data_dic[target_A]\n",
    "    if put_data[1] ==0:\n",
    "        pass\n",
    "    else:\n",
    "        copy_put_data = copy.copy(put_data) #Copy Module to Copy \n",
    "        copy_put_data.insert(0,target_A)\n",
    "        result.put(copy_put_data)\n",
    "        equal_BBH_Data_dic[target_A][1] = 0\n",
    "        print(\"Put zero in Search_Equal_BBH_Data ----\",equal_BBH_data_dic[target_A])\n",
    "        for i in second_equal_BBH_data:\n",
    "            if i[2] == 0:\n",
    "                pass\n",
    "            else:\n",
    "                if i[0]==target_A or i[1] == target_A:\n",
    "                    copy_second_put_data = copy.copy(i)\n",
    "                    tasks.put(copy_second_put_data)\n",
    "                    #Don't put results as queue. Because the tasks will put copy_second_put_data to results as queue.\n",
    "                    i[2] ==0\n",
    "                    print(\"--put zero in second_equal_BBH_data--\",i)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  12. Search_Unequal_BBH_Data(target_B):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Search_Unequal_BBH_Data(target_B):\n",
    "    \"\"\" Search the unequal backward best hit data. ex) AAE_CAC_backward_best_hit\"\"\"\n",
    "    for i in unequal_BBH_data:\n",
    "        if i[2]==0:\n",
    "            pass\n",
    "        else:\n",
    "            if target_B[0] or target_B[0] == i[1] or target_B[1] == i[0] or target_B[1] == i[1]:\n",
    "                copy_i = copy.copy(i)\n",
    "                tasks.put(copy_i)\n",
    "                unequal_BBH_data[unequal_BBH_data.index(i)][2] = 0\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 13.Matching_BBH(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Matching_BBH(target):\n",
    "    \"match the backward best hit\"\n",
    "    if target[2] == 0:\n",
    "        return\n",
    "    else:\n",
    "        copy_target = copy.copy(target)\n",
    "        Search_Equal_BBH_Data(copy_target[0])\n",
    "        Search_Equal_BBH_Data(copy_target[1])\n",
    "        results.put(copy_target)\n",
    "        unequal_BBH_data[unequal_BBH_data.index(target)][2] =0\n",
    "    for j in unequal_BBH_data:\n",
    "        if j[2]==0:\n",
    "            pass\n",
    "        else:\n",
    "            if copy_target[0] or copy_target[0] ==j[1] or copy_target[1] ==j[0] or copy_target[1]==j[1]:\n",
    "                copy_j = copy.copy(j)\n",
    "                print(\"targ_get, j = %s %s (copy_target,j)\",j)\n",
    "                unequal_BBH_data[unequal_BBH_data.index(j)][2] =0\n",
    "                tasks.put(copy_j)\n",
    "        while not tasks.empty():\n",
    "            get_task = tasks.get()\n",
    "            Search_Equal_BBH_Data(get_task[0])\n",
    "            Search_Equal_BBH_Data(get_task[1])\n",
    "            results.put(get_task)\n",
    "            Search_Unequal_BBH_Data(get_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 14.Generating_Matrix_Clustering_Ortholog(element_set,bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generating_Matrix_Clustering_Ortholog(element_set,bar):\n",
    "    \"\"\"Generate the matrix of clustering ortholog \"\"\"\n",
    "    row_data = []\n",
    "    col_data = []\n",
    "    temp_results = queue.Queue()\n",
    "    bar.next()\n",
    "    for element in element_set:\n",
    "        if row_data.count(element[0])>0:\n",
    "            #if element[0] exist, returning the index in the row_data.\n",
    "            row = row_data.index(element[0])\n",
    "            # element[0] is data of row. ['gi|15606057|ref|NP_213434.1|', 'gi|15606057|ref|NP_213434.1|', '3823\\n']\n",
    "        else:\n",
    "            row = len(row_data)\n",
    "            row_data.append(element[0])\n",
    "            if col_data.count(element[0])<1:\n",
    "                #if element[0] doesn't exists, appending the element[0] to the col_data.\n",
    "                col_data.append(element[0])\n",
    "        if col_data.count(element[1])>0:\n",
    "            col = col_data.index(element[1]) #element[1] is data of col\n",
    "        else:\n",
    "            col = len(col_data)\n",
    "            col_data.append(element[1])\n",
    "            \n",
    "        temp_results.put([row,col,element[2]])\n",
    "        \n",
    "    score_metrix = numpy.matlib.zeros((len(row_data),len(col_data)),dtype = numpy.float)\n",
    "    #Create a new matrix of a given shape (the size_results) and type, filled with zeros.\n",
    "    while not temp_results.empty():\n",
    "        get_temp_results = temp_results.get()\n",
    "        row = get_temp_results[0]\n",
    "        col = get_temp_results[1]\n",
    "        score_matrix[row,col] = get_temp_results[2]\n",
    "        score_matrix[col,row] = get_temp_results[2]\n",
    "    if len(row_data)> 1000 and cpu-count >1:\n",
    "        #The big size of Matrix(bigger than 1000 X 1000) will be computed by Parallel_Matrix_Multiplication_Using_Numpy function\n",
    "        score_matrix = Parallel_MCL(score_matrix)\n",
    "        \n",
    "    else:\n",
    "        score_matrix = MCL(score_matrix)\n",
    "    Clustering(row_data,col_data, score_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 15.Parallel_MCL(score_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Parallel_MCL(score_matrix):\n",
    "    count = 0\n",
    "    infinitesimal_value = 10**-10\n",
    "    idempotent_matrix = numpy.matlib.ones((2,2)) #In newer Version np.ones\n",
    "    while idempotent_matrix.sum() > infinitesimal_value: \n",
    "        #Greater than infinitesimal _ value\n",
    "        MCL_time_start = time.time()\n",
    "        pool = multiprocessing.Pool(cpu_count)\n",
    "        #multiprocessing.Pool(processes=None,initializer=None,initargs=(),maxtasksperchild=None,) PARAMETER\n",
    "        multipplication_results = pool.map(Parallel_Matrix_Multiplication_Using_Numpy, zip(score_matrix, repeat(score_matrix)))\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        \n",
    "        pool = multiprocessing.Pool(cpu_count) # Create a inflation_matrix (part 1)\n",
    "        power_results = pool.map(Parallel_Matrix_Power_Usint_Numpy, multipplication_results)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        \n",
    "        sum_matrix = 0\n",
    "        for i in power_results:\n",
    "            sum_matrix = i+ sum_matrix\n",
    "        pool = multiprocessing.Pool(cpu_count) #Create a inflation_matrix (part 2)\n",
    "        divide_results = pool.map(Parallel_Matrix_Power_Usint_Numpy, zip(power_results, repeat(sum_matrix)))\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        \n",
    "        for i in range(len(divide_results)):\n",
    "            #Make a Combined matrix for results of Parallel_Matrix_Multiplication_Using_Numpy Function.\n",
    "            if i ==0:\n",
    "                score_matrix = divide_results[i]\n",
    "            else:\n",
    "                score_matrix = numpy.concatenate((score_matrix, divide_results[i]),axis =0)\n",
    "        sum_results = 0\n",
    "        for i in multiplication_results:\n",
    "            sum_results += i\n",
    "        idempotent_matrix = abs(numpy.sum(score_matrix) - sum_results)\n",
    "        #identify whether inflation_matrix is idempotent matrix or not\n",
    "        count +=1\n",
    "        if count > infinite_loop:\n",
    "            #It will prevent the infinite loop of MCL algorithms.\n",
    "            break\n",
    "        MCL_time_finish = time.time()\n",
    "        if verbose:\n",
    "            print(\"MCL time %f, count :%d, matrix size :%d *%d \"%((MCL_time_finish-MCL_time_start)/60),count,score_matrix[0].size)\n",
    "    return score_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 16. MCL(score_matrix)\n",
    "A^2 = A. In linear  algebra, an idempotent matrix is a matrix which, when multiplied by itself, yields itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MCL(score_matrix):\n",
    "    \"This function run MCL.....\"\n",
    "    print(\"MCL is Runing\")\n",
    "    count = 0\n",
    "    infinitesimal_value = 10*-10\n",
    "    idempotent_matrix = numpy.matlib.ones((2,2)) #np.ones((2,2))\n",
    "    while idempotent_matrix.sum() > infinitesimal_value: # sum is 4\n",
    "        MCL_time_start = time.time()\n",
    "        expansion_matrix = score_matrix ** 2\n",
    "        score_matrix = numpy.power(expansion_matrix, inflation_factor)\n",
    "        #np.power(matrix, int)\n",
    "        score_matrix_sum = score_matrix.sum(axis = 0)\n",
    "        score_matrix = numpy.divide(score_matrix,score_matrix_sum) # Create a inflation_Matrix\n",
    "        idempotent_matrix = abs(score_matrix - expansion_matrix)\n",
    "        #identify weather inflation_matrix is idempotent matrix or not\n",
    "        count +=1\n",
    "        if count > infinite_loop:\n",
    "            #It will prevent the infinite loop of MCL algorithm.\n",
    "            break\n",
    "        MCL_time_finish = time.time()\n",
    "        if verbose:\n",
    "            print(\"MCL time:%f, Count: %d , matrix size: %d *%d\"%((MCL_time_finish-MCL_time_start)/60,count, score_matrix[0].size,\\\n",
    "                                                                 score_matrix[0].size))\n",
    "            \n",
    "    return score_matrix\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 17. Clustering(row_data, col_data, score_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Clustering(row_data, col_data, score_matrix):\n",
    "    global cluster_count, ortholog_count\n",
    "    ortholog_temp_list = []\n",
    "    for i in range(len(row_data)):\n",
    "        ortholog_list = []\n",
    "        ortholog_sum = 0\n",
    "        ortholog = queue.Queue() #It is Queue which is put the orhtolog.\n",
    "        gene_id_queue = queue.Queue() # It is Queue which is put the orhtolog having changed gene ID\n",
    "        for j in range(len(col_data)):\n",
    "            if 0.1 <= score_matrix[i,j]:\n",
    "                ortholog.put(col_data[j])\n",
    "                ortholog_list.append(col_data[j])\n",
    "    if ortholog.qsize() >=3: # If the ortholog queue has the element of ortholog more than2, it will be printed.\n",
    "        for element in ortholog_list:\n",
    "            try:\n",
    "                ortholog_sum += ortholog_temp_list.index(element) + 1\n",
    "            except ValueError:\n",
    "                with open(Cluster_out+\"_geneID_S\"+str(threshold_score)+\"_\"+str(inflation_factor),\"a\") as ortholog_list_save:\n",
    "                    ortholog_print = \"cluster\"+str(cluster_count)+\":\"\n",
    "                    ortholog_list_save.write(ortholog_print)\n",
    "                    \n",
    "                    while not ortholog.empty():\n",
    "                        get_ortholog = ortholog.get()\n",
    "                        ortholog_list_save.write(\"\\t\"+get_ortholog)\n",
    "                        try:\n",
    "                            get_ortholog_split = get_ortholog.split(\"_\") #get_ortholog --> ECO_170082288, get_ortholog.split(\"_\")\n",
    "                            gene_id_que.put(get_ortholog_split[0]+\"_\"+gene_id_dic[get_ortholog_split[1]])\n",
    "                        except KeyError:\n",
    "                            gene_id_que.put(get_ortholog) #If the gene_id_dic dont have get_ortholog, it will print the original ID(get_ortholog).\n",
    "                    ortholog_list_save.write(\"\\n\")\n",
    "                with open(Cluster_out +\"_KO_ID_S\"+str(threshold_score)+\"_\"+str(inflation_factor),\"a\") as ortholog_list_geneID_save:\n",
    "                    \n",
    "                        ortholog_list_geneID_print = \"cluster \"+ str(cluster_count)+\" :\"\n",
    "                        ortholog_list_geneID_save.write(ortholog_list_geneID_print)\n",
    "                        while not gene_id_queue.empty():\n",
    "                            ortholog_list_geneID_save.write(\"\\t\"+gene_id_queue.get())\n",
    "                            ortholog_count +=1\n",
    "                            cluster_count +=1\n",
    "                            ortholog_list_geneID_save.write(\"\\n\")\n",
    "                        break\n",
    "                ortholog_temp_list = operator.concat(ortholog_temp_list, ortholog_list)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 18.Parallel_Matrix_Multiplication_Using_Numpy(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Parallel_Matrix_Multiplication_Using_Numpy(data):\n",
    "    \n",
    "    matrix_element, matrix  = data\n",
    "    #mul = numpy.multiply(matrix_element,matrix)\n",
    "    mul = matrix_element * matrix\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 19.Parallel_Matrix_Power_Using_Numpy(matrix_element):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Parallel_matrix_Power_Using_Numpy(matrix_element):\n",
    "    \"This Function return Metrix Power by inflation_factor\"\n",
    "    power_matrix_element = numpy.power(matrix_element, inflation_factor)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 20.Parallel_Matrix_Divide_Using_Numpy(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Parallel_Matrix_Divide_Using_Numpy(data):\n",
    "    \"data is 2 values. First is matrix_element, sum_data.\"\n",
    "    matrix_element , sum_data = data\n",
    "    return numpy.divide(matrix_element, sum_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 21. Read_Species_List()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "def Read_Species_List(pr=0):\n",
    "    \"\"\" If pr is 1, it will print \"Species_List\"  Other wise only return the Value in dic Format \"\"\" \n",
    "    read_species = os.listdir(Species) \n",
    "    selected_species_dic = {}  #list \n",
    "    backward_selected_species_dic = {} #list\n",
    "    for i, species in enumerate(sorted(read_species), start=1): #\n",
    "        selected_species_dic[i] = species\n",
    "        backward_selected_species_dic[species] = i \n",
    "        if pr == 1 :\n",
    "            print (str(i)+\".\", species)\n",
    "        number = i\n",
    "    return selected_species_dic, backward_selected_species_dic, number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 22. Del_File(path,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Del_File(path,file):\n",
    "    import subprocess\n",
    "    \n",
    "    del_file = subprocess.Popen([\"del \"+path+file],stdout=subprocess.PIPE, stderr = subprocess.PIPE,shell= True)\n",
    "    del_file_stream = del_file.communicate()\n",
    "    if not del_file_stream[1]:\n",
    "        print(\"Done to del\"+path+file)\n",
    "    elif del_file_stream[1]:\n",
    "        print(del_file_stream[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i developed the below Function to remove the file may be different but will check later "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_(file):\n",
    "    import os\n",
    "    try:\n",
    "        os.remove(file)\n",
    "        print(\"Successfully_remove\",file)\n",
    "    except:\n",
    "        print(\"Check the File And Path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 23.Check_File(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Check_File(file):\n",
    "    \"Check the file weather exist or not\"\n",
    "    file_list = glob.glob(file+'*')\n",
    "    \n",
    "    print(file_list)\n",
    "    \n",
    "    if (Cluster_out+\"_geneID_S\"+str(threshold_score)+\"_\"+str(inflation_factor) or Cluster_out+\"_KO_ID_S\"+str(threshold_score)+\"_\"+str(inflation_factor)) in file_list:\n",
    "        print (\"Please, set other name of output.\")\n",
    "        sys.exit(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 24.Read_Equal_BBH(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Read_Equal_BBH(path):\n",
    "    \"Read_Equal BBH by user path\"\n",
    "    global threshold_score\n",
    "    with open(path + \"_oneway_threshold_best_hit_S\"+str(threshold_score),\"r\") as equal_RBH:\n",
    "        for j in equal_RBH:\n",
    "            split_data = j.split()\n",
    "            split_data[2] = int(split_data[2])\n",
    "            equal_BBH_data.append(split_data)\n",
    "            equal_BBH_data_dic[split_data[0]] = split_data[1:]\n",
    "    try:\n",
    "        with open(path+\"_second_oneway_threshold_best_hit_S\"+str(threshold_score),\"r\") as second_equal_RBH:\n",
    "            for j in second_equal_RBH:\n",
    "                split_data = j.split()\n",
    "                split_data[2] = int(split_data[2])\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 25. Read_Unequal_BBH(path):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Read_Unequal_BBH(path):\n",
    "    \"Read Unequal BBH path passed by User\"\n",
    "    with open(path+\"_oneway_threshold_best_hit_S\"+str(threshold_score),\"r\") as unequal_RBH:\n",
    "        for j in unequal_RBH:\n",
    "            split_data[2] = int(split_data[2])\n",
    "            unequal_BBH_data.append(split_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Main Program Testing\n",
    "`************* No Function below **************************`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arg parse is not done here\n",
    "# If parameter not Provided the below code will run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. BLASTP. \n",
      "2. BLASTP using precalculated data. \n",
      "3. Clustering.\n",
      "\n",
      ">> Select a mode or modes (1 or 2 or 1 3 or 2 3): 1 3\n",
      "1. AAE\n",
      "2. CAC\n",
      "3. ECO\n",
      "4. ECU\n",
      "5. HIN\n",
      "6. LLA\n",
      "7. Log\n",
      "8. SCE\n",
      "9. SPO\n",
      "10. SPY\n",
      "11. SYN\n",
      "12. TMA\n",
      "13. YPE\n",
      "14. chicken.faa\n",
      "15. chimpanzee.faa\n",
      "16. dog.faa\n",
      "17. elegans.faa\n",
      "18. fruitfly.faa\n",
      "19. human.faa\n",
      "20. mouse.faa\n",
      "21. opossum.faa\n",
      "22. pufferfish.faa\n",
      "23. rat.faa\n",
      "24. seasquirt.faa\n",
      "25. zebrafish.faa\n",
      ">> Select Genomes to detect Orthologs(e.g. 1 2 3 4 5 or 1-5) : 1 2 3 4 5\n",
      "AAE CAC ECO ECU HIN \n",
      "1. BLOSUM45\n",
      "2. BLOSUM62\n",
      "3. BLOSUM82\n",
      "4. Quit\n",
      "\n",
      "Enter a matrix number: 1\n",
      "You can use 12 processors. \n",
      "If you input >2, The Program will run a parallel--Enter the number of CPU you want to use :2\n",
      "Enter the inflation factor to cluster1.4\n",
      "Set the name of Clustering output :result_jupyternotebook\n"
     ]
    }
   ],
   "source": [
    "#if not sys.argv[1:]\n",
    "# Program Not Run by Command Line\n",
    "import sys\n",
    "import multiprocessing\n",
    "\n",
    "# If Program not Passed with Parameter\n",
    " # if Not verified Conflict arise for local and global Variable\n",
    "print (\"1. BLASTP. \\n2. BLASTP using precalculated data. \\n3. Clustering.\\n\")\n",
    "mode = input(\">> Select a mode or modes (1 or 2 or 1 3 or 2 3): \") \n",
    "selected_species_dic, backward_selected_species_dic, number_i = Read_Species_List(pr=1)\n",
    "selected_number = input(\">> Select Genomes to detect Orthologs(e.g. 1 2 3 4 5 or 1-5) : \")\n",
    "\n",
    "\n",
    "if selected_number.find(\"-\")>0:\n",
    "    SN = selected_number.split(\"-\")\n",
    "    if int(SN[-1])>number_i:\n",
    "        print(\"Your input must be less than\" , number_i)\n",
    "        sys.exit(2)\n",
    "    else:\n",
    "        user_selected_number = range(int(SN[0]),int(SN[-1])+1)\n",
    "        print(user_selected_number)\n",
    "        for j in user_selected_number:\n",
    "            print(selected_species_dic[j],end=\" \")\n",
    "        print(\"Are Selected\")\n",
    "else:\n",
    "    user_selected_number = sorted(set([int(read_species) for read_species in selected_number.split()]))\n",
    "    if int(user_selected_number[-1])>number_i:\n",
    "        print (\"\\nWrongInput\\nInput must be less than\",number_i)\n",
    "        sys.exit(2)\n",
    "    else:\n",
    "        for j in user_selected_number:\n",
    "            print(selected_species_dic[j], end =\" \")\n",
    "\n",
    "blastp_matrix = GetMatrixNumber()\n",
    "#This Function only Return Matrix name as String\n",
    "\n",
    "cpu_count = int(input(\"You can use %s processors. \\nIf you input >2, The Program will run a parallel--\"\\\n",
    "                      %multiprocessing.cpu_count()+\"Enter the number of CPU you want to use :\"))\n",
    "if \"3\" in mode:\n",
    "    inflation_factor = input(\"Enter the inflation factor to cluster\")\n",
    "    Cluster_out = input(\"Set the name of Clustering output :\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### if Paramater Passed with Python Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "##elif sys.argv[1:]\n",
    "##command_options = argparse_value\n",
    "#genomes = command_options.genomes\n",
    "#mode = command_options.mode\n",
    "#cpu_count= command_options.cpu_count\n",
    "#blastp_matrix = command_options.blastp_matrix\n",
    "#inflation_factor = command_options.inflation_factor\n",
    "#selected_species_dic,backward_selected_species_dic, number_i = Read_Species_List()\n",
    "#user_selected_number = [backward_selected_species_dic[ele] for ele in genomes]\n",
    "#Cluster_out = command_options.Cluster_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#score_file = \"score\"\n",
    "#save_raw_blastp_score  = 100\n",
    "#import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are default Valuse\n",
    "Blastp = \"blastp\"\n",
    "Blastp_data = \"./blastp_data/\"\n",
    "#blastp_matrix = #input by user default = BLOSUM62\n",
    "#cpu_count  = #user input we use 1\n",
    "infinite_loop = 60\n",
    "inflation_factor = 1.4\n",
    "Cluster_out = \"./cluster_out/\"\n",
    "threshold_score =0  # 5 is Passed in Server\n",
    "save_raw_blastp_score = False\n",
    "Score_file = \"./score_file/\"\n",
    "Species = \"./species/\"\n",
    "verbose = False\n",
    "\n",
    "#infinite_loop = command_options.infinite_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "#Species = command_options.Species\n",
    "#Blastp = COmmand_options.Blastp\n",
    "#Score_file = command_options.Score_file\n",
    "#Blastp_data = command_options.Blastp_data\n",
    "#save_raw_blastp_score = command_options.threshold_score\n",
    "#threshold_score = command_options.threshold_score\n",
    "#verbose = command_options.verbose\n",
    "#infinite_loop = command_options.infinite_loop\n",
    "#\n",
    "if \"3\" in mode:\n",
    "    Check_File(Cluster_out)\n",
    "\n",
    "#Del_File(score_file, \"*\")\n",
    "if \"3\" in mode:\n",
    "    Log_file_name = Cluster_out+\"_S\"+str(threshold_score)+\"_\"+str(inflation_factor)+\".log\"\n",
    "elif not \"3\" in mode:\n",
    "    Log_file_name = \"Log.txt\"\n",
    "\n",
    "with open(Log_file_name, \"w\") as log:\n",
    "    log.write(\"\\t\\t\\t\"+str(datetime.datetime.now()))\n",
    "    log.write(\"\\n\\t\\t\\tkrishdb38@gmail.com\")\n",
    "    log.write(\"\\nmode\")\n",
    "    for i in mode:\n",
    "        log.write(\" \"+i)\n",
    "    log.write(\"\\ngenomes:\")\n",
    "    for i in user_selected_number:\n",
    "        log.write(selected_species_dic[i]+\" \")\n",
    "    log.write(\"\\nCPU_Count:\"+str(cpu_count))\n",
    "    log.write(\"\\nblastp matrix :\"+blastp_matrix)\n",
    "    if \"3\" in mode:\n",
    "        log.write(\"\\ninflation_factor: \"+str(inflation_factor))\n",
    "        log.write(\"\\nCluster out:\"+Cluster_out)\n",
    "    log.write(\"\\nSpecies :\"+Species)\n",
    "    log.write(\"\\nBlastp: \"+Blastp)\n",
    "    log.write(\"\\nScore file : \"+ Score_file)\n",
    "    log.write(\"\\nBlastp_data : \"+ Blastp_data)\n",
    "    log.write(\"\\nsave rawblastp score : \"+ str(save_raw_blastp_score))\n",
    "    log.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing the blastp & Forward best hit searches between AAE genome and AAE genome\n"
     ]
    },
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-9322283c6082>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mstart_time_OBH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;34m\"1\"\u001b[0m \u001b[1;32min\u001b[0m  \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mbackward_best_hit_work_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOneway_Threshold_Best_Hit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mpool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcpu_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBackward_Best_Hit\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbackward_best_hit_work_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-61-f99acb613c03>\u001b[0m in \u001b[0;36mOneway_Threshold_Best_Hit\u001b[1;34m(mode)\u001b[0m\n\u001b[0;32m     50\u001b[0m                                 \u001b[1;31m#args(i =>species of query , k = > species of subject , m+1 => cpu_count ex)1,2 ...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m                                 \u001b[0mprocess_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m                                 \u001b[0mprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m                             \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mprocess_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m                                 \u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\multiprocessing\\process.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    110\u001b[0m                \u001b[1;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpopen_spawn_win32\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[1;32mclass\u001b[0m \u001b[0mSpawnContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\multiprocessing\\popen_spawn_win32.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[0mset_spawning_popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\multiprocessing\\reduction.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;34m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[0mForkingPickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
     ]
    }
   ],
   "source": [
    "start_time_OBH = time.time()\n",
    "if \"1\" in  mode:\n",
    "    backward_best_hit_work_list = Oneway_Threshold_Best_Hit(mode)\n",
    "    pool = multiprocessing.Pool(cpu_count)\n",
    "    results = pool.map(Backward_Best_Hit,backward_best_hit_work_list)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "elif \"2\" in mode:\n",
    "    used_precalculated_data_list = []\n",
    "    new_calculated_data_list = []\n",
    "    precalculated_data_list = glob.glob(Blastp_data+\"*oneway_threshold_best_hit_S\"+str(threshold_score))\n",
    "    print(precalculated_data_list)\n",
    "    backward_best_hit_work_list = Oneway_Threshold_Best_Hit(mode)\n",
    "    \n",
    "    if not backward_best_hit_work_list ==[]:\n",
    "        #If backward_best_hit_work_list is an empty list, pool instance can't finish the work.\n",
    "        pool = multiprocessing.Pool(cpu_count)\n",
    "        results = pool.map(Backward_Best_Hit, backward_best_hit_work_list)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "    else:\n",
    "        results = [0,0]\n",
    "#Del_File(\"./\",\"query*\")   # !!! Later We remove these query Files\n",
    "finish_time_OBH = time.time()\n",
    "blastp_time_log = float(((finish_time_OBH - start_time_OBH)/60))\n",
    "print(\"BLASTP searches + forward best Hit + backwardbest hit took %f minutes\"%blastp_time_log)\n",
    "\n",
    "with open(Log_file_name , \"a\") as log:\n",
    "    log.write(\"Backward_Best_Hit took \"+str(max(results))+\" minutes\\n\")\n",
    "    log.write(\"BLASTP + Best_Hit + backward_best_hit searches took\"+ str(blastp_time_log)+\" minutes\\n\")\n",
    "if \"3\" in mode:\n",
    "    start_time_clustering = time.time()\n",
    "    #generate matrix calculate the matrix using MCL algorithm and cluster the Ortholog\n",
    "    \n",
    "    print(\"\\n >>> Start MCL algorithm and Clustering ortholog <<<\")\n",
    "    equal_BBH_data = []\n",
    "    unequal_BBH_data = []\n",
    "    equal_BBH_data_dic = {}\n",
    "    second_equal_BBH_data = []\n",
    "    results = que.Queue()\n",
    "    tasks = queue.Queue()\n",
    "    cluster_count = 1\n",
    "    ortholog_count = 0\n",
    "    gene_id_dic = {}\n",
    "    \n",
    "    with open(\"myva=gb\",\"r\") as id_read:\n",
    "        #myvba=gb is a database\n",
    "        for i in id_read:\n",
    "            gene_name,gene_id = i.split()\n",
    "            gene_id_dic[gene_id.replace(\"\\n\",\"\")] = gene_name # remove '\\n'\n",
    "            \n",
    "        if \"1\" in mode:\n",
    "            for i in user_selected_number:\n",
    "                for k in user_selected_number:\n",
    "                    if k<i:\n",
    "                        pass\n",
    "                    elif i ==k:\n",
    "                        Read_Equal_BBH(Score_file+selected_species_dic[i]+\"_\"+selected_species_dic[k])\n",
    "                    elif i !=k:\n",
    "                        Read_Unequal_BBH(Score_file+selected_species_dic[i]+\"_\"+selected_species_dic[k])\n",
    "        elif \"2\" in mode:\n",
    "            for used_data in used_precalculated_data_list:\n",
    "                first,second = used_data.split(\"_\")\n",
    "                if first == second:\n",
    "                    Read_Equal_BBH(Blastp_data+used-data)\n",
    "                elif first != second:\n",
    "                    Read_Unequal_BBH(Blastp_data+used_data)\n",
    "            for new_data in new_calculated_data_list:\n",
    "                first,second = new_data.split(\"_\")\n",
    "                if first== second:\n",
    "                    Read_Equal_BBH(Score_file+new_data)\n",
    "                elif first != second:\n",
    "                    Read_Unequal_BBH(Score_file+new_data)\n",
    "        matched_BBH_data = []\n",
    "        matched_BBH_element_data_set = []\n",
    "        \n",
    "        for unequal_RBH_element in unequal_BBH_data:\n",
    "            Matching_BBH(unequal_RBH_element)\n",
    "            temp_results_list = []\n",
    "            if results._qsize() !=0: #return the number of results as Queue.\n",
    "                while not results.empty():\n",
    "                    get_results = results.get()\n",
    "                    temp_results_list.append(get_results)\n",
    "                matched_BBH_data.append(temp_results_list)\n",
    "        #bar = bar(\"Processing\", max= len(matched_BBH_data))\n",
    "        #bar = bar(\"Processing\", max= len(matched_BBH_data)) #Not Supported in Python 3\n",
    "        #for data in matched_BBH_data:\n",
    "        #    Generating_Matrix_Clustering_Ortholog(data, bar)\n",
    "        #bar.finish()\n",
    "        finish_time_clustering = time.time()\n",
    "        mcl_time_log = float((finish_time_clustering - start_time_clustering)/60)\n",
    "        remark_time_log = float((finish_time_clustering- start_time_OBH)/60)\n",
    "        print(\"MCL algorithm and Ortholog Clustering took %.2f minutes\"%remark_time_log)\n",
    "        \n",
    "        if \"3\" in mode:\n",
    "            with open(Log_file_name,\"a\") as log:\n",
    "                log.write(\"Ortholog Count: \"+str(ortholog_count)+\",\"+\"Cluster count: \"+ str(cluster_count-1)+\"\\n\")\n",
    "                log.write(\"MCL algorithm and Ortholog Clustering took \"+str(mcl_time_log)+\"minutes\\n\")\n",
    "                log.write(\"xxx Program took \"+str(remark_time_log)+\"minutes\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
