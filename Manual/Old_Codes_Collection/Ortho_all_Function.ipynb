{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `        -------------------->     Ortho Prediction Software Developing by krish:     <----------------------------`\n",
    "                   Check all Function Working better or Not\n",
    "                   main / Global Variable use in this program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time , os, sys,subprocess , numpy as np , pandas as pd\n",
    "from alive_progress import alive_bar  #This is for bar / Progress\n",
    "from Bio import SeqIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Blastp = /usr/bin/blastp\n",
    "Blastp = \"blastp\"\n",
    "Blastp_data =  './blastp_data/'\n",
    "cpu_count = 1 # Default value\n",
    "blastp_matrix = \"BLOSUM62\" \n",
    "genome = \"\"\n",
    "infinite_loop = 70\n",
    "inflation_factor = 1.4\n",
    "#mode = None\n",
    "mode = '1' #Change Later\n",
    "Cluster_out = \"./cluster_out/\"\n",
    "#threshold_score : 0\n",
    "threshold_score =0\n",
    "Save_raw_blastp_score = False\n",
    "Score_file = \"./score_file\"\n",
    "Species = \"./species/\"\n",
    "species = \"./species/\"\n",
    "verbose = False\n",
    "Log_file_name = \"./logfiles\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1. MatrixName()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MatrixName():\n",
    "    \"\"\"This Function will Return the Matrix name choosed by User.\n",
    "    BLOSUM45 ,  BLOSUM62 , BLOSUM82 \"\"\"\n",
    "    print(\"BLOcks SUbstitution Matrix (BLOSUM) is a Substitution matrix used for sequence alignment of Proteins\")\n",
    "    print(\"\"\"\\n1. BLOSUM45 :-For more distantly related Proteins alignment DataBase\\n2. BLOSUM62 :- MidRange Seq with more than 62%similarity\\\n",
    "         \\n3. BLOSUM82 :- More related Proteins\\nAny other Key to exit -- Quit\"\"\")\n",
    "    metrix_num = input(\"\\nEnter a matrix number: \")\n",
    "    if metrix_num not in (\"1\", \"2\", '3'):\n",
    "        print(\"Wrong input *%s*Sorry not in list\\n\" %\n",
    "              metrix_num, \"*\"*20, \"Good Bye\", \"*\"*20, \"\\n\")\n",
    "        sys.exit(1)\n",
    "    if metrix_num == \"1\":return \"BLOSUM45\"\n",
    "    if metrix_num == \"2\":return \"BLOSUM62\"\n",
    "    if metrix_num == \"3\":return \"BLOSUM82\"\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLOcks SUbstitution Matrix (BLOSUM) is a Substitution matrix used for sequence alignment of Proteins\n",
      "\n",
      "1. BLOSUM45 :-For more distantly related Proteins alignment DataBase\n",
      "2. BLOSUM62 :- MidRange Seq with more than 62%similarity         \n",
      "3. BLOSUM82 :- More related Proteins\n",
      "Any other Key to exit -- Quit\n"
     ]
    }
   ],
   "source": [
    "MatrixName()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MatrixName(num):\n",
    "    if num ==\"1\":return print(\"564\")\n",
    "    if num == \"2\": return \"BLOSUM62\"\n",
    "    if num == \"3\" \n",
    "    \n",
    "MatrixName(\"2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2. QuerySequence(genome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QuerySequence(genome):\n",
    "    \"\"\"This Function Read Fastaq Files and return as a list Format with gene Position as a index\n",
    "    Later I will reduce code Lengths and Speed\"\"\"\n",
    "    global species\n",
    "    gene_sequence=\"\"\n",
    "    gene_sequence_list= []\n",
    "    try:\n",
    "        with open(species+genome) as gene:\n",
    "            for each_line in gene:\n",
    "                if \">\" in each_line:\n",
    "                    if gene_sequence != \"\":                        \n",
    "                        gene_sequence_list.append(gene_sequence)\n",
    "                        gene_sequence = \"\"\n",
    "                gene_sequence = gene_sequence+each_line\n",
    "            gene_sequence_list.append(gene_sequence)                   \n",
    "        return gene_sequence_list                                                                \n",
    "    except IOError as err:\n",
    "        print (\"IOError occurred in QuerySequence function : \" + str(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Query_seq(genome):\n",
    "    \"\"\"This Function Read Fasta (Genome) file and return as a list Format with gene Position and Sequence Developed by Krish\"\"\"\n",
    "    global species\n",
    "    try:\n",
    "        return [str((seq_record.description+seq_record.seq)) for seq_record in SeqIO.parse(species+genome, \"fasta\")]\n",
    "        # To understand this Function Bio Python library needs to be studied\n",
    "    except IOError as err:\n",
    "        print(str(err))\n",
    "a = Query_seq(\"AAE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3.Write Query(query, parallel_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WriteQuery(query, parallel_num):\n",
    "    # query is j , part of Genome , gene id , a single sequence\n",
    "    try:\n",
    "        with open(\"./query\"+\"_\"+str(parallel_num)+\".csv\", \"w\") as write_query:\n",
    "            write_query.write(query)\n",
    "    except IOError as err:\n",
    "        print (\"IOError occurred in WriteQuery function : \" + str(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WriteQuery(\"hello\",5)\n",
    "#hello is is query name and "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 4. RunBlast( Subject , parallel_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "def RunBlast(subject, parallel_num):\n",
    "    \"\"\"By this Function it will create a Pipe line to run Blastp in Computer by input Parameter\n",
    "     subject is a small size gene passed not a whole\n",
    "     parallel_nu is the CPU selected by user\"\"\"\n",
    "    subject = Species+subject \n",
    "    Blastp = \"blastp\"\n",
    "    cmd = [Blastp, \"-query\",\"./query\"+\"_\"+str(parallel_num), \"-subject\", subject,\"-matrix\", blastp_matrix,\n",
    "           \"-outfmt\", \"10 qseqid sseqid score length\"]\n",
    "    \n",
    "    run_blastp =subprocess.Popen(cmd,stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    #qseqid = query (e.g. gene sequence id) , sseqid = subject(e.g. reference genome)\n",
    "    #sequence id score = Rawscore length = Alignment length\n",
    "    \n",
    "    run_blastp_stream = run_blastp.communicate()\n",
    "    run_blastp_output_stream = run_blastp_stream[0]\n",
    "    run_blastp_error_stream = run_blastp_stream[1]  \n",
    "    \n",
    "    #display(run_blastp)\n",
    "    #display(run_blastp_error_stream)\n",
    "    return run_blastp_output_stream\n",
    "#RunBlast(\"AAE\",\"AAE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunBlast(\"AAE\",1)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 5. Get_Same_Species_Forward_Best_Hit(blastp_score):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Same_Species_Forward_Best_Hit(blastp_score):\n",
    "    blastp_score_split_list  = []\n",
    "    temp_best_score = [\"-1\",'-1','-1']\n",
    "    second_temp_best_score = []\n",
    "    best_score = []\n",
    "    second_best_score = []\n",
    "    blastp_score_split = blastp_score.split('\\n')\n",
    "    \n",
    "    del blastp_score_split[-1] #delete of [''] in last index \n",
    "    for i in blastp_score_split:\n",
    "        blastp_score_element = i.split(\",\")\n",
    "        blastp_score_split_list.append(blastp_score_element)\n",
    "    for k in blastp_score_split_list: #ex) k in ['gi|15605613|ref|NP_212986.1|', 'gi|15605613|ref|NP_212986.1|', '3702', '699']\n",
    "        if k[0] == k[1]:\n",
    "            best_score.append(k)\n",
    "        elif k[0] !=k[1]:\n",
    "            print(k[2], temp_best_score[2])\n",
    "            if int(k[2]) >int(temp_best_score[2]): #Compare the Score , Greater than\n",
    "                temp_best_score = k\n",
    "            elif int(k[2]) == int(temp_best_score[2]): # Is equal\n",
    "                if int(k[3])> int(temp_best_score[3]) : #Compare the length\n",
    "                    temp_best_score = k\n",
    "                elif int(k[3]) == int(temp_best_score[3]):\n",
    "                    second_temp_best_score.append(k)\n",
    "        #print( \"################ temp Best Score #####################\", temp_best_score)\n",
    "        second_best_score.append(temp_best_score)\n",
    "        for j in second_temp_best_score:\n",
    "            if j[2] == temp_best_score[2] and j[3] == temp_best_score[3]:\n",
    "                second_best_score.append(j)\n",
    "        for m in second_best_score:\n",
    "            if (best_score[0][2] == m[2] and int(best_score[0][3])<= int(m[3])) or int(best_score[0][2])< int(m[2]): \n",
    "                #'104'<\"23\" is True because of string. So the int Function is Used.\n",
    "                best_score.append(m)\n",
    "    return best_score\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 6. GetForwardBestHit(blastp_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetForwardBestHit(blastp_score):\n",
    "    \"Search the forward best hit among blastp scores of same species\"\n",
    "    print(\"Running Get Forward Best Hit\")\n",
    "    blastp_score_split_list = []\n",
    "    temp_best_score = ['-1','-1','-1']\n",
    "    second_temp_best_score = []\n",
    "    best_score = []\n",
    "    blastp_score_split = blastp_score.split(\"\\n\")\n",
    "    \n",
    "    del blastp_score_split[-1] #Remove last index of \"\\n\"\n",
    "    for i in blastp_score_split:\n",
    "        blastp_score_element = i.split(\",\")\n",
    "        blastp_score_split_list.append(blastp_score_element)\n",
    "    for k in blastp_score_split_list:\n",
    "        # ex) k is [\"gi|15605613|ref|NP_212986.1|\",'gi|15605613|ref|NP_212986.1|', '3702', '699']\n",
    "        print(\">>>>>>>>>>>>>>>> Get Forward Best Hit>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "        if int(k[2])>int(temp_best_score[2]): # Compare Score\n",
    "            temp_best_score = k\n",
    "        elif int(k[2]) == int(temp_best_score[2]):\n",
    "            if int(k[3])> int(temp_best_score[3]): #Comapare the Length\n",
    "                temp_best_score = k\n",
    "            elif int(k[3]) == int(temp_best_score[3]):\n",
    "                second_temp_best_score.append(k)\n",
    "    print(\"## temp best Score ###\",temp_best_score)\n",
    "    best_score.append(temp_best_score)\n",
    "    for j in second_temp_best_score:\n",
    "        if j[2] == temp_best_score[2] and j[3] == temp_best_score[3]:\n",
    "            best_score.append(j)\n",
    "    return best_score , blastp_score_split_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 7. DivisionParallelQuery(queryV,query_division_value, cpu_count , queryV_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DivisionParallelQuery( queryV ,query_division_value,cpu_count,queryV_len):\n",
    "    \"\"\" Running Division parallel Query,input is species_of_query , species_of_subject, queryV, parallel_num\"\"\"\n",
    "    parallel_query = []\n",
    "    parallel_query_start = 0\n",
    "    if queryV_len % cpu_count == 0: #Perfect division by CPU Count\n",
    "        for i in range(cpu_count):\n",
    "            i+=1\n",
    "            if parallel_query_start ==0:\n",
    "                parallel_query.append(queryV[int(parallel_query_start):i*int(query_division_value)])\n",
    "                parallel_query_start +=1\n",
    "            else:\n",
    "                parallel_query.append(queryV[int(parallel_query_start)*int(query_division_value):i*int(query_division_value)])\n",
    "                parallel_query_start +=1\n",
    "    else: # If the division is imperfect\n",
    "        for i in range(cpu_count):\n",
    "            i += 1\n",
    "            if parallel_query_start == 0:\n",
    "                parallel_query.append(queryV[int(parallel_query_start):i*int(query_division_value)])\n",
    "                parallel_query_start +=1\n",
    "            elif i<cpu_count:\n",
    "                parallel_query.append(queryV[int(parallel_query_start)*int(query_division_value):])\n",
    "                parallel_query_start +=1\n",
    "        return parallel_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 8. Run Parallel Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RunParallelQuery(species_of_query, species_of_subject, queryV, parallel_num):\n",
    "    #from progress import bar\n",
    "    #i --> Species_of_query , k --> Species_of_subject , queryV is Genome list whole file (i , k are part of this)\n",
    "    \"\"\"This Function helps to run operation on Several Process\n",
    "    Run the Following Functions. WriteQuery , RunBlast , Get_Same_Species_Forward_Best_Hit, GetForwardBestHit\"\"\"\n",
    "    \n",
    "    print(\"Parallel Query Running\") \n",
    "    #bar = Bar(\"processing\"+str(cpu_count),max = len(queryV))\n",
    "    #bar = bar(\"processing\"+str(cpu_count),max = len(queryV))  #python 3\n",
    "    for j in queryV:\n",
    "        #bar.next()\n",
    "        WriteQuery(j, cpu_count)\n",
    "        #WriteQuery() Function Only write file with info\n",
    "        blastp_score= RunBlast(selected_species_dic[species_of_subject],parallel_num)\n",
    "        \n",
    "        if blastp_score != \"\":\n",
    "            best_score, blastp_score_split_list = GetForwardBestHit(blastp_score.decode())\n",
    "            #decode() will change to str Format\n",
    "            if species_of_query == species_of_subject: # e.g. AAE == AAE It will save best_score without reversing RunBlast.\n",
    "                same_species_forward_best_score = Get_Same_Species_Forward_Best_Hit(blastp_score.decode())\n",
    "                \n",
    "                for best_score_element in same_species_forward_best_score:\n",
    "                    if best_score_element[0] == best_score_element[1]: #ex [A1 of AAE, A! of AAE , 30 ]\n",
    "                        with open(Score_file+selected_species_dic[species_of_query]+\"_\" + selected_species_dic[species_of_subject]+\"_oneway_threshold_best_hit_S\"+str(threshold_score,\"a\")) as oneway_threshold_best_hit:\n",
    "                                  save_best_score= selected_species_dic[species_of_query]+\"_\"+best_score_element[0].split(\"\\s\"[0])+\" \"+selected_species_dic[species_of_subject]+ \"_\"+best_score_element[1].split(\"\\s\")[0]+\" \"+best_score_element[2]+\"\\n\"\n",
    "                                  oneway_threshold_best_hit.write(save_best_score)\n",
    "                                 #best_score_element[0].split(\"|\" ==> [\"gi\", '15642790','ref', \"NP_227831.1\",\"\"])\n",
    "                    else:\n",
    "                        # ex ) [ A1 of AAE , A2 of AAE, 30]\n",
    "                        with open (Score_file+selected_species_dic[species_of_query]+\"_\"+selected_species_dic[species_of_subject]+\n",
    "                                  \"_second_oneway_threshold_best_hit_S\"+ str(threshold_score),\"a\") as second_onewaythreshold_best_hit:\n",
    "                                  second_save_best_score = selected_species_dic[species_of_query]+\"_\"+best_score_element[0].split(\"\\s\")[0].split(\"\\s\")[0]+\\\n",
    "                                  selected_species_dic[species_of_subject]+\"_\"+best_score_element[1].split(\"\\s\")[0]+\" \"+best_score_element[2]+\" \"+best_score_element[3]+\"\\n\"\n",
    "                                  second_oneway_threshold_best_hit.write(second_save_best_score)\n",
    "            \n",
    "                else: # If species_of_query not equal with species_of_subject , run reversing RunBlast\n",
    "                    for best_score_element in best_score:\n",
    "                        if not \"-1\" in best_score_element :\n",
    "                            with open(Score_file+selected_species_dic[species_of_query]+\"_\"+selected_species_dic[species_of_subject]+\"_S\"+str(threshold_score)+\"_\"+str(parallel_num),\"a\") as save_blastp:\n",
    "                                save_blastp_score_split_list.write(blastp_score_split_list_save)\n",
    "            #bar.finish()\n",
    "            return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 9.Oneway_Threshold_Best_Hit(mode)\n",
    "modes are 1. Blastp 2. BLASTP using precalculated data 3. Clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Oneway_Threshold_Best_Hit(mode):\n",
    "    \"This Function accept the mode and Run Program and return backward_best_hit_work_list\"\n",
    "    print(\"Oneway_Threshold_Best_Hit Is running\")\n",
    "    process_list = []\n",
    "    backward_best_hit_work_list = [] # Container to sava backward bect hit work\n",
    "    if \"1\" in mode:\n",
    "        print(\"Blastp Mode is Selected\")\n",
    "        for i in user_selected_number: # user_selected_number is the list of Species 1,2 ..\n",
    "            queryV =  QuerySequence(selected_species_dic[i]) #select the user select number \n",
    "            #queryV is the list type \n",
    "            for k in user_selected_number:\n",
    "                if k <i:\n",
    "                    print(k)\n",
    "                    continue\n",
    "                else:\n",
    "                    print(\"Doing the blastp & Forward best hit searches between %s genome and %s genome\"%\\\n",
    "                          (selected_species_dic[i],selected_species_dic[k]))\n",
    "                    queryV_len = len(queryV) #Repeating Time\n",
    "                    \n",
    "                    if cpu_count ==1:\n",
    "                        blastp_time_start = time.time() #start Time\n",
    "                        print(i,k, cpu_count) # 2 2 1 \n",
    "                        RunParallelQuery(i , k , queryV , cpu_count)\n",
    "                        blastp_time_end = time.time()\n",
    "                        print(\"The Blastp & Forward best hit searches took %.2f minutes\"%((blastp_time_end- blastp_time_start)/60))\n",
    "                    else:\n",
    "                        if queryV_len < cpu_count : \n",
    "                            # If the number of queryV_len is less than cpu_count, Remark will select the number of queryV_len\n",
    "                            blastp_time_start = time.time()\n",
    "                            parallel_query = DivisionParallelQuery(queryV, 1, queryV_len, queryV_len)\n",
    "                            #1 is query_division_value. Because queryV_len / queryV_len( = cpu_count) is 1\n",
    "                            for m in range(queryV_len):\n",
    "                                process = multiprocessing.Process(target = RunParallelQuery, args = (i,k, parallel_query[m],m+1))\n",
    "                                #args(i => species of query, k = > species of subject , m+1 = > cpu_count ex) 1,2 ...)\n",
    "                                process_list.append(process)\n",
    "                                process.start()\n",
    "                            for n in process_list:\n",
    "                                n.join()\n",
    "                            blastp_time_end = time.time()\n",
    "                            print(\"The blastp & forward best hit searches took %.2f minutes\"%((blastp_time_end - blastp_time_start)/60))\n",
    "                        else:\n",
    "                            blastp_time_start = time.time()\n",
    "                            query_division_value = queryV_len / cpu_count\n",
    "                            parallel_query = DivisionParallelQuery(queryV, query_division_value , cpu_count, queryV_len)\n",
    "                            \n",
    "                            for m in range(cpu_count):\n",
    "                                process = multiprocessing.Process(target=RunParallelQuery, args= (i,k,parallel_query[m],m+1))\n",
    "                                #args(i =>species of query , k = > species of subject , m+1 => cpu_count ex)1,2 ...\n",
    "                                process_list.append(process)\n",
    "                                process.start()\n",
    "                            for n in process_list:\n",
    "                                n.join()\n",
    "                            blastp_time_end = time.time()\n",
    "                            print(\"The blastp & forward best hit searches took %.2f minutes\"%((blastp_time_end - blastp_time_start)/60))\n",
    "                            \n",
    "                    if not i == k:\n",
    "                        backward_best_hit_work_list.append((i,k,queryV_len))\n",
    "    elif \"2\" in mode:\n",
    "        print(\"BLASTP using precalculated data mode Running\")\n",
    "        for i in user_selected_number: #Select species to write query\n",
    "            queryV = QuerySequence(selected_species_dic[i])\n",
    "            for k in user_selected_number: #Select of subject\n",
    "                if Blastp_data + selected_species_dic[i]+\"_\"+selected_species_dic[k] + \"_oneway_threshold_best_hit_S\"+\\\n",
    "                str(threshold_score) in precalculated_data_list:\n",
    "                    used_precalculated_data_list.append(selected_species_dic[i]+\"_\"+selected_species_dic[k])\n",
    "                    continue\n",
    "                else:\n",
    "                    if k<i: # gene ===> query 1 -> 1 , 1 ->2  , 1 ->3 ,2 ->2 , 2->3\n",
    "                        continue\n",
    "                    else:\n",
    "                        print(\"Doing the blastp & Forward best hit searches between %s genome and %s genome\"%(selected_species_dic[i],selected_species_dic[k]))\n",
    "                        queryV_len = len(queryV)\n",
    "                                    \n",
    "                        if cpu_count ==1:\n",
    "                            blastp_time_start = time.time()\n",
    "                            RunParallelQuery(i,k,queryV, c)\n",
    "                            blastp_time_end = time.time()\n",
    "                            print(\"The blastp & forward best hit searches took %.2f minutes\"%\\\n",
    "                                  ((blastp_time_end-blastp_time_start)/60)) \n",
    "                        else:\n",
    "                            if queryV_len<cpu_count:\n",
    "                                #if the number of queryV_len is less than cpu_count, Remark will select the number of queryV_len.\n",
    "                                blastp_time_start = time.time()\n",
    "                                parallel_query = DivisionParallelQuery(queryV,1,queryV_len, queryV_len)\n",
    "                                #1 is query division_value. Because queryV_len /queryV_len( = cpu_count) is 1\n",
    "                                for m in range(queryV_len):\n",
    "                                    process = multiprocessing.Process(target= RunParallelQuery, args = (i,k,parallel_query[m],m+1))\n",
    "                                    #args (i => species of query, k = > species of subject, m+1 = > cpu_count e.x) 1,2,3...)\n",
    "                                    process_list.append(process)\n",
    "                                    process.start()\n",
    "                                    for n in process_list:\n",
    "                                        n.join()\n",
    "                                    blastp_time_end = time.time()\n",
    "                                    print(\"The blastp & forward best hit searches took %.2f minutes\"%\\\n",
    "                                          ((blsatp_time_end- blastp_time_start)/60))\n",
    "                                else:\n",
    "                                    blastp_time_start = time.time()\n",
    "                                    query_division_value = queryV_len / cpu_count\n",
    "                                    parallel_query = DivisionParallelQuery(queryV , query_division_value, cpu_count , queryV_len)\n",
    "                                    \n",
    "                                    for m in range(cpu_count):\n",
    "                                        process = multiprocessing.Process(target = RunParallelQuery, args = (i,k,parallel_query[m],m+1))\n",
    "                                        #args (i = > species of query, k = > species of subject, m+1 = > cpu_count ex ) 1,2,...\n",
    "                                        process_list.append(process)\n",
    "                                        process.start()\n",
    "                                    for n in process_list:\n",
    "                                        n.join()\n",
    "                                    blastp_time_end = time.time()\n",
    "                                    print(\"The blastp & forward best hit searches took %.2f minutes\"%((blastp_time_end - blastp_time_start)/60))\n",
    "                            new_calculated_data_list.append(selected_species_dic[i]+\"_\"+selected_species_dic[k])\n",
    "                            if not i ==k :\n",
    "                                backward_best_hit_work_list.append(i,k,queryV_len)\n",
    "    return backward_best_hit_work_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 10. Backward_Best_Hit(args):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Backward_Best_Hit(args):\n",
    "    species_of_query, species_of_subject, queryV_len = args\n",
    "    #Received 3 types of input argument\n",
    "    start_time_BBH = time.time()\n",
    "    forward_best_hit_score_list = []\n",
    "    blastp_score_split_list = []\n",
    "    print(\"Start to run the backward best hit between %s genome %genome\"%(selected_species_dic[species_of_query],selected_species_dic[species_of_subject]))\n",
    "    if queryV_len <cpu_count:\n",
    "        #if the number of queryV_len is less than cpu_count, the cpu_count is changed to queryV_len\n",
    "        for parallel_num in range(queryV_len):\n",
    "            parallel_num +=1\n",
    "            with open(Score_file+selected_species_dic[species_of_query]+\"_\"+selected_species_dic[species_of_subject]+\"_\"+\"best_score_S\"+\\\n",
    "                     str(threshold_score)+\"_\"+str(parallel_num),\"r\") as best_hit_score:\n",
    "                for each_line in best_hit_score:\n",
    "                    split_each_line = each_line.split(\" \")\n",
    "                    split_each_line[2] = int(split_each_line[2])\n",
    "                    split_each_line[3] = int(split_each_line[3])\n",
    "                    print(split_each_line)  #to check\n",
    "                    forward_best-hit_score_list.append(split_each_line)\n",
    "            with open(Score_file+selected_species_dic[species_of_query]+\"_\"+selected_species_dic[species_of_subject]+\"_\"+\"blastp_score_split_list_S\"+\\\n",
    "                     str(threshold_score)+\"_\"+str(parallel_num),\"r\") as blastp_score:\n",
    "                for each_line in blastp_score:\n",
    "                    split_each_line = each_line.split(\" \")\n",
    "                    split_each_line[2] = int(split_each_line[2])\n",
    "                    split_each_line[3] = int(split_each_line[3])\n",
    "                    print(split_each_line)\n",
    "                    blastp_score_split_list.append(split_each_line)\n",
    "    else:\n",
    "        for parallel_num in range(cpu_count):\n",
    "            parallel_num += 1\n",
    "            with open(Score_file+selected_species_dic[species_of_query]+\"_\"+selected_species_dic[species_of_subject]+\"_\"+\"best_score_S\"+\\\n",
    "                     str(threshold_score)+\"_\"+str(parallel_num),\"r\") as best_hit_score:\n",
    "                for each_line in best_hit_score:\n",
    "                    split_each_line = each_line.split(\" \")\n",
    "                    split_each_line[2] = int(split_each_line[2])\n",
    "                    split_each_line[3] = int(split_each_line[3])\n",
    "                    print(split_each_line)\n",
    "                    \n",
    "                    forward_best_hit_score_list.append(split_each_line)\n",
    "            with open(Score_file+selected_species_dic[species_of_query]+\"_\"+selected_species_dic[species_of_subject]+\"_\"+\"blastp_score_split_list_S\"+\\\n",
    "                     str(threshold_score)+\"_\"+str(parallel_num),\"r\") as balstp_score:\n",
    "                for each_line in blastp_score:\n",
    "                    split_each_line = each_line.split(\" \")\n",
    "                    split_each_line[2] = int(split_each_line[2])\n",
    "                    split_each_line[3] = int(split_each_line[3])\n",
    "                    print(split_each_line)\n",
    "                    blastp_score_split_list.append(split_each_line)\n",
    "    #bar = Bar(\"Searching: \"+selected_species_dic[species_of_query]+\"_\"+selected_species_dic[species_of_subject],max=len(forward_best_hit_score_list))\n",
    "    \n",
    "    for forward_best_hit_score_element in forward_best_hit_score_list:\n",
    "        matching_list = []\n",
    "        backward_best_score = [\"-1\",\"-1\",'-1']\n",
    "        bar.next()\n",
    "        for element in blastp_score_split:\n",
    "            if element[1] == forward_best_hit_score_element[1]:\n",
    "                matching_list.append(element)\n",
    "            for element in matching_list:\n",
    "                if int(element[2])> int(backward_best_score[2]):\n",
    "                    backward_best_score = element\n",
    "    #        with open('./'+selected_species_dic[species_of_query]+\"_\"+selected_species_dic[species_of_subject]+'_subtraction'+\"_\"+str(threshold_score), 'a') as subtraction :\n",
    "    #            save_data = int(backward_best_score[2]) - int(forward_best_hit_score_element[2])\n",
    "    #            subtraction.write(str(save_data)+\"\\n\")\n",
    "    \n",
    "        if  int(backward_best_score[2])-int(forward_best_hit_score_element[2])<=threshold_score :\n",
    "            #\n",
    "            with open(Score_file+selected_species_dic[species_of_query]+\"_\"+selected_species_dic[species_of_subject]+\"_oneway_threshold_best_hit_S\"+\\\n",
    "                     str(threshold_score),\"a\") as other_oneway_threshold_best_hit:\n",
    "                save_data = forward_best_hit_score_element[0]+\" \"+forward_best_hit_score_element[1]+\" \"+str(int(forward_best_hit_score_element[2]))+\"\\n\"\n",
    "                other_oneway_threshold_best_hit.write(save_data)\n",
    "    #bar.finish()\n",
    "    finish_time_BBH = time.time()\n",
    "    RBH_time = float((finish_time_BBH-start_time_BBH)/60)\n",
    "    print(\"BackwardBestHit of %s -%s took %.2f minutes\"%(selected_species_dic[species_of_query],selected_species_dic[species_of_subject],RBH_time))\n",
    "    return RBH_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 11. Search_Equal_BBH_Data(target_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  Search_Equal_BBH_Data(target_A):\n",
    "    \"\"\" Search the equal backward best hit data. ex --> AAE_AAE_backward_best_hit \"\"\"\n",
    "    put_data = equal_BBH_data_dic[target_A]\n",
    "    if put_data[1] ==0:\n",
    "        pass\n",
    "    else:\n",
    "        copy_put_data = copy.copy(put_data) #Copy Module to Copy \n",
    "        copy_put_data.insert(0,target_A)\n",
    "        result.put(copy_put_data)\n",
    "        equal_BBH_Data_dic[target_A][1] = 0\n",
    "        print(\"Put zero in Search_Equal_BBH_Data ----\",equal_BBH_data_dic[target_A])\n",
    "        for i in second_equal_BBH_data:\n",
    "            if i[2] == 0:\n",
    "                pass\n",
    "            else:\n",
    "                if i[0]==target_A or i[1] == target_A:\n",
    "                    copy_second_put_data = copy.copy(i)\n",
    "                    tasks.put(copy_second_put_data)\n",
    "                    #Don't put results as queue. Because the tasks will put copy_second_put_data to results as queue.\n",
    "                    i[2] ==0\n",
    "                    print(\"--put zero in second_equal_BBH_data--\",i)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  12. Search_Unequal_BBH_Data(target_B):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Search_Unequal_BBH_Data(target_B):\n",
    "    \"\"\" Search the unequal backward best hit data. ex) AAE_CAC_backward_best_hit\"\"\"\n",
    "    for i in unequal_BBH_data:\n",
    "        if i[2]==0:\n",
    "            pass\n",
    "        else:\n",
    "            if target_B[0] or target_B[0] == i[1] or target_B[1] == i[0] or target_B[1] == i[1]:\n",
    "                copy_i = copy.copy(i)\n",
    "                tasks.put(copy_i)\n",
    "                unequal_BBH_data[unequal_BBH_data.index(i)][2] = 0\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 13.Matching_BBH(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Matching_BBH(target):\n",
    "    \"match the backward best hit\"\n",
    "    if target[2] == 0:\n",
    "        return\n",
    "    else:\n",
    "        copy_target = copy.copy(target)\n",
    "        Search_Equal_BBH_Data(copy_target[0])\n",
    "        Search_Equal_BBH_Data(copy_target[1])\n",
    "        results.put(copy_target)\n",
    "        unequal_BBH_data[unequal_BBH_data.index(target)][2] =0\n",
    "    for j in unequal_BBH_data:\n",
    "        if j[2]==0:\n",
    "            pass\n",
    "        else:\n",
    "            if copy_target[0] or copy_target[0] ==j[1] or copy_target[1] ==j[0] or copy_target[1]==j[1]:\n",
    "                copy_j = copy.copy(j)\n",
    "                print(\"targ_get, j = %s %s (copy_target,j)\",j)\n",
    "                unequal_BBH_data[unequal_BBH_data.index(j)][2] =0\n",
    "                tasks.put(copy_j)\n",
    "        while not tasks.empty():\n",
    "            get_task = tasks.get()\n",
    "            Search_Equal_BBH_Data(get_task[0])\n",
    "            Search_Equal_BBH_Data(get_task[1])\n",
    "            results.put(get_task)\n",
    "            Search_Unequal_BBH_Data(get_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 14.Generating_Matrix_Clustering_Ortholog(element_set,bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generating_Matrix_Clustering_Ortholog(element_set,bar):\n",
    "    \"\"\"Generate the matrix of clustering ortholog \"\"\"\n",
    "    row_data = []\n",
    "    col_data = []\n",
    "    temp_results = queue.Queue()\n",
    "    bar.next()\n",
    "    for element in element_set:\n",
    "        if row_data.count(element[0])>0:\n",
    "            #if element[0] exist, returning the index in the row_data.\n",
    "            row = row_data.index(element[0])\n",
    "            # element[0] is data of row. ['gi|15606057|ref|NP_213434.1|', 'gi|15606057|ref|NP_213434.1|', '3823\\n']\n",
    "        else:\n",
    "            row = len(row_data)\n",
    "            row_data.append(element[0])\n",
    "            if col_data.count(element[0])<1:\n",
    "                #if element[0] doesn't exists, appending the element[0] to the col_data.\n",
    "                col_data.append(element[0])\n",
    "        if col_data.count(element[1])>0:\n",
    "            col = col_data.index(element[1]) #element[1] is data of col\n",
    "        else:\n",
    "            col = len(col_data)\n",
    "            col_data.append(element[1])\n",
    "            \n",
    "        temp_results.put([row,col,element[2]])\n",
    "        \n",
    "    score_metrix = numpy.matlib.zeros((len(row_data),len(col_data)),dtype = numpy.float)\n",
    "    #Create a new matrix of a given shape (the size_results) and type, filled with zeros.\n",
    "    while not temp_results.empty():\n",
    "        get_temp_results = temp_results.get()\n",
    "        row = get_temp_results[0]\n",
    "        col = get_temp_results[1]\n",
    "        score_matrix[row,col] = get_temp_results[2]\n",
    "        score_matrix[col,row] = get_temp_results[2]\n",
    "    if len(row_data)> 1000 and cpu-count >1:\n",
    "        #The big size of Matrix(bigger than 1000 X 1000) will be computed by Parallel_Matrix_Multiplication_Using_Numpy function\n",
    "        score_matrix = Parallel_MCL(score_matrix)\n",
    "        \n",
    "    else:\n",
    "        score_matrix = MCL(score_matrix)\n",
    "    Clustering(row_data,col_data, score_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 15.Parallel_MCL(score_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Parallel_MCL(score_matrix):\n",
    "    count = 0\n",
    "    infinitesimal_value = 10**-10\n",
    "    idempotent_matrix = numpy.matlib.ones((2,2)) #In newer Version np.ones\n",
    "    while idempotent_matrix.sum() > infinitesimal_value: \n",
    "        #Greater than infinitesimal _ value\n",
    "        MCL_time_start = time.time()\n",
    "        pool = multiprocessing.Pool(cpu_count)\n",
    "        #multiprocessing.Pool(processes=None,initializer=None,initargs=(),maxtasksperchild=None,) PARAMETER\n",
    "        multipplication_results = pool.map(Parallel_Matrix_Multiplication_Using_Numpy, zip(score_matrix, repeat(score_matrix)))\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        \n",
    "        pool = multiprocessing.Pool(cpu_count) # Create a inflation_matrix (part 1)\n",
    "        power_results = pool.map(Parallel_Matrix_Power_Usint_Numpy, multipplication_results)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        \n",
    "        sum_matrix = 0\n",
    "        for i in power_results:\n",
    "            sum_matrix = i+ sum_matrix\n",
    "        pool = multiprocessing.Pool(cpu_count) #Create a inflation_matrix (part 2)\n",
    "        divide_results = pool.map(Parallel_Matrix_Power_Usint_Numpy, zip(power_results, repeat(sum_matrix)))\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        \n",
    "        for i in range(len(divide_results)):\n",
    "            #Make a Combined matrix for results of Parallel_Matrix_Multiplication_Using_Numpy Function.\n",
    "            if i ==0:\n",
    "                score_matrix = divide_results[i]\n",
    "            else:\n",
    "                score_matrix = numpy.concatenate((score_matrix, divide_results[i]),axis =0)\n",
    "        sum_results = 0\n",
    "        for i in multiplication_results:\n",
    "            sum_results += i\n",
    "        idempotent_matrix = abs(numpy.sum(score_matrix) - sum_results)\n",
    "        #identify whether inflation_matrix is idempotent matrix or not\n",
    "        count +=1\n",
    "        if count > infinite_loop:\n",
    "            #It will prevent the infinite loop of MCL algorithms.\n",
    "            break\n",
    "        MCL_time_finish = time.time()\n",
    "        if verbose:\n",
    "            print(\"MCL time %f, count :%d, matrix size :%d *%d \"%((MCL_time_finish-MCL_time_start)/60),count,score_matrix[0].size)\n",
    "    return score_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 16. MCL(score_matrix)\n",
    "A^2 = A. In linear  algebra, an idempotent matrix is a matrix which, when multiplied by itself, yields itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MCL(score_matrix):\n",
    "    \"This function run MCL.....\"\n",
    "    print(\"MCL is Runing\")\n",
    "    count = 0\n",
    "    infinitesimal_value = 10*-10\n",
    "    idempotent_matrix = numpy.matlib.ones((2,2)) #np.ones((2,2))\n",
    "    while idempotent_matrix.sum() > infinitesimal_value: # sum is 4\n",
    "        MCL_time_start = time.time()\n",
    "        expansion_matrix = score_matrix ** 2\n",
    "        score_matrix = numpy.power(expansion_matrix, inflation_factor)\n",
    "        #np.power(matrix, int)\n",
    "        score_matrix_sum = score_matrix.sum(axis = 0)\n",
    "        score_matrix = numpy.divide(score_matrix,score_matrix_sum) # Create a inflation_Matrix\n",
    "        idempotent_matrix = abs(score_matrix - expansion_matrix)\n",
    "        #identify weather inflation_matrix is idempotent matrix or not\n",
    "        count +=1\n",
    "        if count > infinite_loop:\n",
    "            #It will prevent the infinite loop of MCL algorithm.\n",
    "            break\n",
    "        MCL_time_finish = time.time()\n",
    "        if verbose:\n",
    "            print(\"MCL time:%f, Count: %d , matrix size: %d *%d\"%((MCL_time_finish-MCL_time_start)/60,count, score_matrix[0].size,\\\n",
    "                                                                 score_matrix[0].size))\n",
    "            \n",
    "    return score_matrix\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 17. Clustering(row_data, col_data, score_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Clustering(row_data, col_data, score_matrix):\n",
    "    global cluster_count, ortholog_count\n",
    "    ortholog_temp_list = []\n",
    "    for i in range(len(row_data)):\n",
    "        ortholog_list = []\n",
    "        ortholog_sum = 0\n",
    "        ortholog = queue.Queue() #It is Queue which is put the orhtolog.\n",
    "        gene_id_queue = queue.Queue() # It is Queue which is put the orhtolog having changed gene ID\n",
    "        for j in range(len(col_data)):\n",
    "            if 0.1 <= score_matrix[i,j]:\n",
    "                ortholog.put(col_data[j])\n",
    "                ortholog_list.append(col_data[j])\n",
    "    if ortholog.qsize() >=3: # If the ortholog queue has the element of ortholog more than2, it will be printed.\n",
    "        for element in ortholog_list:\n",
    "            try:\n",
    "                ortholog_sum += ortholog_temp_list.index(element) + 1\n",
    "            except ValueError:\n",
    "                with open(Cluster_out+\"_geneID_S\"+str(threshold_score)+\"_\"+str(inflation_factor),\"a\") as ortholog_list_save:\n",
    "                    ortholog_print = \"cluster\"+str(cluster_count)+\":\"\n",
    "                    ortholog_list_save.write(ortholog_print)\n",
    "                    \n",
    "                    while not ortholog.empty():\n",
    "                        get_ortholog = ortholog.get()\n",
    "                        ortholog_list_save.write(\"\\t\"+get_ortholog)\n",
    "                        try:\n",
    "                            get_ortholog_split = get_ortholog.split(\"_\") #get_ortholog --> ECO_170082288, get_ortholog.split(\"_\")\n",
    "                            gene_id_que.put(get_ortholog_split[0]+\"_\"+gene_id_dic[get_ortholog_split[1]])\n",
    "                        except KeyError:\n",
    "                            gene_id_que.put(get_ortholog) #If the gene_id_dic dont have get_ortholog, it will print the original ID(get_ortholog).\n",
    "                    ortholog_list_save.write(\"\\n\")\n",
    "                with open(Cluster_out +\"_KO_ID_S\"+str(threshold_score)+\"_\"+str(inflation_factor),\"a\") as ortholog_list_geneID_save:\n",
    "                    \n",
    "                        ortholog_list_geneID_print = \"cluster \"+ str(cluster_count)+\" :\"\n",
    "                        ortholog_list_geneID_save.write(ortholog_list_geneID_print)\n",
    "                        while not gene_id_queue.empty():\n",
    "                            ortholog_list_geneID_save.write(\"\\t\"+gene_id_queue.get())\n",
    "                            ortholog_count +=1\n",
    "                            cluster_count +=1\n",
    "                            ortholog_list_geneID_save.write(\"\\n\")\n",
    "                        break\n",
    "                ortholog_temp_list = operator.concat(ortholog_temp_list, ortholog_list)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 18.Parallel_Matrix_Multiplication_Using_Numpy(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Parallel_Matrix_Multiplication_Using_Numpy(data):\n",
    "    \n",
    "    matrix_element, matrix  = data\n",
    "    #mul = numpy.multiply(matrix_element,matrix)\n",
    "    mul = matrix_element * matrix\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 19.Parallel_Matrix_Power_Using_Numpy(matrix_element):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Parallel_matrix_Power_Using_Numpy(matrix_element):\n",
    "    \"This Function return Metrix Power by inflation_factor\"\n",
    "    power_matrix_element = numpy.power(matrix_element, inflation_factor)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 20.Parallel_Matrix_Divide_Using_Numpy(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Parallel_Matrix_Divide_Using_Numpy(data):\n",
    "    \"data is 2 values. First is matrix_element, sum_data.\"\n",
    "    matrix_element , sum_data = data\n",
    "    return numpy.divide(matrix_element, sum_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 21. Read_Species_List()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "def Read_Species_List(pr=0):\n",
    "    \"\"\" If pr is 1, it will print \"Species_List\". \"\"\" \n",
    "    global species\n",
    "    read_species =glob.glob(species+\"*\")   \n",
    "    selected_species_dic = {}\n",
    "    backward_selected_species_dic = {}\n",
    "    #number = 0\n",
    "    for i, spec in enumerate(sorted(read_species), start=1):\n",
    "        #variable name spec is required because local Variable and global Variable can cause Conflict\n",
    "        selected_species_dic[i] = spec.split('\\\\')[-1]\n",
    "        backward_selected_species_dic[spec.split('\\\\')[-1]] = i \n",
    "        if pr == 1 :\n",
    "            print (str(i)+\".\", spec.split('\\\\')[-1])\n",
    "        number = i\n",
    "    return selected_species_dic, backward_selected_species_dic, number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_species(pr =1):\n",
    "    import os\n",
    "    read_species = os.listdir(\"species/\")\n",
    "    for i,species in enumerate(read_species,start =1):\n",
    "        print(i,species)\n",
    "#read_species()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 22. Del_File(path,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Del_File(path,file):\n",
    "    import subprocess\n",
    "    \n",
    "    del_file = subprocess.Popen([\"del \"+path+file],stdout=subprocess.PIPE, stderr = subprocess.PIPE,shell= True)\n",
    "    del_file_stream = del_file.communicate()\n",
    "    if not del_file_stream[1]:\n",
    "        print(\"Done to del\"+path+file)\n",
    "    elif del_file_stream[1]:\n",
    "        print(del_file_stream[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i developed the below Function to remove the file may be different but will check later "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_(file):\n",
    "    import os\n",
    "    try:\n",
    "        os.remove(file)\n",
    "        print(\"Successfully_remove\",file)\n",
    "    except:\n",
    "        print(\"Check the File And Path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 23.Check_File(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Check_File(file):\n",
    "    \"Check the file weather exist or not\"\n",
    "    file_list = glob.glob(file+'*')    \n",
    "    if (Cluster_out+\"_geneID_S\"+str(threshold_score)+\"_\"+str(inflation_factor) or Cluster_out+\"_KO_ID_S\"+str(threshold_score)+\"_\"+str(inflation_factor)) in file_list:\n",
    "        print (\"Please, set other name of output.\")\n",
    "        sys.exit(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 24.Read_Equal_BBH(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Read_Equal_BBH(path):\n",
    "    \"Read_Equal BBH by user path\"\n",
    "    global threshold_score\n",
    "    with open(path + \"_oneway_threshold_best_hit_S\"+str(threshold_score),\"r\") as equal_RBH:\n",
    "        for j in equal_RBH:\n",
    "            split_data = j.split()\n",
    "            split_data[2] = int(split_data[2])\n",
    "            equal_BBH_data.append(split_data)\n",
    "            equal_BBH_data_dic[split_data[0]] = split_data[1:]\n",
    "    try:\n",
    "        with open(path+\"_second_oneway_threshold_best_hit_S\"+str(threshold_score),\"r\") as second_equal_RBH:\n",
    "            for j in second_equal_RBH:\n",
    "                split_data = j.split()\n",
    "                split_data[2] = int(split_data[2])\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 25. Read_Unequal_BBH(path):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Read_Unequal_BBH(path):\n",
    "    \"Read Unequal BBH path passed by User\"\n",
    "    with open(path+\"_oneway_threshold_best_hit_S\"+str(threshold_score),\"r\") as unequal_RBH:\n",
    "        for j in unequal_RBH:\n",
    "            split_data[2] = int(split_data[2])\n",
    "            unequal_BBH_data.append(split_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Main Program Testing\n",
    "`************* No Function below **************************`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arg parse is not done here\n",
    "# If parameter not Provided the below code will run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if not sys.argv[1:]\n",
    "import sys\n",
    "import multiprocessing\n",
    "\n",
    "# If Program not Passed with Parameter\n",
    " # if Not verified Conflict arise for local and global Variable\n",
    "print (\"1. BLASTP. \\n2. BLASTP using precalculated data. \\n3. Clustering.\\n\")\n",
    "mode = input(\">> Select a mode or modes (1 or 2 or 1 3 or 2 3): \") \n",
    "selected_species_dic, backward_selected_species_dic, number_i = Read_Species_List(pr=1)\n",
    "selected_number = input(\">> Select Genomes to detect Orthologs(e.g. 1 2 3 4 5 or 1-5) : \")\n",
    "\n",
    "\n",
    "if selected_number.find(\"-\")>0:\n",
    "    SN = selected_number.split(\"-\")\n",
    "    if int(SN[-1])>number_i:\n",
    "        print(\"Your input must be less than\" , number_i)\n",
    "        sys.exit(2)\n",
    "    else:\n",
    "        user_selected_number = range(int(SN[0]),int(SN[-1])+1)\n",
    "        print(user_selected_number)\n",
    "        for j in user_selected_number:\n",
    "            print(selected_species_dic[j],end=\" \")\n",
    "        print(\"Are Selected\")\n",
    "else:\n",
    "    user_selected_number = sorted(set([int(read_species) for read_species in selected_number.split()]))\n",
    "    if int(user_selected_number[-1])>number_i:\n",
    "        print (\"\\nWrongInput\\nInput must be less than\",number_i)\n",
    "        sys.exit(2)\n",
    "    else:\n",
    "        for j in user_selected_number:\n",
    "            print(selected_species_dic[j], end =\" \")\n",
    "\n",
    "blastp_matrix = MatrixName()\n",
    "cpu_count = int(input(\"You can use %s processors. \\nIf you input >2, The Program will run a parallel--\"\\\n",
    "                      %multiprocessing.cpu_count()+\"Enter the number of CPU you want to use :\"))\n",
    "if \"3\" in mode:\n",
    "    inflation_factor = input(\"Enter the inflation factor to cluster\")\n",
    "    Cluster_out = input(\"Set the name of Clustering output :\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### if Paramater Passed with Python Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##elif sys.argv[1:]\n",
    "##command_options = argparse_value\n",
    "#genomes = command_options.genomes\n",
    "#mode = command_options.mode\n",
    "#cpu_count= command_options.cpu_count\n",
    "#blastp_matrix = command_options.blastp_matrix\n",
    "#inflation_factor = command_options.inflation_factor\n",
    "#selected_species_dic,backward_selected_species_dic, number_i = Read_Species_List()\n",
    "#user_selected_number = [backward_selected_species_dic[ele] for ele in genomes]\n",
    "#Cluster_out = command_options.Cluster_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Species = command_options.Species\n",
    "#Blastp = COmmand_options.Blastp\n",
    "#Score_file = command_options.Score_file\n",
    "#Blastp_data = command_options.Blastp_data\n",
    "#save_raw_blastp_score = command_options.threshold_score\n",
    "#threshold_score = command_options.threshold_score\n",
    "#verbose = command_options.verbose\n",
    "#infinite_loop = command_options.infinite_loop\n",
    "#\n",
    "#if \"3\" in mode:\n",
    "#    Check_File(Cluster_out)\n",
    "#Del_File(score_file, \"*\")\n",
    "#if \"3\" in mode:\n",
    "#    Log_file_name = Cluster_out+\"_S\"+str(threshold_score)+\"_\"+str(inflation_factor)+\".log\"\n",
    "#elif not \"3\" in mode:\n",
    "#    Log_file_name = \"Log\"\n",
    "#\n",
    "#with open(Log_file_name, \"w\") as log:\n",
    "#    log.write(str(datetime.date_time.now()))\n",
    "#    log.write(\"\\nmode\")\n",
    "#    for i in mode:\n",
    "#        log.write(\" \"+i)\n",
    "#    log.write(\"\\ngenomes:\")\n",
    "#    for i in user_selected_number:\n",
    "#        log.write(selected_species_dic[i]+\" \")\n",
    "#    log.write(\"\\nCPU_Count:\"+str(cpu_count))\n",
    "#    log.write(\"\\nblastp matrix :\"+blastp_matrix)\n",
    "#    if \"3\" in mode:\n",
    "#        log.write(\"\\ninflation_factor: \"+str(inflation_factor))\n",
    "#        log.write(\"\\nCluster out:\"+Cluster_out)\n",
    "#    log.write(\"\\nSpecies :\"+Species)\n",
    "#    log.write(\"\\nBlastp: \"+Blastp)\n",
    "#    log.write(\"\\nScore file : \"+ Score_file)\n",
    "#    log.write(\"\\nBlastp_data : \"+ Blastp_data)\n",
    "#    log.write(\"\\nsave rawblastp score : \"+ str(save_raw_blastp_score))\n",
    "#    log.write(\"\\n\")\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_OBH = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"1\" in  mode:\n",
    "    backward_best_hit_work_list = Oneway_Threshold_Best_Hit(mode)\n",
    "    pool = multiprocessing.Pool(cpu_count)\n",
    "    results = pool.map(Backward_Best_Hit, backward_best_hit_work_list)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "elif \"2\" in mode:\n",
    "    used_precalculated_data_list = []\n",
    "    new_calculated_data_list = []\n",
    "    precalculated_data_list = glob.glob(Blastp_data+\"*oneway_threshold_best_hit_S\"+str(threshold_score))\n",
    "    print(precalculated_data_list)\n",
    "    backward_best_hit_work_list = Oneway_Threshold_Best_Hit(mode)\n",
    "    \n",
    "    if not backward_best_hit_work_list ==[]:\n",
    "        #If backward_best_hit_work_list is an empty list, pool instance can't finish the work.\n",
    "        pool = multiprocessing.Pool(cpu_count)\n",
    "        results = pool.map(Backward_Best_Hit, backward_best_hit_work_list)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "    else:\n",
    "        results = [0,0]\n",
    "Del_File(\"./\",\"query*\")\n",
    "finish_time_OBH = time.time()\n",
    "blastp_time_log = float(((finish_time_OBH - start_time_OBH)/60))\n",
    "print(\"BLASTP searches + forward best Hit + backwardbest hit took %f minutes\"%blastp_time_log)\n",
    "\n",
    "with open(Log_file_name , \"a\") as log:\n",
    "    log.write(\"Backward_Best_Hit took \"+str(max(results))+\" minutes\\n\")\n",
    "    log.write(\"BLASTP + Best_Hit + backward_best_hit searches took\"+ str(blastp_time_log)+\" minutes\\n\")\n",
    "if \"3\" in mode:\n",
    "    start_time_clustering = time.time()\n",
    "    #generate matrix calculate the matrix using MCL algorithm and cluster the Ortholog\n",
    "    \n",
    "    print(\"\\n >>> Start MCL algorithm and Clustering ortholog <<<\")\n",
    "    equal_BBH_data = []\n",
    "    unequal_BBH_data = []\n",
    "    equal_BBH_data_dic = {}\n",
    "    second_equal_BBH_data = []\n",
    "    results = que.Queue()\n",
    "    tasks = queue.Queue()\n",
    "    cluster_count = 1\n",
    "    ortholog_count = 0\n",
    "    gene_id_dic = {}\n",
    "    \n",
    "    with open(\"myva=gb\",\"r\") as id_read:\n",
    "        #myvba=gb is a database\n",
    "        for i in id_read:\n",
    "            gene_name,gene_id = i.split()\n",
    "            gene_id_dic[gene_id.replace(\"\\n\",\"\")] = gene_name # remove '\\n'\n",
    "            \n",
    "        if \"1\" in mode:\n",
    "            for i in user_selected_number:\n",
    "                for k in user_selected_number:\n",
    "                    if k<i:\n",
    "                        pass\n",
    "                    elif i ==k:\n",
    "                        Read_Equal_BBH(Score_file+selected_species_dic[i]+\"_\"+selected_species_dic[k])\n",
    "                    elif i !=k:\n",
    "                        Read_Unequal_BBH(Score_file+selected_species_dic[i]+\"_\"+selected_species_dic[k])\n",
    "        elif \"2\" in mode:\n",
    "            for used_data in used_precalculated_data_list:\n",
    "                first,second = used_data.split(\"_\")\n",
    "                if first == second:\n",
    "                    Read_Equal_BBH(Blastp_data+used-data)\n",
    "                elif first != second:\n",
    "                    Read_Unequal_BBH(Blastp_data+used_data)\n",
    "            for new_data in new_calculated_data_list:\n",
    "                first,second = new_data.split(\"_\")\n",
    "                if first== second:\n",
    "                    Read_Equal_BBH(Score_file+new_data)\n",
    "                elif first != second:\n",
    "                    Read_Unequal_BBH(Score_file+new_data)\n",
    "        matched_BBH_data = []\n",
    "        matched_BBH_element_data_set = []\n",
    "        \n",
    "        for unequal_RBH_element in unequal_BBH_data:\n",
    "            Matching_BBH(unequal_RBH_element)\n",
    "            temp_results_list = []\n",
    "            if results._qsize() !=0: #return the number of results as Queue.\n",
    "                while not results.empty():\n",
    "                    get_results = results.get()\n",
    "                    temp_results_list.append(get_results)\n",
    "                matched_BBH_data.append(temp_results_list)\n",
    "        #bar = bar(\"Processing\", max= len(matched_BBH_data))\n",
    "        #bar = bar(\"Processing\", max= len(matched_BBH_data)) #Not Supported in Python 3\n",
    "        #for data in matched_BBH_data:\n",
    "        #    Generating_Matrix_Clustering_Ortholog(data, bar)\n",
    "        #bar.finish()\n",
    "        finish_time_clustering = time.time()\n",
    "        mcl_time_log = float((finish_time_clustering - start_time_clustering)/60)\n",
    "        remark_time_log = float((finish_time_clustering- start_time_OBH)/60)\n",
    "        print(\"MCL algorithm and Ortholog Clustering took %.2f minutes\"%remark_time_log)\n",
    "        \n",
    "        if \"3\" in mode:\n",
    "            with open(Log_file_name,\"a\") as log:\n",
    "                log.write(\"Ortholog Count: \"+str(ortholog_count)+\",\"+\"Cluster count: \"+ str(cluster_count-1)+\"\\n\")\n",
    "                log.write(\"MCL algorithm and Ortholog Clustering took \"+str(mcl_time_log)+\"minutes\\n\")\n",
    "                log.write(\"xxx Program took \"+str(remark_time_log)+\"minutes\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
