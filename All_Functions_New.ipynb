{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import copy\n",
    "import operator\n",
    "import glob\n",
    "#import pprint\n",
    "import argparse\n",
    "import datetime\n",
    "import queue\n",
    "import multiprocessing\n",
    "from alive_progress import alive_bar\n",
    "import numpy.matlib\n",
    "import numpy as np     # For Mathmatical (Algebra) Operation\n",
    "from sys import platform   # To verify the Operating Sys Win or Linux\n",
    "from Bio import SeqIO  # For Bio Python Sequence Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Blastp = /usr/bin/blastp\n",
    "Blastp = \"blastp\"\n",
    "Blastp_data =  './blastp_data/'\n",
    "cpu_count = 1 # Default value\n",
    "blastp_matrix = \"BLOSUM62\" \n",
    "genome = \"\"\n",
    "infinite_loop = 70\n",
    "inflation_factor = 1.4\n",
    "#mode = None\n",
    "mode = '1' #Change Later\n",
    "Cluster_out = \"./cluster_out/\"\n",
    "#threshold_score : 0\n",
    "threshold_score =0\n",
    "Save_raw_blastp_score = False\n",
    "Score_file = \"./score_file\"\n",
    "Species = \"./species/\"\n",
    "species = \"./species/\"\n",
    "verbose = False\n",
    "Log_file_name = \"log_files/\"\n",
    "save_raw_blastp_score = \"score_file/\"\n",
    "\n",
    "mode = '1'\n",
    "matrix = \"BLOSUM62\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Calling the Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1. matrix_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ],
    "lang": "en"
   },
   "outputs": [],
   "source": [
    "def matrix_name():\n",
    "    \"\"\"This Function will Return the Matrix name choosed by User.\n",
    "    BLOSUM45 ,  BLOSUM62 , BLOSUM82 \"\"\"\n",
    "    print(\"BLOcks SUbstitution Matrix (BLOSUM) is a Substitution matrix used for sequence alignment of Proteins\")\n",
    "    print(\"\"\"\\n1. BLOSUM45 :-For more distantly related Proteins alignment DataBase\\n2. BLOSUM62 :- MidRange Seq with more than 62%similarity\\\n",
    "         \\n3. BLOSUM82 :- More related Proteins\\nOther Keys will exit the Program\"\"\")\n",
    "    metrix_num = input(\"\\nEnter a matrix number: \")\n",
    "    if metrix_num not in (\"1\", \"2\", '3'):\n",
    "        print(\"Wrong input *%s*Sorry not in list\\n\" %\n",
    "              metrix_num, \"*\"*20, \"Good Bye\", \"*\"*20, \"\\n\")\n",
    "        sys.exit(1)\n",
    "    if metrix_num == \"1\":\n",
    "        return \"BLOSUM45\"\n",
    "    if metrix_num == \"2\":\n",
    "        return \"BLOSUM62\"  # 62 is not availiable currently\n",
    "    if metrix_num == \"3\":\n",
    "        return \"BLOSUM82\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###### 2. Query_Sequence(genome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def Query_Seq(genome):\n",
    "    \"\"\"This Function Read Fasta (Genome) file and return as a list Format with gene Position and Sequence Developed by Krish\"\"\"\n",
    "    try:\n",
    "        return [str((seq_record.id+\"\\n\"+seq_record.seq)) for seq_record in SeqIO.parse(genome, \"fasta\")]\n",
    "        # To understand this Function Bio Python library needs to be studied\n",
    "    except IOError as err:\n",
    "        print(str(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def query_sequence(genome):\n",
    "    \"\"\"This Function Read Fastaq Files and return as a list Format with gene Position as a index\n",
    "    This Function create a list of Seprate Query Sequence\"\"\"\n",
    "    # This Function is created by using Bio Python we will test later\n",
    "    gene_seq = \"\"\n",
    "    gene_seq_list = []\n",
    "    try:\n",
    "        with open(Species+genome) as gene:\n",
    "            for each_line in gene:\n",
    "                if \">\" in each_line:\n",
    "                    if gene_seq != \"\":\n",
    "                        gene_seq_list.append(gene_seq)\n",
    "                        gene_seq = \"\"\n",
    "                gene_seq = gene_seq+each_line\n",
    "            gene_seq_list.append(gene_seq)\n",
    "            #print(\"query_sequence() Run Successfully\")\n",
    "            return gene_seq_list\n",
    "\n",
    "    except IOError as err:\n",
    "        print(\"IOError occurred in query_sequence function : \" + str(err))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Query_Sequence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-eecc76104a3c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mQuery_Seq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSpecies\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"AAE\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mb\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mQuery_Sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"AAE\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'Query_Sequence' is not defined"
     ]
    }
   ],
   "source": [
    "a = Query_Seq(Species+\"AAE\")\n",
    "b= Query_Sequence(\"AAE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "display(a[1])\n",
    "display(b[1])\n",
    "a[1]==b[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3.write_query(query, parallel_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def write_query(query, parallel_num):\n",
    "    \"This Function Write Query with file Name query+ parallel_num in same directory and raise IO error if Error rises\"\n",
    "    # print(\"write_query(query,parallel_num)\")\n",
    "    try:\n",
    "        with open(\"./query/query_\"+str(parallel_num), \"w\") as write_query:\n",
    "            write_query.write(query)\n",
    "    except IOError as err:\n",
    "        print(\"IOError occurred in write_query function : \" + str(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 4. Run_Blast( Subject , parallel_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def run_blast(subject, parallel_num):\n",
    "    \"\"\"By this Function it will create a Pipe line to run Blastp in Computer by input Parameter Subject is whole Genome\n",
    "         and parallel_number is query file Created by early step.to run a big file it is time Consuming. So reduce a File size and run\n",
    "         The Output Format 10 --> Comma Separated Values \n",
    "         qseqid --> Query Seq ID\n",
    "         ssequid --> Subject of Seq. id ID\n",
    "         Score -->Raw Score\n",
    "         length-->Alignment length\"\"\"\n",
    "\n",
    "    subject = Species+subject  # later remove this\n",
    "    cmd = [\"blastp\", \"-query\", \"./query/query_\"+str(parallel_num), \"-subject\", subject,\n",
    "           \"-matrix\", blastp_matrix, \"-outfmt\", \"10 qseqid sseqid score length\"]\n",
    "\n",
    "    # query subject is inside query Folder\n",
    "    #run_blastp =subprocess.Popen(cmd,stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    #run_blastp = subprocess.run(cmd,shell=True ,capture_output = True,text = True)\n",
    "    # output Format 10 qseqid query (e.g. gene sequence id  ,  sseqid subject (e.g. reference genome) genome id)\n",
    "    run_blastp = subprocess.Popen(\n",
    "        cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    #run_blastp_stream = run_blastp.stdout()\n",
    "    run_blastp_stream = run_blastp.communicate()\n",
    "    run_blastp_output_stream = run_blastp_stream[0]\n",
    "    #run_blastp_error_stream = run_blastp_stream[1]\n",
    "\n",
    "    # Later We can return by this\n",
    "    # return run_blastp.communicate[0]\n",
    "    return run_blastp_output_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def Run_Blast1(subject, parallel_num):\n",
    "    \"By this Function it will create a Pipe line to run Blastp in Computer by input Parameter Subject is whole Genome and parallel_number is query file Created by early step\"\n",
    "    #print(\"Runing Run_Blast(subject,parallel_number)\")\n",
    "    # print(\"Passed Parallel Number is \",parallel_num) #parallel_number is the cpu_count\n",
    "    subject = Species+subject\n",
    "    cmd = [\"blastp\", \"-query\", \"./query/query_\"+str(parallel_num), \"-subject\", subject,\n",
    "           \"-matrix\", blastp_matrix, \"-outfmt\", \"10 qseqid sseqid score length\"]\n",
    "    run_blastp = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    \n",
    "    return run_blastp.communicate()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 5. same_species_forward_best_hit(blastp_score):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "lang": "en"
   },
   "outputs": [],
   "source": [
    "def same_species_forward_best_hit(blastp_score):\n",
    "    \"\"\"Search the forward best hit among the blastp scores of same species.\n",
    "    Because there are an duplicated genes in a same genome.\n",
    "    blstp_score file is return value of run_blast() Function\n",
    "    When the blastp score compare with blastp score of duplicate gene, if score and length are same, blasp score of duplicated gene is added to a second best score.\"\"\"\n",
    "    #print(blastp_score)\n",
    "    blastp_score_split_list = []\n",
    "    temp_best_score = ['-1', '-1', '-1']\n",
    "    second_temp_best_score = []\n",
    "    best_score = []\n",
    "    second_best_score = []\n",
    "    blastp_score_split = blastp_score.split(\"\\n\")\n",
    "    # delete of [''] in the last index  ex) ['gi,gi,1,1','gi,gi,2,2','']\n",
    "    del blastp_score_split[-1]\n",
    "    for i in blastp_score_split:\n",
    "        blastp_score_element = i.split(',')\n",
    "        blastp_score_split_list.append(blastp_score_element)\n",
    "    # ex) k is ['gi|15605613|ref|NP_212986.1|', 'gi|15605613|ref|NP_212986.1|', '3702', '699']\n",
    "    for k in blastp_score_split_list:\n",
    "        if k[0] == k[1]:  # if the Position (gene) is Same\n",
    "            best_score.append(k)\n",
    "        elif k[0] != k[1]:\n",
    "            if int(k[2]) > int(temp_best_score[2]):  # Compare score\n",
    "                temp_best_score = k\n",
    "            elif int(k[2]) == int(temp_best_score[2]):  # Is Equal\n",
    "                if int(k[3]) > int(temp_best_score[3]):  # compare length\n",
    "                    temp_best_score = k\n",
    "                elif int(k[3]) == int(temp_best_score[3]):\n",
    "                    second_temp_best_score.append(k)\n",
    " #    print (\"############ temp best score ############\", temp_best_score)\n",
    "    second_best_score.append(temp_best_score)\n",
    "    for j in second_temp_best_score:\n",
    "        if j[2] == temp_best_score[2] and j[3] == temp_best_score[3]:\n",
    "            second_best_score.append(j)\n",
    "    for m in second_best_score:\n",
    "        if (best_score[0][2] == m[2] and int(best_score[0][3]) <= int(m[3])) or int(best_score[0][2]) < int(m[2]):\n",
    "             # '104' < '23' is True because of string. So the int function is used.\n",
    "            best_score.append(m)\n",
    "    #print(\"best Score is\", best_score)\n",
    "    return best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same_Species_Forward_Best_HIt(a)\n",
    "#a = str(blastp_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 6. forward_best_hit(blastp_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def forward_best_hit(blastp_score):\n",
    "    \"\"\"Search the forward best hit among the blastp scores of same species.\"\"\"\n",
    "    #print(\"Rnunning GetForward BestHit\")\n",
    "    blastp_score_split_list = []\n",
    "    temp_best_score = ['-1', '-1', '-1']\n",
    "    second_temp_best_score = []\n",
    "    best_score = []\n",
    "    blastp_score_split = blastp_score.split(\"\\n\")\n",
    "    # delete of ['']   ex) ['gi,gi,1,1','gi,gi,2,2','']\n",
    "    del blastp_score_split[-1]\n",
    "    for i in blastp_score_split:\n",
    "        blastp_score_element = i.split(',')\n",
    "        blastp_score_split_list.append(blastp_score_element)\n",
    "\n",
    "    for k in blastp_score_split_list:\n",
    "        # ex) k is ['gi|15605613|ref|NP_212986.1|', 'gi|15605613|ref|NP_212986.1|', '3702', '699']\n",
    "        # print \">>>>>>>>>>>>>>>forward_best_hit   k\", k\n",
    "        if int(k[2]) > int(temp_best_score[2]):  # Compare the Score of the Blast\n",
    "            temp_best_score = k\n",
    "        elif int(k[2]) == int(temp_best_score[2]):\n",
    "            if int(k[3]) > int(temp_best_score[3]):  # Compare the Lenth of Sequence\n",
    "                temp_best_score = k\n",
    "            elif int(k[3]) == int(temp_best_score[3]):\n",
    "                second_temp_best_score.append(k)\n",
    "  #print \"############ temp best score ############\", temp_best_score\n",
    "    best_score.append(temp_best_score)\n",
    "    for j in second_temp_best_score:\n",
    "        if j[2] == temp_best_score[2] and j[3] == temp_best_score[3]:\n",
    "            best_score.append(j)\n",
    "    return best_score, blastp_score_split_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a,b  = Forward_Best_HIt(blastp_score.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 7. division_parallel_query(queryV,query_division_value, cpu_count , queryV_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def division_parallel_query(query_v, query_division_value, cpu_count, query_v_len):\n",
    "    \"query_v is list of queries, query_division_value is , cpu_count is the Number of CPU User Entered and query_v_len is the length of the query\"\n",
    "    parallel_query = []\n",
    "    parallel_query_start = 0\n",
    "    if query_v_len % cpu_count == 0:  # perfect division\n",
    "        for i in range(cpu_count):\n",
    "            i += 1\n",
    "            if parallel_query_start == 0:\n",
    "                parallel_query.append(\n",
    "                    query_v[int(parallel_query_start):i*int(query_division_value)])\n",
    "                parallel_query_start += 1\n",
    "            else:\n",
    "                parallel_query.append(query_v[int(\n",
    "                    parallel_query_start)*int(query_division_value):i*int(query_division_value)])\n",
    "                parallel_query_start += 1\n",
    "    else:  # imperfect division\n",
    "        for i in range(cpu_count):\n",
    "            i += 1\n",
    "            if parallel_query_start == 0:\n",
    "                parallel_query.append(\n",
    "                    query_v[int(parallel_query_start):i*int(query_division_value)])\n",
    "                parallel_query_start += 1\n",
    "            elif i < cpu_count:\n",
    "                parallel_query.append(query_v[int(\n",
    "                    parallel_query_start)*int(query_division_value):i*int(query_division_value)])\n",
    "                parallel_query_start += 1\n",
    "            elif i == cpu_count:\n",
    "                parallel_query.append(\n",
    "                    query_v[int(parallel_query_start)*int(query_division_value):])\n",
    "                parallel_query_start += 1\n",
    "\n",
    "    return parallel_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 8. Run_Parallel_Query(species_of_query, species_of_subject, queryV, parallel_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def run_parallel_query(species_of_query, species_of_subject, query_v, parallel_num):\n",
    "    \"\"\" run_parallel_query(i ,k , query_v , cpu_count) i and k are user selected number in a list Format\n",
    "    Run the following functions. write_query, run_blast, same_species_forward_best_hit, forward_best_hit\n",
    "    Save the files which are oneway_threshold_best_hit, second_oneway_threshold_best_hit, \n",
    "    blastp_score_split_list and raw_blastp_score (optional) by each species. \n",
    "    parallel_num is the Number of CPU selected by the user if CPu 1 selected then 1 \"\"\"\n",
    "\n",
    "    #print(\"run_parallel_query Running\")\n",
    "    global selected_number, selected_species_dic\n",
    "    # bar = Bar('Processing '+str(parallel_num), max = len(query_v)) #progressing bar setting , Creating a Object\n",
    "    # bar is not Supported in Python 3    We use alive_bar instead\n",
    "    with alive_bar(len(query_v)) as bar:  # declare your set of items for loop\n",
    "        for j in query_v:\n",
    "            # bar.next() #progressing bar not supported\n",
    "            bar()# Call after Consuming One Item\n",
    "            write_query(j, parallel_num)  # if 1 Added Here Add also to Run\n",
    "        # This Function Only Write a file with j name and parallel_num i.e CPU Count\n",
    "            blastp_score = run_blast(\n",
    "                selected_species_dic[species_of_subject], parallel_num)\n",
    "        # \" Return Byte File type, and this is only for one gene position query and Whole Genome of species\"\n",
    "            if blastp_score != '':  # if blastp run successfully\n",
    "                best_score, blastp_score_split_list = forward_best_hit(\n",
    "                    blastp_score.decode())  # .decode() Convert in to str format\n",
    "                # ex) AAE == AAE. It will save best_score without reversing run_blast.\n",
    "                if species_of_query == species_of_subject:\n",
    "                    same_species_forward_best_score = same_species_forward_best_hit(\n",
    "                        blastp_score.decode())\n",
    "                    for best_score_element in same_species_forward_best_score:\n",
    "                        # ex) [A1 of AAE, A1 of AAE, 30]\n",
    "                        if best_score_element[0] == best_score_element[1]:\n",
    "                            with open(Score_file+selected_species_dic[species_of_query] + \"_\" +\n",
    "                                      selected_species_dic[species_of_subject]\n",
    "                                      + \"_oneway_threshold_best_hit_Score\"\n",
    "                                      + str(threshold_score), \"a\") as oneway_threshold_best_hit:\n",
    "                                save_best_score = selected_species_dic[species_of_query]+\"_\"+best_score_element[0].split(\"\\s\")[0]\\\n",
    "                                    + \" \"+selected_species_dic[species_of_subject]+\"_\"+best_score_element[1].split(\n",
    "                                        \"\\s\")[0]+\" \"+best_score_element[2]+\"\\n\"\n",
    "                                # best_score_element[0].split(\"|\") ==> ['gi', '15642790', 'ref', 'NP_227831.1', '']\n",
    "                                oneway_threshold_best_hit.write(\n",
    "                                    save_best_score)\n",
    "                        else:  # ex) [A1 of AAE, A2 of AAE, 30]\n",
    "                            with open(Score_file+selected_species_dic[species_of_query]+\"_\"\n",
    "                                      + selected_species_dic[species_of_subject]\n",
    "                                      + \"_second_oneway_threshold_best_hit_Score\"\n",
    "                                      + str(threshold_score), \"a\") as second_oneway_threshold_best_hit:\n",
    "                                second_save_best_score = selected_species_dic[species_of_query]\\\n",
    "                                    + \"_\"+best_score_element[0].split(\"\\s\")[0]\\\n",
    "                                    + \" \"+selected_species_dic[species_of_subject]+\"_\"\\\n",
    "                                    + best_score_element[1].split(\"\\s\")[0]+\" \"+best_score_element[2]+\"\\n\"\n",
    "                                second_oneway_threshold_best_hit.write(\n",
    "                                    second_save_best_score)\n",
    "                else:  # If species_of_query not equal with species_of_subject, run reversing run_blast\n",
    "                    for best_score_element in best_score:\n",
    "                        if not '-1' in best_score_element:\n",
    "                            with open(Score_file+selected_species_dic[species_of_query]\n",
    "                                      + \"_\" +\n",
    "                                      selected_species_dic[species_of_subject] +\n",
    "                                      \"_\"+\"best_score_S\"\n",
    "                                      + str(threshold_score)+\"_\"+str(parallel_num), \"a\") as save_best_hit:\n",
    "                                best_score_save = selected_species_dic[species_of_query]+\"_\"\\\n",
    "                                    + best_score_element[0].split(\"\\s\")[0]+\" \"+selected_species_dic[species_of_subject]\\\n",
    "                                    + \"_\"+best_score_element[1].split(\n",
    "                                        \"\\s\")[0]+\" \"+best_score_element[2]+\" \"+best_score_element[3]+\"\\n\"\n",
    "                                save_best_hit.write(best_score_save)\n",
    "\n",
    "                    for blastp_score_split_list_element in blastp_score_split_list:\n",
    "                        with open(Score_file+selected_species_dic[species_of_query]+\"_\"\n",
    "                                  + selected_species_dic[species_of_subject]+\"_\"+\"blastp_score_split_list_S\"\n",
    "                                  + str(threshold_score)+\"_\"+str(parallel_num), \"a\") as save_blastp_score_split_list:\n",
    "                            blastp_score_split_list_save = selected_species_dic[species_of_query]+\"_\"\\\n",
    "                                + blastp_score_split_list_element[0].split(\"\\s\")[0]\\\n",
    "                                + \" \"+selected_species_dic[species_of_subject]+\"_\"+blastp_score_split_list_element[1].split(\"\\s\")[0]\\\n",
    "                                + \" \" + \\\n",
    "                                blastp_score_split_list_element[2]+\" \" \\\n",
    "                                + blastp_score_split_list_element[3]+\"\\n\"\n",
    "                            save_blastp_score_split_list.write(\n",
    "                                blastp_score_split_list_save)\n",
    "\n",
    "            else:\n",
    "                print(\"No value in blastp_score\")\n",
    "            if save_raw_blastp_score:\n",
    "                with open(Score_file+selected_species_dic[species_of_query]\n",
    "                          + \"_\"+selected_species_dic[species_of_subject]+\"_S\"\n",
    "                          + str(threshold_score)+\"_\"+str(parallel_num), \"a\") as save_blastp:\n",
    "                    save_blastp.write(blastp_score)\n",
    "    # bar.finish() # progressing bar finish\n",
    "    return  # None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 9.oneway_threshold_best_hit(mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def oneway_threshold_best_hit(mode):\n",
    "    \"\"\" This Function accept the mode and Run program and return backward_best_hit_work_list mode 1 and 2 is supported \"\"\"\n",
    "    #print(\"oneway_threshold_best_hit running\")\n",
    "    global user_selected_number, cpu_count\n",
    "    b_info = \"Running the blastp & forward best hit searches \"\n",
    "    process_list = []\n",
    "    backward_best_hit_work_list = []\n",
    "    if \"1\" in mode:\n",
    "        \"\"\"We have 3 Mode 1 is for blastp, Mode 2 is for BLASTP using precalcualted data and Mode 3 is for clustering\"\"\"\n",
    "        for i in user_selected_number:  # Select species to write query \n",
    "            query_v = query_sequence(selected_species_dic[i])\n",
    "            # query_v is a list Format  with a position gene id , Seq\n",
    "            # User Selected  [1, 3, 5] is list of User input\n",
    "            for k in user_selected_number:\n",
    "                if k < i:  # gene ====> query 1->1 1->2 1->3 2->2 2->3  Forward checking will skip the same or less\n",
    "                    continue\n",
    "                else:\n",
    "                    print(b_info+\" between %s genome and %s genome\"\n",
    "                          % (selected_species_dic[i], selected_species_dic[k]))\n",
    "                    # this will Create a loop though 2 times\n",
    "                    # length of Genome gene ID in Sequence File\n",
    "                    query_v_len = len(query_v)\n",
    "                    if cpu_count == 1:\n",
    "                        #\"No Parallel Computing while cpu count == 1\"\n",
    "                        blastp_time_start = time.time()\n",
    "                        run_parallel_query(i, k, query_v, cpu_count)\n",
    "                        #\"i is first species k is second species number queryv is list file of i Position genome\"\n",
    "                        blastp_time_end = time.time()\n",
    "                        print(b_info+ \"took %.2f minutes\" % (\n",
    "                            (blastp_time_end-blastp_time_start)/60))\n",
    "                    else:\n",
    "                        # If the number of query_v_len is less than cpu_count, Remark will select the number of query_v_len.\n",
    "                        if query_v_len < cpu_count:\n",
    "                            #\"because the cpu_count will seprate list items from query_v file.In general this will not happened because length of query is always long\"\n",
    "                            blastp_time_start = time.time()\n",
    "                            # 1 is query_division_value. Because query_v_len / query_v_len(=cpu_count) is 1.\n",
    "                            parallel_query = division_parallel_query(\n",
    "                                query_v, 1, query_v_len, query_v_len)\n",
    "                            for m in range(query_v_len):\n",
    "                                process = multiprocessing.Process(\n",
    "                                    target=run_parallel_query, args=(i, k, parallel_query[m], m+1))\n",
    "                                # args( i => species of query , k => species of subject, m+1 => cpu_count ex) 1, 2 ...)\n",
    "                                process_list.append(process)\n",
    "                                process.start()\n",
    "                            for n in process_list:\n",
    "                                n.join()\n",
    "                            blastp_time_end = time.time()\n",
    "                            print(b_info+\"took %.2f minutes \" % (\n",
    "                                (blastp_time_end-blastp_time_start)/60))\n",
    "                        else:\n",
    "                            #\"this wiil run if cpu_count is more than 1\"\n",
    "                            blastp_time_start = time.time()\n",
    "                            query_division_value = query_v_len / cpu_count\n",
    "                            parallel_query = division_parallel_query(\n",
    "                                query_v, query_division_value, cpu_count, query_v_len)\n",
    "                            for m in range(cpu_count):\n",
    "                                process = multiprocessing.Process(\n",
    "                                    target=run_parallel_query, args=(i, k, parallel_query[m], m+1))\n",
    "                                # args( i => species of query , k => species of subject, m+1 => cpu_count ex) 1, 2 ...)\n",
    "                                process_list.append(process)\n",
    "                                process.start()\n",
    "                            for n in process_list:\n",
    "                                n.join()\n",
    "                            blastp_time_end = time.time()\n",
    "                            print(b_info+\"took %.2f minutes \" % (\n",
    "                                (blastp_time_end-blastp_time_start)/60))\n",
    "                if not i == k:\n",
    "                    backward_best_hit_work_list.append((i, k, query_v_len))\n",
    "                    print(\"Backward\")\n",
    "    elif \"2\" in mode:\n",
    "        \"\"\"If blast Files exist then this will pass the blast time other wise blast will run.\n",
    "        This will reduce the time for blast and increase the speed of the system.\n",
    "        \"\"\"\n",
    "        for i in user_selected_number:  # Select species to write query\n",
    "            query_v = query_sequence(selected_species_dic[i])\n",
    "            for k in user_selected_number:  # Select of subject\n",
    "                if Blastp_data+selected_species_dic[i]+\"_\"+selected_species_dic[k]\\\n",
    "                    + \"_oneway_threshold_best_hit_Score\"\\\n",
    "                        + str(threshold_score) in precalculated_data_list:\n",
    "                    used_precalculated_data_list.append(\n",
    "                        selected_species_dic[i]+\"_\"+selected_species_dic[k])\n",
    "                    #used_precalcualted _data_list is created before calling this function\n",
    "                    #if files exist then passed\n",
    "                    continue #File will skkipped\n",
    "                else:\n",
    "                    if k < i:  # gene ====> query 1->1 1->2 1->3 2->2 2->3\n",
    "                        continue\n",
    "                    else:\n",
    "                        \"same as Mode 1, if Precalculated file doesnot exist\"\n",
    "                        print(b_info+ \" between %s genome and %s genome\" % (\n",
    "                            selected_species_dic[i], selected_species_dic[k]))\n",
    "                        query_v_len = len(query_v)\n",
    "                        if cpu_count == 1:\n",
    "                            blastp_time_start = time.time()\n",
    "                            run_parallel_query(i, k, query_v, cpu_count)\n",
    "                            blastp_time_end = time.time()\n",
    "                            print(b_info+ \"took %.2f minutes\" % (\n",
    "                                (blastp_time_end-blastp_time_start)/60))\n",
    "                        else:\n",
    "                            # If the number of query_v_len is less than cpu_count, Remark will select the number of query_v_len.\n",
    "                            if query_v_len < cpu_count:\n",
    "                                blastp_time_start = time.time()\n",
    "                                # 1 is query_division_value. Because query_v_len / query_v_len(=cpu_count) is 1.\n",
    "                                parallel_query = division_parallel_query(\n",
    "                                    query_v, 1, query_v_len, query_v_len)\n",
    "                                for m in range(query_v_len):\n",
    "                                    process = multiprocessing.Process(\n",
    "                                        target=run_parallel_query,\n",
    "                                        args=(i, k, parallel_query[m], m+1))\n",
    "                                    # args( i => species of query , k => species of subject, m+1 => cpu_count ex) 1, 2 ...)\n",
    "                                    process_list.append(process)\n",
    "                                    process.start()\n",
    "                                for n in process_list:\n",
    "                                    n.join()\n",
    "                                blastp_time_end = time.time()\n",
    "                                print(b_info+\" took %.2f minutes\" % (\n",
    "                                    (blastp_time_end-blastp_time_start)/60))\n",
    "                            else:\n",
    "                                blastp_time_start = time.time()\n",
    "                                query_division_value = query_v_len / cpu_count\n",
    "                                parallel_query = division_parallel_query(\n",
    "                                    query_v, query_division_value, cpu_count, query_v_len)\n",
    "                                for m in range(cpu_count):\n",
    "                                    process = multiprocessing.Process(\n",
    "                                        target=run_parallel_query,\n",
    "                                        args=(i, k, parallel_query[m], m+1))\n",
    "                                    # args( i => species of query , k => species of subject, m+1 => cpu_count ex) 1, 2 ...)\n",
    "                                    process_list.append(process)\n",
    "                                    process.start()\n",
    "                                for n in process_list:\n",
    "                                    n.join()\n",
    "                                blastp_time_end = time.time()\n",
    "                                print(b_info+ \" took %.2f minutes\" % (\n",
    "                                    (blastp_time_end-blastp_time_start)/60))\n",
    "                        new_calculated_data_list.append(\n",
    "                            selected_species_dic[i]+\"_\"+selected_species_dic[k])\n",
    "                        if not i == k:\n",
    "                            backward_best_hit_work_list.append(\n",
    "                                (i, k, query_v_len))\n",
    "    return backward_best_hit_work_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 10. Backward_Best_Hit(args):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def backward_best_hit(args):\n",
    "    bck_info = \"Running the backward_best_hit\"\n",
    "    test =\" \"\n",
    "    print(bck_info)\n",
    "    species_of_query, species_of_subject, query_v_len = args\n",
    "    start_time_bbh = time.time()\n",
    "    forward_best_hit_score_list = []\n",
    "    blastp_score_split_list = []\n",
    "    print(bck_info+ \" between %s genome %s genome\" % (\n",
    "        selected_species_dic[species_of_query], selected_species_dic[species_of_subject]))\n",
    "    # If the number of query_v_len is less than cpu_count, the cpu_count is changed to query_v_len.\n",
    "    if query_v_len < cpu_count:\n",
    "        for parallel_num in range(query_v_len):\n",
    "            parallel_num += 1\n",
    "            with open(Score_file+selected_species_dic[species_of_query]+\"_\"\n",
    "                      + selected_species_dic[species_of_subject] + \"_\"+\"best_score_S\"\n",
    "                      + str(threshold_score)+\"_\"+str(parallel_num), \"r\") as best_hit_score:\n",
    "                for each_line in best_hit_score:\n",
    "                    split_each_line = each_line.split(\" \")\n",
    "                    split_each_line[2] = int(split_each_line[2])\n",
    "                    split_each_line[3] = int(split_each_line[3])\n",
    "                    forward_best_hit_score_list.append(split_each_line)\n",
    "            with open(Score_file+selected_species_dic[species_of_query]+\"_\"\n",
    "                      + selected_species_dic[species_of_subject] +\n",
    "                      \"_\"+\"blastp_score_split_list_S\"\n",
    "                      + str(threshold_score)+\"_\"+str(parallel_num), 'r') as blastp_score:\n",
    "                for each_line in blastp_score:\n",
    "                    split_each_line = each_line.split(\" \")\n",
    "                    split_each_line[2] = int(split_each_line[2])\n",
    "                    split_each_line[3] = int(split_each_line[3])\n",
    "                    blastp_score_split_list.append(split_each_line)\n",
    "    else:\n",
    "        for parallel_num in range(cpu_count):\n",
    "            parallel_num += 1\n",
    "            with open(Score_file+selected_species_dic[species_of_query]+\"_\"\n",
    "                      + selected_species_dic[species_of_subject]+\"_\"+\"best_score_S\"\n",
    "                      + str(threshold_score)+\"_\"+str(parallel_num), \"r\") as best_hit_score:\n",
    "                for each_line in best_hit_score:\n",
    "                    split_each_line = each_line.split(\" \")\n",
    "\n",
    "                    split_each_line[3] = int(split_each_line[3])\n",
    "                    forward_best_hit_score_list.append(split_each_line)\n",
    "            with open(Score_file+selected_species_dic[species_of_query] + \"_\"\n",
    "                      + selected_species_dic[species_of_subject]\n",
    "                      + \"_\"+\"blastp_score_split_list_S\"\n",
    "                      + str(threshold_score)+\"_\"+str(parallel_num), 'r') as blastp_score:\n",
    "                for each_line in blastp_score:\n",
    "                    split_each_line = each_line.split(\" \")\n",
    "                    split_each_line[2] = int(split_each_line[2])\n",
    "                    split_each_line[3] = int(split_each_line[3])\n",
    "                    blastp_score_split_list.append(split_each_line)\n",
    "\n",
    "    # \"bar = Bar(\"Searching : \"+selected_species_dic[species_of_query]+\"-\"+selected_species_dic[species_of_subject], max = len(forward_best_hit_score_list))\" #remove \"\"\n",
    "\n",
    "    for forward_best_hit_score_element in forward_best_hit_score_list:\n",
    "        matching_list = []\n",
    "        backward_best_score = ['-1', '-1', '-1']\n",
    "        # bar.next()\n",
    "        for element in blastp_score_split_list:\n",
    "            if element[1] == forward_best_hit_score_element[1]:\n",
    "                matching_list.append(element)\n",
    "\n",
    "        for element in matching_list:\n",
    "            if int(element[2]) > int(backward_best_score[2]):\n",
    "                backward_best_score = element\n",
    "    #        with open('./'+selected_species_dic[species_of_query]\\\n",
    "    # +\"_\"+selected_species_dic[species_of_subject]+'_subtraction'+\"_\"+str(threshold_score), 'a') as subtraction :\n",
    "    #            save_data = int(backward_best_score[2]) - int(forward_best_hit_score_element[2])\n",
    "    #            subtraction.write(str(save_data)+\"\\n\")\n",
    "\n",
    "        if int(backward_best_score[2]) - int(forward_best_hit_score_element[2]) <= threshold_score:\n",
    "            with open(Score_file+selected_species_dic[species_of_query]\n",
    "                      + \"_\"+selected_species_dic[species_of_subject]\n",
    "                      + \"_oneway_threshold_best_hit_Score\"+str(threshold_score), \"a\")\\\n",
    "                    as other_oneway_threshold_best_hit:\n",
    "                save_data = forward_best_hit_score_element[0]\\\n",
    "                    + \" \"+forward_best_hit_score_element[1]\\\n",
    "                    + \" \" + str(int(forward_best_hit_score_element[2]))+\"\\n\"\n",
    "                other_oneway_threshold_best_hit.write(save_data)\n",
    "\n",
    "    # bar.finish()\n",
    "    finish_time_bbh = time.time()\n",
    "    rbh_time = float((finish_time_bbh - start_time_bbh)/60)\n",
    "    print(bck_info+ \" %s-%s took %.2f minutes\" %\n",
    "          (selected_species_dic[species_of_query], selected_species_dic[species_of_subject], rbh_time))\n",
    "    return rbh_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 11. search_equal_bbh_data(target_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def search_unequal_bbh_data(target_b):\n",
    "    \"\"\"Search the unequal backward best hit data. ex) AAE_CAC_backward_best_hit\n",
    "    No Return !! vlaues means Null return\"\"\"\n",
    "    global unequal_BBH_data\n",
    "    print(unequal_BBH_data)\n",
    "    for i in unequal_BBH_data:\n",
    "        if i[2] == 0:\n",
    "            \" \"\n",
    "            pass\n",
    "        else:\n",
    "            if target_b[0] == i[0] or target_b[0] == i[1] or target_b[1] == i[0] or target_b[1] == i[1]:\n",
    "                # sample of B  - [\"AAE_gi|156|ref....\",\"CAC_gi|1560..\",85]\n",
    "                copy_i = copy.copy(i)\n",
    "                tasks.put(copy_i)\n",
    "                unequal_BBH_data[unequal_BBH_data.index(i)][2] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  12. search_unequal_bbh_data(target_B):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def search_unequal_bbh_data(target_b):\n",
    "    \"\"\"Search the unequal backward best hit data. ex) AAE_CAC_backward_best_hit\n",
    "    No Return !! vlaues means Null return\"\"\"\n",
    "    global unequal_BBH_data\n",
    "    print(unequal_BBH_data)\n",
    "    for i in unequal_BBH_data:\n",
    "        if i[2] == 0:\n",
    "            \" \"\n",
    "            pass\n",
    "        else:\n",
    "            if target_b[0] == i[0] or target_b[0] == i[1] or target_b[1] == i[0] or target_b[1] == i[1]:\n",
    "                # sample of B  - [\"AAE_gi|156|ref....\",\"CAC_gi|1560..\",85]\n",
    "                copy_i = copy.copy(i)\n",
    "                tasks.put(copy_i)\n",
    "                unequal_BBH_data[unequal_BBH_data.index(i)][2] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 13.matching_bbh(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def matching_bbh(target):\n",
    "    \"\"\" Match the backward best hit \"\"\"\n",
    "    #print(\"running matching_bbh\")\n",
    "    if target[2] == 0:\n",
    "        return\n",
    "\n",
    "    else:\n",
    "        copy_target = copy.copy(target)\n",
    "        search_equal_bbh_data(copy_target[0])\n",
    "        search_equal_bbh_data(copy_target[1])\n",
    "        results.put(copy_target)\n",
    "        unequal_BBH_data[unequal_BBH_data.index(target)][2] = 0\n",
    "\n",
    "    for j in unequal_BBH_data:\n",
    "        if j[2] == 0:\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            if copy_target[0] == j[0] or copy_target[0] == j[1] or copy_target[1] == j[0] or copy_target[1] == j[1]:\n",
    "                copy_j = copy.copy(j)\n",
    "                unequal_BBH_data[unequal_BBH_data.index(j)][2] = 0\n",
    "                tasks.put(copy_j)\n",
    "\n",
    "    while not tasks.empty():\n",
    "        get_task = tasks.get()\n",
    "        search_equal_bbh_data(get_task[0])\n",
    "        search_equal_bbh_data(get_task[1])\n",
    "        results.put(get_task)\n",
    "        search_unequal_bbh_data(get_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 14. matrix_clustering_ortholog(element_set) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def matrix_clustering_ortholog(element_set):\n",
    "    # matrix_clustering_ortholog(element_set, bar): # this is Old Method old is generating is  removed\n",
    "    \"\"\" Generate the matrix of clustering ortholog. \"\"\"\n",
    "    print(\"matrix_clustering_ortholog Function is running element_set is Passed is \")\n",
    "    print(element_set)\n",
    "    row_data = []\n",
    "    col_data = []\n",
    "    temp_results = queue.Queue()\n",
    "    \"Create a queue object with a given maximum size. If maxsize is <=0, teh queue size is infinite\"\n",
    "    # bar.next()\n",
    "    for element in element_set:\n",
    "        # if element[0] exist, returning the index in the row_data.\n",
    "        if row_data.count(element[0]) > 0:\n",
    "            # element[0] is data of row. ['gi|15606057|ref|NP_213434.1|', 'gi|15606057|ref|NP_213434.1|', '3823\\n']\n",
    "            row = row_data.index(element[0])\n",
    "\n",
    "        else:\n",
    "            row = len(row_data)\n",
    "            row_data.append(element[0])\n",
    "            # if element[0] doesn't exist, appending the element[0] to the col_data.\n",
    "            if col_data.count(element[0]) < 1:\n",
    "                col_data.append(element[0])\n",
    "\n",
    "        if col_data.count(element[1]) > 0:\n",
    "            col = col_data.index(element[1])  # element[1] is data of col.\n",
    "\n",
    "        else:\n",
    "            col = len(col_data)\n",
    "            col_data.append(element[1])\n",
    "            if row_data.count(element[1]) < 1:\n",
    "                col_data.append(element[1])\n",
    "\n",
    "        temp_results.put([row, col, element[2]])\n",
    "\n",
    "    # create a new matrix of given shape(the size_resuls) and type, filled with zeros.\n",
    "    score_matrix = numpy.matlib.zeros(\n",
    "        (len(row_data), len(col_data)), dtype=np.float)\n",
    "    # np.zeros() can be used, will test later,\n",
    "    while not temp_results.empty():\n",
    "        get_temp_results = temp_results.get()\n",
    "        row = get_temp_results[0]\n",
    "        col = get_temp_results[1]\n",
    "        score_matrix[row, col] = get_temp_results[2]\n",
    "        score_matrix[col, row] = get_temp_results[2]\n",
    "\n",
    "    # If the elements of row and col is less than 2, it is excluded.\n",
    "    if len(row_data)*len(col_data) > 4:\n",
    "        # The big size of matrix(bigger than 1000 X 1000) will be computed by parallel_matrix_multiplication function.\n",
    "        if len(row_data) > 1000 and cpu_count > 1:\n",
    "            score_matrix = parallel_mcl(score_matrix)\n",
    "        else:\n",
    "            score_matrix = mcl(score_matrix)\n",
    "        clustering(row_data, col_data, score_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 15.parallel_mcl(score_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def parallel_mcl(score_matrix):\n",
    "    # print(\"parallel_mcl\")\n",
    "    count = 0\n",
    "    infinitesimal_value = 10**-10\n",
    "    idempotent_matrix = numpy.matlib.ones((2, 2))\n",
    "\n",
    "    while idempotent_matrix.sum() > infinitesimal_value:  # > infinitesimal_value\n",
    "        mcl_time_start = time.time()\n",
    "        pool = multiprocessing.Pool(cpu_count)  # create a expansion_matrix\n",
    "        multiplication_results = pool.map(parallel_matrix_multiplication,\n",
    "                                          zip(score_matrix, repeat(score_matrix)))\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "        # create a inflation_matrix(part 1)\n",
    "        pool = multiprocessing.Pool(cpu_count)\n",
    "        power_results = pool.map(\n",
    "            parallel_matrix_power, multiplication_results)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "        sum_matrix = 0\n",
    "        for i in power_results:\n",
    "            sum_matrix = i + sum_matrix\n",
    "\n",
    "        # create a inflation_matrix(part 2)\n",
    "        pool = multiprocessing.Pool(cpu_count)\n",
    "        divide_results = pool.map(parallel_matrix_division,\n",
    "                                  zip(power_results, repeat(sum_matrix)))\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "        # Make a Combined matrix for results of parallel_matrix_multiplication function.\n",
    "        for i in range(len(divide_results)):\n",
    "            if i == 0:\n",
    "                score_matrix = divide_results[i]\n",
    "            else:\n",
    "                score_matrix = np.concatenate(\n",
    "                    (score_matrix, divide_results[i]), axis=0)\n",
    "\n",
    "        sum_results = 0\n",
    "        for i in multiplication_results:\n",
    "            sum_results += i\n",
    "        # identify whether inflation_matrix is idempotent matrix or not.\n",
    "        idempotent_matrix = abs(np.sum(score_matrix) - sum_results)\n",
    "\n",
    "        count += 1\n",
    "        if count > infinite_loop:  # It will prevent the infinite loop of mcl algorithm.\n",
    "            break\n",
    "        mcl_time_finish = time.time()\n",
    "        if verbose:\n",
    "            print(\" mcl time : %f, count : %d, matrix size : %d * %d\"\n",
    "                  % ((mcl_time_finish - mcl_time_start)/60, count, score_matrix[0].size, score_matrix[0].size))\n",
    "    return score_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 16. mcl(score_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def mcl(score_matrix):\n",
    "    count = 0\n",
    "    infinitesimal_value = 10**-10\n",
    "    idempotent_matrix = numpy.matlib.ones((2, 2))\n",
    "    #\"idempotent_matrix = np.ones((2,2)) # i will later test with this\"\n",
    "    while idempotent_matrix.sum() > infinitesimal_value:  # > infinitesimal_value\n",
    "        mcl_time_start = time.time()\n",
    "        expansion_matrix = score_matrix ** 2\n",
    "        print(expansion_matrix)\n",
    "        print(\"\\n\")\n",
    "        print(inflation_factor)\n",
    "        score_matrix = np.power(expansion_matrix, inflation_factor)\n",
    "        score_matrix_sum = score_matrix.sum(axis=0)\n",
    "        # create a inflation_matrix\n",
    "        score_matrix = np.divide(score_matrix, score_matrix_sum)\n",
    "        # identify whether inflation_matrix is idempotent matrix or not.\n",
    "        idempotent_matrix = abs(score_matrix - expansion_matrix)\n",
    "        count += 1\n",
    "        if count > infinite_loop:  # It will prevent the infinite loop of mcl algorithm.\n",
    "            break\n",
    "        mcl_time_finish = time.time()\n",
    "        if verbose:\n",
    "            print(\" mcl time : %f, count : %d, matrix size : %d * %d\"\n",
    "                  % ((mcl_time_finish - mcl_time_start)/60, count, score_matrix[0].size, score_matrix[0].size))\n",
    "    return score_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 17. clustering(row_data, col_data, score_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def clustering(row_data, col_data, score_matrix):\n",
    "    global cluster_count\n",
    "    global ortholog_count\n",
    "    ortholog_temp_list = []\n",
    "    for i in range(len(row_data)):\n",
    "        ortholog_list = []\n",
    "        ortholog = queue.Queue()  # It is Queue which is put the ortholog.\n",
    "        # It is Queue which is put the ortholog having changed gene ID\n",
    "        gene_id_queue = queue.Queue()\n",
    "        for j in range(len(col_data)):\n",
    "            if 0.1 <= score_matrix[i, j]:\n",
    "                ortholog.put(col_data[j])\n",
    "                ortholog_list.append(col_data[j])\n",
    "\n",
    "        # If the ortholog queue  has the element of ortholog more than 2, it will be printed.\n",
    "        if ortholog.qsize() >= 3:\n",
    "            for element in ortholog_list:\n",
    "                try:\n",
    "                    ortholog_sum += ortholog_temp_list.index(element)+1\n",
    "\n",
    "                except ValueError:\n",
    "                    with open(Cluster_out+\"_geneID_S\"+str(threshold_score) +\n",
    "                              \"_\"+str(inflation_factor), \"a\") as ortholog_list_save:\n",
    "                        ortholog_print = \"cluster \"+str(cluster_count)+\" :\"\n",
    "                        ortholog_list_save.write(ortholog_print)\n",
    "\n",
    "                        while not ortholog.empty():\n",
    "                            get_ortholog = ortholog.get()\n",
    "                            ortholog_list_save.write(\"\\t\"+get_ortholog)\n",
    "\n",
    "                            try:\n",
    "                                # get_ortholog --> ECO_170082288,  get_ortholog.split('_') --> ['ECO', '170082288']\n",
    "                                get_ortholog_split = get_ortholog.split('_')\n",
    "                                gene_id_queue.put(\n",
    "                                    get_ortholog_split[0]+\"_\"+gene_id_dic[get_ortholog_split[1]])\n",
    "                            except KeyError:\n",
    "                                # If the gene_id_dic don't have get_ortholog, it will print the original ID(get_ortholog).\n",
    "                                gene_id_queue.put(get_ortholog)\n",
    "                        ortholog_list_save.write(\"\\n\")\n",
    "                    with open(Cluster_out+\"_KO_ID_S\"\n",
    "                              + str(threshold_score)\n",
    "                              + \"_\"+str(inflation_factor), \"a\")\\\n",
    "                            as ortholog_list_geneID_save:\n",
    "                        ortholog_list_geneid_print = \"cluster \" + \\\n",
    "                            str(cluster_count)+\" :\"\n",
    "                        ortholog_list_geneID_save.write(\n",
    "                            ortholog_list_geneid_print)\n",
    "                        while not gene_id_queue.empty():\n",
    "                            ortholog_list_geneID_save.write(\n",
    "                                \"\\t\"+gene_id_queue.get())\n",
    "                            ortholog_count += 1\n",
    "                        cluster_count += 1\n",
    "                        ortholog_list_geneID_save.write(\"\\n\")\n",
    "                    break\n",
    "            ortholog_temp_list = operator.concat(\n",
    "                ortholog_temp_list, ortholog_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 18.parallel_matrix_multiplication(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def parallel_matrix_multiplication(data):\n",
    "    \"Doing parallel_matrix_multiplication Using Numpy\"\n",
    "    matrix_element, matrix = data\n",
    "    return  matrix_element * matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 19.parallel_matrix_power(matrix_element):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def parallel_matrix_power(matrix_element):\n",
    "    \"This Function Compute Parallel_Matrix Power by using Numpy np.power() Function\"\n",
    "    global inflation_factor\n",
    "    return np.power(matrix_element, inflation_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 20.parallel_matrix_division(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def parallel_matrix_division(data):\n",
    "    \"This Function Will Perform Matrix Division by Using Numpy library\"\n",
    "    matrix_element, sum_data = data\n",
    "    return np.divide(matrix_element, sum_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 21. read_species(pr = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "def read_species(pr=1):  # default 1 which will always shows the name of Species\n",
    "    \"\"\" If pr is 1, it will print \"Species_List\"  Other wise only return the Value in dic Format \"\"\"\n",
    "    read_species = os.listdir(Species)\n",
    "    selected_species_dic = {}  # list\n",
    "    backward_selected_species_dic = {}  # list\n",
    "    for i, species in enumerate(sorted(read_species), start=1):\n",
    "        selected_species_dic[i] = species\n",
    "        backward_selected_species_dic[species] = i\n",
    "        if pr == 1:\n",
    "            print(str(i)+\".\", species)\n",
    "        number = i\n",
    "    return selected_species_dic, backward_selected_species_dic, number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 22. Del_File(path,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "code_folding": [],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def del_file(path, file):\n",
    "    \"This Function delete the file passed with path and file\"\n",
    " #   \"Delete the Unnecessary file according to path and file name passed\"\n",
    " #  del_file =subprocess.Popen([\"rm \"+path+file], stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n",
    " #   del_file_stream = del_file.communicate()\n",
    " #   if not del_file_stream[1]:\n",
    " #       print (\"Done to del \"+path+file)\n",
    " #   elif del_file_stream[1]:\n",
    " #       print (del_file_stream[1])\n",
    "    try:\n",
    "        os.remove(path+file)\n",
    "        print(\"File Successfully Removed\")\n",
    "    except:\n",
    "        print(\"Check the File or Path\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Del_File(\"./query/\",\"query_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 23.check_file(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def check_file(file):\n",
    "    \"\"\"When mode 3 is Selected.This function will run. file is user input value for clustering output.\n",
    "    Check the file weather exists or not . if exist this will warn to use other name\n",
    "    Cluster_out is file.Cluster_out(results) + _geneID_S+ str(threshold_score)+\"_\"\\\n",
    "        +str(inflation_factor) or Cluster_out+\"_KO_ID_S\"+str(threshold_score)+\"_\"+str(inflation_factor))\n",
    "        are files created by mcl algorithm so if these file exist system will exit\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    Python2 Code\n",
    "        file_list = glob.glob(file+'*')    \n",
    "    if (Cluster_out+\"_geneID_S\"+str(threshold_score)+\"_\"+str(inflation_factor) or Cluster_out+\"_KO_ID_S\"+str(threshold_score)+\"_\"+str(inflation_factor)) in file_list:\n",
    "        print \"Please, set other name of output.\"\n",
    "        sys.exit(2)\n",
    "    \"\"\"\n",
    "    # Declare all variable as a globally Added\n",
    "    global Cluster_out, threshold_score, infinite_loop\n",
    "    file_list = glob.glob(file+'*')  # list all related files in same directory local variable\n",
    "    if (Cluster_out+\"_geneID_S\"+str(threshold_score)\n",
    "        + \"_\"+str(inflation_factor) or Cluster_out+\"_KO_ID_S\"\n",
    "            + str(threshold_score)+\"_\"+str(inflation_factor)) in file_list:\n",
    "        print(\"Please, set other name of output.line Number 966\")\n",
    "        sys.exit(2)\n",
    "    else :\n",
    "        print(file,\" is checked\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 24.read_equal_bbh(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def read_equal_bbh(path):\n",
    "    \"\"\"Read_Equal BBH by user path Blast Best Hit\n",
    "    the score fiie location is ./score_file/ + species + species like ./score_file/A_C\n",
    "    \"\"\"\n",
    "    global threshold_score\n",
    "\n",
    "    with open(path+\"_oneway_threshold_best_hit_Score\"\n",
    "              + str(threshold_score), 'r') as equal_RBH:\n",
    "        for j in equal_RBH:\n",
    "            split_data = j.split()\n",
    "            # Split Data sample\n",
    "            # ['A_gi|15605613|ref|NP_212986.1|', 'C_gi|15004707|ref|NP_149167.1|', '51']\n",
    "            split_data[2] = int(split_data[2]) #Only Converting to int format\n",
    "            equal_BBH_data.append(split_data)\n",
    "            equal_BBH_data_dic[split_data[0]] = split_data[1:]\n",
    "    try:\n",
    "        with open(path+\"_second_oneway_threshold_best_hit_Score\"\n",
    "                  + str(threshold_score), 'r') as second_equal_RBH:\n",
    "            for j in second_equal_RBH:\n",
    "                split_data = j.split()\n",
    "                #\"['AAE_gi|15606128|ref|NP_213505.1|', 'AAE_gi|15606877|ref|NP_214257.1|', 898] split_data sample\"\n",
    "                split_data[2] = int(split_data[2])\n",
    "                second_equal_BBH_data.append(split_data)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 25. read_unequal_bbh(path):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def read_unequal_bbh(path):\n",
    "    \"\"\" Read unequal BBH path passed by User.\n",
    "    Since this Function will append value to unequal_BBH_data which created before running this function\n",
    "    Later i will changed to return type of list from this function.create blank list and append and return\"\"\"\n",
    "\n",
    "    with open(path+\"_oneway_threshold_best_hit_Score\"\n",
    "              + str(threshold_score), 'r') as unequal_RBH:\n",
    "        for j in unequal_RBH:\n",
    "            split_data = j.split()\n",
    "            split_data[2] = int(split_data[2])\n",
    "            unequal_BBH_data.append(split_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read_unequal_bbh(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Main Program Testing\n",
    "`************* No Function below **************************`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. BLASTP. \n",
      "2. BLASTP using precalculated data. \n",
      "3. clustering.\n",
      "\n",
      ">> Select a mode or modes (1 2 *OR* 1 3 *OR* 2 3): 1 2 3\n",
      "1. AAE\n",
      "2. CAC\n",
      "3. ECO\n",
      "4. ECU\n",
      "5. HIN\n",
      "6. LLA\n",
      "7. Log\n",
      "8. SCE\n",
      "9. SPO\n",
      "10. SPY\n",
      "11. SYN\n",
      "12. TMA\n",
      "13. YPE\n",
      "14. chicken.faa\n",
      "15. chimpanzee.faa\n",
      "16. dog.faa\n",
      "17. elegans.faa\n",
      "18. fruitfly.faa\n",
      "19. human.faa\n",
      "20. mouse.faa\n",
      "21. opossum.faa\n",
      "22. pufferfish.faa\n",
      "23. rat.faa\n",
      "24. seasquirt.faa\n",
      "25. zebrafish.faa\n",
      ">> Select Genomes to detect Orthologs(e.g. 1 2 3 4 5 or 1-5) : 1 2 3\n",
      "AAE CAC ECO Selected!!\n",
      "BLOcks SUbstitution Matrix (BLOSUM) is a Substitution matrix used for sequence alignment of Proteins\n",
      "\n",
      "1. BLOSUM45 :-For more distantly related Proteins alignment DataBase\n",
      "2. BLOSUM62 :- MidRange Seq with more than 62%similarity         \n",
      "3. BLOSUM82 :- More related Proteins\n",
      "Other Keys will exit the Program\n",
      "\n",
      "Enter a matrix number: 1\n",
      "You can use 12 Core.\n",
      "If you input >= 2,    The Program will run a parallel computation for the blastp.\n",
      "Enter the number of Core to use in this program (1 ~ 12): 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"1. BLASTP. \\n2. BLASTP using precalculated data. \\n3. clustering.\\n\")\n",
    "mode = input(\">> Select a mode or modes (1 2 *OR* 1 3 *OR* 2 3): \")\n",
    "selected_species_dic, backward_selected_species_dic, number_i = read_species()\n",
    "selected_number = input(\n",
    "    \">> Select Genomes to detect Orthologs(e.g. 1 2 3 4 5 or 1-5) : \")\n",
    "\n",
    "if selected_number.find('-') > 0:\n",
    "    # find() return the index position of first occurance\n",
    "    SN = selected_number.split(\"-\")\n",
    "    if int(SN[-1]) > number_i:  # number_i is length of Genome file inside folder exit the process\n",
    "        print(\"\\nWrongInput\\nInput must be less than\", number_i)\n",
    "        sys.exit(2)\n",
    "    else:\n",
    "        user_selected_number = range(int(SN[0]), int(SN[-1])+1)\n",
    "        for j in user_selected_number:\n",
    "            print(selected_species_dic[j], end=\" \")  # loop in Dic\n",
    "        print(\"Selected!!\")\n",
    "\n",
    "else:\n",
    "    user_selected_number = sorted(\n",
    "        set([int(read_species) for read_species in selected_number.split()]))\n",
    "    # Create a set (remove repeating)\n",
    "    if int(user_selected_number[-1]) > number_i:\n",
    "        print(\"\\nWrongInput\\nInput must be less than\", number_i)\n",
    "        sys.exit(2)\n",
    "        # Greater than Genome list will system error\n",
    "    else:\n",
    "        for j in user_selected_number:\n",
    "            print(selected_species_dic[j], end=\" \")\n",
    "        print(\"Selected!!\")\n",
    "blastp_matrix = matrix_name()\n",
    "cpu_count = int(input(\"You can use %s Core.\\nIf you input >= 2,\\\n",
    "    The Program will run a parallel computation for the blastp.\\n\" % multiprocessing.cpu_count()\n",
    "                          + \"Enter the number of Core to use in this program (1 ~ %s): \" % multiprocessing.cpu_count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### part 2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inflation factor to cluster1.4\n",
      "Set the name of clustering output Folderresult\n",
      "result  is checked\n"
     ]
    }
   ],
   "source": [
    "if \"3\" in mode:\n",
    "    \"3 is for BLOSUM82\"\n",
    "    inflation_factor = input(\"inflation factor to cluster\")\n",
    "    Cluster_out= input(\"Set the name of clustering output Folder\")\n",
    "    \"cluster_out is user  input value\"\n",
    "    check_file(Cluster_out)\n",
    "    \n",
    "    Log_file_name = \"./cluster_out/\"+Cluster_out+\"_S\"+str(threshold_score)+\"_\"+str(inflation_factor)+\".log\"\n",
    "    with open(Log_file_name+datetime.datetime.now().strftime(\"_%Y_%m_%d_%H_%M_%S\")+\".txt\", 'w') as log:\n",
    "        \"if 3 is passed in mode then the log file created with Cluster and inflation_factor\"\n",
    "        log.write(\"\\t\\t\\tThis Log File is Created on\\t\")\n",
    "        log.write(str(datetime.datetime.now()))\n",
    "        log.write(\"\\nmode :\")\n",
    "        for i in mode:\n",
    "            log.write(\" \"+i)\n",
    "        log.write(\"\\ngenomes : \")\n",
    "        for i in user_selected_number:\n",
    "            log.write(selected_species_dic[i]+\" \")\n",
    "        log.write(\"\\ncpu_count : \"+str(cpu_count))\n",
    "        log.write(\"\\nblastp matrix : \"+blastp_matrix)\n",
    "        if \"3\" in mode:\n",
    "            log.write(\"\\ninflation_factor : \"+str(inflation_factor))\n",
    "            log.write(\"\\nCluster out : \"+Cluster_out)\n",
    "        log.write(\"\\nSpecies : \"+Species)\n",
    "        log.write(\"\\nBlastp : \"+Blastp)\n",
    "        log.write(\"\\nScore file : \"+Score_file)\n",
    "        log.write(\"\\nBlastp_data : \"+Blastp_data)\n",
    "        log.write(\"\\nsave rawblastp score : \"+str(save_raw_blastp_score))\n",
    "        log.write(\"\\n\")\n",
    "elif not \"3\" in mode:\n",
    "    \"\"\"If mode 3 Clustering is not Passed then simple log wile will created \"\"\"\n",
    "    Log_file_name = 'log_files/Log'  # log file rename + .txt\n",
    "    with open(Log_file_name+datetime.datetime.now().strftime(\"_%Y_%m_%d_%H_%M_%S\")+\".txt\", 'w') as log:\n",
    "        log.write(\"\\t\\t\\tThis Log File is Created on\\t\")\n",
    "        log.write(str(datetime.datetime.now()))\n",
    "        log.write(\"\\nmode :\")\n",
    "        for i in mode:\n",
    "            log.write(\" \"+i)\n",
    "        log.write(\"\\ngenomes : \")\n",
    "        for i in user_selected_number:\n",
    "            log.write(selected_species_dic[i]+\" \")\n",
    "        log.write(\"\\ncpu_count : \"+str(cpu_count))\n",
    "        log.write(\"\\nblastp matrix : \"+blastp_matrix)\n",
    "        if \"3\" in mode:\n",
    "            log.write(\"\\ninflation_factor : \"+str(inflation_factor))\n",
    "            log.write(\"\\nCluster out : \"+Cluster_out)\n",
    "        log.write(\"\\nSpecies : \"+Species)\n",
    "        log.write(\"\\nBlastp : \"+Blastp)\n",
    "        log.write(\"\\nScore file : \"+Score_file)\n",
    "        log.write(\"\\nBlastp_data : \"+Blastp_data)\n",
    "        log.write(\"\\nsave rawblastp score : \"+str(save_raw_blastp_score))\n",
    "        log.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "code_folding": [
     6,
     26
    ]
   },
   "outputs": [
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'cp949' codec can't encode character '\\u26a0' in position 1: illegal multibyte sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\alive_progress\\progress.py\u001b[0m in \u001b[0;36malive_bar\u001b[1;34m(total, title, calibrate, **options)\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 265\u001b[1;33m         \u001b[1;32myield\u001b[0m \u001b[0mbar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-11e03ed0f8d8>\u001b[0m in \u001b[0;36mrun_parallel_query\u001b[1;34m(species_of_query, species_of_subject, query_v, parallel_num)\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m# Call after Consuming One Item\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m             \u001b[0mwrite_query\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparallel_num\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# if 1 Added Here Add also to Run\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[1;31m# This Function Only Write a file with j name and parallel_num i.e CPU Count\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'write_query' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-1a45ed506c02>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mstart_time_OBH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;34m\"1\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mbackward_best_hit_work_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moneway_threshold_best_hit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mpool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcpu_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbackward_best_hit\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbackward_best_hit_work_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-278e574d8e31>\u001b[0m in \u001b[0;36moneway_threshold_best_hit\u001b[1;34m(mode)\u001b[0m\n\u001b[0;32m     24\u001b[0m                         \u001b[1;31m#\"No Parallel Computing while cpu count == 1\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m                         \u001b[0mblastp_time_start\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m                         \u001b[0mrun_parallel_query\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery_v\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcpu_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m                         \u001b[1;31m#\"i is first species k is second species number queryv is list file of i Position genome\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m                         \u001b[0mblastp_time_end\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-11e03ed0f8d8>\u001b[0m in \u001b[0;36mrun_parallel_query\u001b[1;34m(species_of_query, species_of_subject, query_v, parallel_num)\u001b[0m\n\u001b[0;32m     83\u001b[0m                           \u001b[1;33m+\u001b[0m \u001b[1;34m\"_\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mselected_species_dic\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mspecies_of_subject\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"_S\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m                           + str(threshold_score)+\"_\"+str(parallel_num), \"a\") as save_blastp:\n\u001b[1;32m---> 85\u001b[1;33m                     \u001b[0msave_blastp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblastp_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m     \u001b[1;31m# bar.finish() # progressing bar finish\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[1;32mreturn\u001b[0m  \u001b[1;31m# None\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\alive_progress\\progress.py\u001b[0m in \u001b[0;36malive_bar\u001b[1;34m(total, title, calibrate, **options)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m         \u001b[0mend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstats_end\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 275\u001b[1;33m         \u001b[0malive_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\alive_progress\\progress.py\u001b[0m in \u001b[0;36malive_repr\u001b[1;34m(spin)\u001b[0m\n\u001b[0;32m    116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mline_len\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_line_len\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m                 \u001b[0mclear_traces\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m             \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__stdout__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mspin\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m'\\r'\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m             \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__stdout__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'cp949' codec can't encode character '\\u26a0' in position 1: illegal multibyte sequence"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the blastp & forward best hit searches  between AAE genome and AAE genome\n"
     ]
    }
   ],
   "source": [
    "start_time_OBH = time.time()\n",
    "if \"1\" in mode:\n",
    "    backward_best_hit_work_list = oneway_threshold_best_hit(mode)\n",
    "    pool = multiprocessing.Pool(cpu_count)\n",
    "    results = pool.map(backward_best_hit,backward_best_hit_work_list)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "elif \"2\" in mode:\n",
    "    used_precalculated_data_list = []\n",
    "    new_calculated_data_list = []\n",
    "    print(\"Threshold_score passed is \", threshold_score)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python 2 whole Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.\\\\species']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.git',\n",
       " '.ipynb_checkpoints',\n",
       " '.vscode',\n",
       " 'All_Functions_New.ipynb',\n",
       " 'blastp_data',\n",
       " 'blastp_memo.txt',\n",
       " 'cluster_out',\n",
       " 'db',\n",
       " 'Log.txt',\n",
       " 'log_files',\n",
       " 'Manual',\n",
       " 'nohup.out',\n",
       " 'Ortho_all_Function_old.ipynb',\n",
       " 'ortho_functions.py',\n",
       " 'owPReMark.py',\n",
       " 'owPReMark.sh',\n",
       " 'query',\n",
       " 'query_1',\n",
       " 'save_raw_blastp_score',\n",
       " 'score_file',\n",
       " 'species',\n",
       " 'Untitled.ipynb',\n",
       " '_S5_10.log.txt',\n",
       " '__pycache__']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precalculated_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "import glob\n",
    "import sys\n",
    "import subprocess\n",
    "import pdb\n",
    "import time\n",
    "from progress.bar import Bar\n",
    "import multiprocessing\n",
    "import Queue\n",
    "import numpy.matlib\n",
    "import numpy\n",
    "import copy\n",
    "import operator\n",
    "import pprint\n",
    "from itertools import *\n",
    "import argparse\n",
    "import datetime\n",
    "\n",
    "print \"=\"*77\n",
    "print \"\\t\"*2+\" \"*4+\"owPReMark for Linux version 1.0 (C) 2013\"\n",
    "print \"\\t\"*6+\" \"*4+\"JungWon Park, Sunshin Kim\"\n",
    "print \"\\t\"*6+\" \"*2+\"GPLv3 Licence (see LICENCE)\"\n",
    "print \"=\"*77\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description = 'The owPReMark is a program to detect ortholog between the protein sequences from different genomes and to cluster orthologs to ortholog groups. ', \n",
    "                                 add_help=True, prefix_chars='-+')\n",
    "essential_group = parser.add_argument_group('essential arguments')\n",
    "blastp_group = parser.add_argument_group('blastp optional arguments')\n",
    "mcl_group = parser.add_argument_group('clustering optional arguments')\n",
    "essential_group.add_argument('-M','--mode', \n",
    "                             action='store', \n",
    "                             nargs='+',\n",
    "                             dest='mode',\n",
    "                             choices=['1','2','3'],\n",
    "                             help=\"1 : BLASTP. 2 : BLASTP using precalculated data. 3 : Clustering. ex) 1 or 2 or 1 3 or 2 3\")\n",
    "essential_group.add_argument('-g','--genomes', \n",
    "                             nargs='+',\n",
    "                             dest='genomes', \n",
    "                             help=\"select an genomes to detect orthologs.\")\n",
    "parser.add_argument('-c','--cpu', \n",
    "                             action='store', \n",
    "                             default='1', \n",
    "                             dest='cpu_count', \n",
    "                             type=int, \n",
    "                             help='set the number of CPU to use in program. The default is \"1\". This system has '+str(multiprocessing.cpu_count())+\" CPUs\")\n",
    "blastp_group.add_argument('+r', \n",
    "                             action='store_true', \n",
    "                             default=False,\n",
    "                             dest='save_raw_blastp_score', \n",
    "                             help='save the raw score of blastp to the score_file dirctory.')\n",
    "blastp_group.add_argument('-t', '--threshold', \n",
    "                             action='store', \n",
    "                             default=0, \n",
    "                             type=int,\n",
    "                             dest='threshold_score', \n",
    "                             help='set the threshold score. The threshold score is an allowable range to be an ortholog.'+\n",
    "                             'If the score difference between backward best hit score and forward best hit score is less than threshold score, it is considered the forward best hit pair to be the ortholog.'+\n",
    "                             'That is  called the \"one-way threshold best hit\" by us. The default value is \"0\". \"backward best hit score - forward best hit score <= threshold score\"')\n",
    "blastp_group.add_argument('-m','--matrix', \n",
    "                             action='store', \n",
    "                             default='BLOSUM62', \n",
    "                             dest='blastp_matrix', \n",
    "                             choices=['BLOSUM45', 'BLOSUM62', 'BLOSUM82'],\n",
    "                             help='select the matrix to use in blastp. The default value is \"BLOSUM62\".')\n",
    "blastp_group.add_argument('-s','--species', \n",
    "                             action='store', \n",
    "                             default = './species/', \n",
    "                             dest='Species', \n",
    "                             help='set the path of species directory. The default is  \"./species/\".')\n",
    "blastp_group.add_argument('-b','--blastp', \n",
    "                             action='store', \n",
    "                             default ='/usr/bin/blastp', \n",
    "                             dest='Blastp', \n",
    "                             help='set the path of blastp file to run the blastp program. The default is \"blastp\".')\n",
    "blastp_group.add_argument('-F','--scorefile', \n",
    "                             action='store', \n",
    "                             default='./score_file/',\n",
    "                             dest='Score_file', \n",
    "                             help='The default path is \"./score_file/\".')\n",
    "blastp_group.add_argument('-B','--blastp_data', \n",
    "                             action='store', \n",
    "                             default='./blastp_data/', \n",
    "                             dest='Blastp_data', \n",
    "                             help='set the path of precalculated blastp data. The default path is \"./blastp_data/\".')\n",
    "mcl_group.add_argument('-i' ,'--IF', \n",
    "                             action='store', \n",
    "                             default=1.4, \n",
    "                             dest='inflation_factor', \n",
    "                             type=float, \n",
    "                             help ='The default value is \"1.4\".')\n",
    "mcl_group.add_argument('-l', '--loop', \n",
    "                             action='store', \n",
    "                             default=100, \n",
    "                             type=int,\n",
    "                             dest='infinite_loop', \n",
    "                             help='prevent the infinite loop of MCL algorithm. The default value is \"60\".')\n",
    "mcl_group.add_argument('-o','--out', \n",
    "                             action='store', \n",
    "                             default='./cluster_out', \n",
    "                             dest='Cluster_out', \n",
    "                             help='set the path and name of ortholog cluster file, log and error_warning file. The default path and name is \"./cluster_out\".')\n",
    "mcl_group.add_argument('+v', \n",
    "                             action='store_true', \n",
    "                             default=False,\n",
    "                             dest='verbose', \n",
    "                             help='verbosely show  information of a big matrix computaion.')\n",
    "parser.add_argument('--version', \n",
    "                             action='version', \n",
    "                             version='%(prog)s Ver. 11.1')\n",
    "command_options = parser.parse_args()\n",
    "\n",
    "\n",
    "def GetMatrixNumber():\n",
    "    print \"\\n1. BLOSUM45\\n2. BLOSUM62\\n3. BLOSUM82\\n4. Quit\"\n",
    "    get_matrix_number = raw_input(\"\\nEnter a matrix number: \")\n",
    "    if get_matrix_number == None:\n",
    "        matrix_number = \"BLOSUM62\"\n",
    "    elif get_matrix_number == \"1\":\n",
    "        matrix_number = \"BLOSUM45\"\n",
    "    elif get_matrix_number == \"2\":\n",
    "        matrix_number = \"BLOSUM62\"\n",
    "    elif get_matrix_number == \"3\":\n",
    "        matrix_number = \"BLOSUM82\"\n",
    "    elif get_matrix_number == \"4\":\n",
    "        sys.exit(0)\n",
    "    else :\n",
    "        print \"Wrong typing! Try again!\"\n",
    "        sys.exit(2)\n",
    "    print \"\\nMatrix number is : \" + matrix_number + \"\\n\"\n",
    "    return matrix_number \n",
    "\n",
    "def GetQuerySequence(genome):\n",
    "    gene_sequence=\"\"\n",
    "    gene_sequence_list= []\n",
    "    try:\n",
    "        with open(Species+genome) as gene:\n",
    "            for each_line in gene:\n",
    "                if \">\" in each_line:\n",
    "                    if gene_sequence != \"\":                        \n",
    "                        gene_sequence_list.append(gene_sequence)\n",
    "                        gene_sequence = \"\"\n",
    "                gene_sequence = gene_sequence+each_line\n",
    "            gene_sequence_list.append(gene_sequence)                   \n",
    "        return gene_sequence_list                        \n",
    "                                            \n",
    "    except IOError as err:\n",
    "        print \"IOError occurred in GetQuerySequence function : \" + str(err)    \n",
    "\n",
    "def WriteQuery(query, parallel_num):\n",
    "    try:\n",
    "        with open(\"./query\"+\"_\"+str(parallel_num), \"w\") as write_query:\n",
    "            write_query.write(query)\n",
    "    except IOError as err:\n",
    "        print \"IOError occurred in WriteQuery function : \" + str(err)\n",
    "        \n",
    "def RunBlast(subject, parallel_num):\n",
    "    subject = Species+subject\n",
    "    print \"Subject , parallel_num\" , subject , parallel_num       \n",
    "    run_blastp =subprocess.Popen([Blastp, \"-query\", \"./query\"+\"_\"+str(parallel_num), \"-subject\", subject,\n",
    "                                \"-matrix\", blastp_matrix, \"-outfmt\", \"10 qseqid sseqid score length\"],\n",
    "                                 stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    run_blastp_stream = run_blastp.communicate()\n",
    "    run_blastp_output_stream = run_blastp_stream[0]\n",
    "    run_blastp_error_stream = run_blastp_stream[1]          \n",
    "    return run_blastp_output_stream\n",
    "\n",
    "def Get_Same_Species_Forward_Best_Hit(blastp_score): \n",
    "    \"\"\"Search the forward best hit among the blastp scores of same species. Because there are an duplicated genes in a same genome.\n",
    "    When the blastp score compare with blastp score of duplicate gene, if score and length are same, blasp score of duplicated gene is added to a second best score.\"\"\" \n",
    "    blastp_score_split_list = []                   \n",
    "    temp_best_score =['-1','-1','-1']\n",
    "    second_temp_best_score = []\n",
    "    best_score = [] \n",
    "    second_best_score = []       \n",
    "    blastp_score_split = blastp_score.split(\"\\n\") \n",
    "    del blastp_score_split[-1] # delete of ['']   ex) ['gi,gi,1,1','gi,gi,2,2','']\n",
    "    for i in blastp_score_split:       \n",
    "        blastp_score_element = i.split(',') \n",
    "        blastp_score_split_list.append(blastp_score_element)\n",
    "    for k in blastp_score_split_list: # ex) k is ['gi|15605613|ref|NP_212986.1|', 'gi|15605613|ref|NP_212986.1|', '3702', '699']\n",
    "        if k[0] == k[1]:\n",
    "            best_score.append(k)        \n",
    "        elif k[0] != k[1]:            \n",
    "            if int(k[2]) > int(temp_best_score[2]) : #Compare score\n",
    "                temp_best_score = k\n",
    "            elif int(k[2]) == int(temp_best_score[2]):\n",
    "                if int(k[3]) > int(temp_best_score[3]): #compare length\n",
    "                    temp_best_score = k\n",
    "                elif int(k[3]) == int(temp_best_score[3]):\n",
    "                    second_temp_best_score.append(k) \n",
    "  #print \"############ temp best score ############\", temp_best_score\n",
    "    second_best_score.append(temp_best_score)\n",
    "    for j in second_temp_best_score:\n",
    "        if j[2] == temp_best_score[2] and j[3] == temp_best_score[3]:\n",
    "            second_best_score.append(j)\n",
    "    for m in second_best_score:\n",
    "        if (best_score[0][2] == m[2] and int(best_score[0][3]) <= int(m[3])) or int(best_score[0][2]) < int(m[2]): # '104' < '23' is True because of string. So the int function is used.\n",
    "            best_score.append(m)\n",
    "     \n",
    "    return best_score\n",
    "\n",
    "def GetForwardBestHit(blastp_score):\n",
    "    \"\"\"Search the forward best hit among the blastp scores of same species.\"\"\"\n",
    "    blastp_score_split_list = []\n",
    "    temp_best_score =['-1','-1','-1']\n",
    "    second_temp_best_score = []\n",
    "    best_score = []\n",
    "    blastp_score_split = blastp_score.split(\"\\n\") \n",
    "    del blastp_score_split[-1] # delete of ['']   ex) ['gi,gi,1,1','gi,gi,2,2','']\n",
    "    for i in blastp_score_split:       \n",
    "        blastp_score_element = i.split(',') \n",
    "        blastp_score_split_list.append(blastp_score_element)\n",
    "   \n",
    "    for k in blastp_score_split_list: # ex) k is ['gi|15605613|ref|NP_212986.1|', 'gi|15605613|ref|NP_212986.1|', '3702', '699']\n",
    " #        print \">>>>>>>>>>>>>>>GetForwardBestHit   k\", k\n",
    "        if int(k[2]) > int(temp_best_score[2]) : #Compare score\n",
    "            temp_best_score = k\n",
    "        elif int(k[2]) == int(temp_best_score[2]):\n",
    "            if int(k[3]) > int(temp_best_score[3]): #compare length\n",
    "                temp_best_score = k\n",
    "            elif int(k[3]) == int(temp_best_score[3]):\n",
    "                second_temp_best_score.append(k) \n",
    " #    print \"############ temp best score ############\", temp_best_score\n",
    "    best_score.append(temp_best_score)\n",
    "    for j in second_temp_best_score:\n",
    "        if j[2] == temp_best_score[2] and j[3] == temp_best_score[3]:\n",
    "            best_score.append(j)\n",
    "               \n",
    "    return best_score, blastp_score_split_list\n",
    "\n",
    "def DivisionParallelQuery(queryV, query_division_value, cpu_count, queryV_len):\n",
    "    parallel_query = []\n",
    "    parallel_query_start = 0\n",
    "    \n",
    "    if queryV_len % cpu_count == 0 : # perfect division \n",
    "        for i in range(cpu_count):\n",
    "            i += 1\n",
    "            if parallel_query_start == 0 :\n",
    "                parallel_query.append(queryV[parallel_query_start:i*query_division_value])\n",
    "                parallel_query_start += 1\n",
    "            else :\n",
    "                parallel_query.append(queryV[parallel_query_start*query_division_value:i*query_division_value])\n",
    "                parallel_query_start += 1\n",
    "    else : #imperfect division\n",
    "        for i in range(cpu_count):\n",
    "            i += 1\n",
    "            if parallel_query_start == 0 :\n",
    "                parallel_query.append(queryV[parallel_query_start:i*query_division_value])\n",
    "                parallel_query_start += 1\n",
    "            elif i < cpu_count :\n",
    "                parallel_query.append(queryV[parallel_query_start*query_division_value:i*query_division_value])\n",
    "                parallel_query_start += 1\n",
    "            elif i == cpu_count :\n",
    "                parallel_query.append(queryV[parallel_query_start*query_division_value:])\n",
    "                parallel_query_start += 1       \n",
    "        \n",
    "    return parallel_query\n",
    "    \n",
    "def RunParallelQuery(species_of_query, species_of_subject,queryV, parallel_num):    \n",
    "    \"\"\" Run the following functions. WriteQuery, RunBlast, Get_Same_Species_Forward_Best_Hit, GetForwardBestHit\n",
    "    Save the files which are oneway_threshold_best_hit, second_oneway_threshold_best_hit, blastp_score_split_list and raw_blastp_score (optional) by each species. \"\"\"\n",
    "    bar = Bar('Processing '+str(parallel_num), max = len(queryV)) #progressing bar setting           \n",
    "    for j in queryV:\n",
    "        bar.next() #progressing bar print\n",
    "        WriteQuery(j,parallel_num)        \n",
    "        blastp_score  = RunBlast(selected_species_dic[species_of_subject], parallel_num)        \n",
    "        if blastp_score != '': # Check whether blastp_score has the value\n",
    "            \n",
    "            best_score, blastp_score_split_list = GetForwardBestHit(blastp_score)\n",
    "\n",
    "            if species_of_query == species_of_subject: # ex) AAE == AAE. It will save best_score without reversing RunBlast. \n",
    "                same_species_forward_best_score = Get_Same_Species_Forward_Best_Hit(blastp_score)\n",
    "                for best_score_element in same_species_forward_best_score:\n",
    "                    if best_score_element[0] == best_score_element[1]:  #  ex) [A1 of AAE, A1 of AAE, 30]                                           \n",
    "                   \n",
    "                        with open(Score_file+selected_species_dic[species_of_query]+\"_\"+selected_species_dic[species_of_subject]+\"_oneway_threshold_best_hit_S\"+str(threshold_score), \"a\") as oneway_threshold_best_hit:\n",
    "                            \n",
    "                            save_best_score = selected_species_dic[species_of_query]+\"_\"+best_score_element[0].split(\"|\")[1]+\" \"+selected_species_dic[species_of_subject]+\"_\"+best_score_element[1].split(\"|\")[1]+\" \"+best_score_element[2]+\"\\n\" # best_score_element[0].split(\"|\") ==> ['gi', '15642790', 'ref', 'NP_227831.1', '']\n",
    "                            oneway_threshold_best_hit.write(save_best_score)\n",
    "                            \n",
    "                    else: # ex) [A1 of AAE, A2 of AAE, 30]\n",
    "                        with open(Score_file+selected_species_dic[species_of_query]+\"_\"+selected_species_dic[species_of_subject]+\"_second_oneway_threshold_best_hit_S\"+str(threshold_score), \"a\") as second_oneway_threshold_best_hit:\n",
    "                            second_save_best_score = selected_species_dic[species_of_query]+\"_\"+best_score_element[0].split(\"|\")[1]+\" \"+selected_species_dic[species_of_subject]+\"_\"+best_score_element[1].split(\"|\")[1]+\" \"+best_score_element[2]+\"\\n\"\n",
    "                            second_oneway_threshold_best_hit.write(second_save_best_score)\n",
    "                     \n",
    "            else: # If species_of_query not equal with species_of_subject, run reversing RunBlast\n",
    "                for best_score_element in best_score:\n",
    "                    with open(Score_file+selected_species_dic[species_of_query]+\"_\"+selected_species_dic[species_of_subject]+\"_\"+\"best_score_S\"+str(threshold_score)+\"_\"+str(parallel_num), \"a\") as save_best_hit:\n",
    "                        best_score_save = selected_species_dic[species_of_query]+\"_\"+best_score_element[0].split(\"|\")[1]+\" \"+selected_species_dic[species_of_subject]+\"_\"+best_score_element[1].split(\"|\")[1]+\" \"+best_score_element[2]+\" \"+best_score_element[3]+\"\\n\"\n",
    "                        save_best_hit.write(best_score_save)\n",
    "                \n",
    "                for blastp_score_split_list_element in blastp_score_split_list:\n",
    "                    with open(Score_file+selected_species_dic[species_of_query]+\"_\"+selected_species_dic[species_of_subject]+\"_\"+\"blastp_score_split_list_S\"+str(threshold_score)+\"_\"+str(parallel_num), \"a\") as save_blastp_score_split_list:                            \n",
    "                        blastp_score_split_list_save = selected_species_dic[species_of_query]+\"_\"+blastp_score_split_list_element[0].split(\"|\")[1]+\" \"+selected_species_dic[species_of_subject]+\"_\"+blastp_score_split_list_element[1].split(\"|\")[1]+\" \"+blastp_score_split_list_element[2]+\" \"+blastp_score_split_list_element[3]+\"\\n\"\n",
    "                        save_blastp_score_split_list.write(blastp_score_split_list_save)\n",
    "        \n",
    "        if save_raw_blastp_score :\n",
    "             with open(Score_file+selected_species_dic[species_of_query]+\"_\"+selected_species_dic[species_of_subject]+\"_S\"+str(threshold_score)+\"_\"+str(parallel_num), \"a\") as save_blastp:\n",
    "                 save_blastp.write(blastp_score)\n",
    "    bar.finish() # progressing bar finish \n",
    "    return \n",
    "    \n",
    "def Oneway_Threshold_Best_Hit(mode):    \n",
    "    process_list = []\n",
    "    backward_best_hit_work_list = []\n",
    "    if \"1\" in mode:\n",
    "        for i in user_selected_number: #Select species to write query    \n",
    "            queryV = GetQuerySequence(selected_species_dic[i])\n",
    "            for k in user_selected_number: #Select of subject\n",
    "                if k < i: # gene ====> query 1->1 1->2 1->3 2->2 2->3  \n",
    "                    continue\n",
    "                \n",
    "                else :\n",
    "                    print \"Doing the blastp & forward best hit searches between %s genome and %s genome\" % (selected_species_dic[i], selected_species_dic[k])\n",
    "                    \n",
    "                    queryV_len = len(queryV)\n",
    "                    if cpu_count == 1:\n",
    "                        blastp_time_start = time.time()\n",
    "                        RunParallelQuery(i, k, queryV, cpu_count)\n",
    "                        blastp_time_end = time.time()\n",
    "                        print \"The blastp & forward best hit searches took %.2f minutes\" % ((blastp_time_end-blastp_time_start)/60)\n",
    "                    else :\n",
    "                        if queryV_len < cpu_count: #If the number of queryV_len is less than cpu_count, Remark will select the number of queryV_len.\n",
    "                            blastp_time_start = time.time()\n",
    "                            parallel_query = DivisionParallelQuery(queryV, 1, queryV_len, queryV_len) # 1 is query_division_value. Because queryV_len / queryV_len(=cpu_count) is 1.\n",
    "                            for m in range(queryV_len):\n",
    "                                process = multiprocessing.Process(target=RunParallelQuery, args=(i, k, parallel_query[m], m+1))\n",
    "                                # args( i => species of query , k => species of subject, m+1 => cpu_count ex) 1, 2 ...)\n",
    "                                process_list.append(process)\n",
    "                                process.start()              \n",
    "                            for n in process_list:\n",
    "                                n.join()\n",
    "                            blastp_time_end = time.time()\n",
    "                            print \"The blastp & forward best hit searches took %.2f minutes\" % ((blastp_time_end-blastp_time_start)/60)\n",
    "                        else :\n",
    "                            blastp_time_start = time.time()\n",
    "                            query_division_value = queryV_len / cpu_count\n",
    "                            parallel_query = DivisionParallelQuery(queryV, query_division_value, cpu_count, queryV_len)\n",
    "                            for m in range(cpu_count):\n",
    "                                process = multiprocessing.Process(target=RunParallelQuery, args=(i, k, parallel_query[m], m+1))\n",
    "                                # args( i => species of query , k => species of subject, m+1 => cpu_count ex) 1, 2 ...)\n",
    "                                process_list.append(process)\n",
    "                                process.start()              \n",
    "                            for n in process_list:\n",
    "                                n.join()\n",
    "                            blastp_time_end = time.time()\n",
    "                            print \"The blastp & forward best hit searches took %.2f minutes\" % ((blastp_time_end-blastp_time_start)/60)\n",
    "                    if not i == k :                         \n",
    "                        backward_best_hit_work_list.append((i,k, queryV_len))\n",
    "    elif \"2\" in mode:\n",
    "        for i in user_selected_number: #Select species to write query\n",
    "            queryV = GetQuerySequence(selected_species_dic[i])\n",
    "            for k in user_selected_number: #Select of subject\n",
    "                if Blastp_data+selected_species_dic[i]+\"_\"+selected_species_dic[k]+\"_oneway_threshold_best_hit_S\"+str(threshold_score) in precalculated_data_list :\n",
    "                    used_precalculated_data_list.append(selected_species_dic[i]+\"_\"+selected_species_dic[k])\n",
    "                    continue\n",
    "                else :\n",
    "                    if k < i: # gene ====> query 1->1 1->2 1->3 2->2 2->3  \n",
    "                        continue\n",
    "                    else :\n",
    "                        print \"Doing the blastp & forward best hit searches between %s genome and %s genome\" % (selected_species_dic[i], selected_species_dic[k])\n",
    "\n",
    "                        queryV_len = len(queryV)\n",
    "                        if cpu_count == 1 :\n",
    "                            blastp_time_start = time.time()\n",
    "                            RunParallelQuery(i, k, queryV, cpu_count)\n",
    "                            blastp_time_end = time.time()\n",
    "                            print \"The blastp & forward best hit searches took %.2f minutes\" % ((blastp_time_end-blastp_time_start)/60)\n",
    "                        else :\n",
    "                            if queryV_len < cpu_count: #If the number of queryV_len is less than cpu_count, Remark will select the number of queryV_len.\n",
    "                                blastp_time_start = time.time()\n",
    "                                parallel_query = DivisionParallelQuery(queryV, 1, queryV_len, queryV_len) # 1 is query_division_value. Because queryV_len / queryV_len(=cpu_count) is 1.\n",
    "                                for m in range(queryV_len):\n",
    "                                    process = multiprocessing.Process(target=RunParallelQuery, args=(i, k, parallel_query[m], m+1))\n",
    "                                    # args( i => species of query , k => species of subject, m+1 => cpu_count ex) 1, 2 ...)\n",
    "                                    process_list.append(process)\n",
    "                                    process.start()\n",
    "                                for n in process_list:\n",
    "                                    n.join()\n",
    "                                blastp_time_end = time.time()\n",
    "                                print \"The blastp & forward best hit searches took %.2f minutes\" % ((blastp_time_end-blastp_time_start)/60)\n",
    "                            else :\n",
    "                                blastp_time_start = time.time()\n",
    "                                query_division_value = queryV_len / cpu_count\n",
    "                                parallel_query = DivisionParallelQuery(queryV, query_division_value, cpu_count, queryV_len)\n",
    "                                for m in range(cpu_count):\n",
    "                                    process = multiprocessing.Process(target=RunParallelQuery, args=(i, k, parallel_query[m], m+1))\n",
    "                                    #args( i => species of query , k => species of subject, m+1 => cpu_count ex) 1, 2 ...)\n",
    "                                    process_list.append(process)\n",
    "                                    process.start()\n",
    "                                for n in process_list:\n",
    "                                    n.join()\n",
    "                                blastp_time_end = time.time()\n",
    "                                print \"The blastp & forward best hit searches took %.2f minutes\" % ((blastp_time_end-blastp_time_start)/60)\n",
    "                        new_calculated_data_list.append(selected_species_dic[i]+\"_\"+selected_species_dic[k])\n",
    "                        if not i == k :\n",
    "                            backward_best_hit_work_list.append((i,k, queryV_len))\n",
    "    return backward_best_hit_work_list\n",
    " \n",
    "def Backward_Best_Hit(args):\n",
    "    species_of_query, species_of_subject, queryV_len = args    \n",
    "    start_time_BBH = time.time()\n",
    "    forward_best_hit_score_list = []\n",
    "    blastp_score_split_list = []    \n",
    "    print \"Start to run the backward best hit between %s genome %s genome\" % (selected_species_dic[species_of_query], selected_species_dic[species_of_subject])\n",
    "    if queryV_len < cpu_count : #If the number of queryV_len is less than cpu_count, the cpu_count is changed to queryV_len.\n",
    "        for parallel_num in range(queryV_len):\n",
    "            parallel_num += 1                                  \n",
    "            with open(Score_file+selected_species_dic[species_of_query]+\"_\"+selected_species_dic[species_of_subject]+\"_\"+\"best_score_S\"+str(threshold_score)+\"_\"+str(parallel_num), \"r\") as best_hit_score:\n",
    "                for each_line in best_hit_score:\n",
    "                    split_each_line = each_line.split(\" \")\n",
    "                    split_each_line[2] = int(split_each_line[2])\n",
    "                    split_each_line[3] = int(split_each_line[3])\n",
    "        #            print split_each_line\n",
    "                    forward_best_hit_score_list.append(split_each_line)\n",
    "            with open(Score_file+selected_species_dic[species_of_query]+\"_\"+selected_species_dic[species_of_subject]+\"_\"+\"blastp_score_split_list_S\"+str(threshold_score)+\"_\"+str(parallel_num), 'r') as blastp_score :\n",
    "                for each_line in blastp_score:\n",
    "                    split_each_line = each_line.split(\" \")\n",
    "                    split_each_line[2] = int(split_each_line[2])\n",
    "                    split_each_line[3] = int(split_each_line[3])\n",
    "        #            print split_each_line\n",
    "                    blastp_score_split_list.append(split_each_line)   \n",
    "    else :            \n",
    "        for parallel_num in range(cpu_count):\n",
    "            parallel_num += 1\n",
    "            with open(Score_file+selected_species_dic[species_of_query]+\"_\"+selected_species_dic[species_of_subject]+\"_\"+\"best_score_S\"+str(threshold_score)+\"_\"+str(parallel_num), \"r\") as best_hit_score:\n",
    "                for each_line in best_hit_score:\n",
    "                    split_each_line = each_line.split(\" \")\n",
    "                    split_each_line[2] = int(split_each_line[2])\n",
    "                    split_each_line[3] = int(split_each_line[3])\n",
    "        #            print split_each_line\n",
    "                    forward_best_hit_score_list.append(split_each_line)\n",
    "            with open(Score_file+selected_species_dic[species_of_query]+\"_\"+selected_species_dic[species_of_subject]+\"_\"+\"blastp_score_split_list_S\"+str(threshold_score)+\"_\"+str(parallel_num), 'r') as blastp_score :\n",
    "                for each_line in blastp_score:\n",
    "                    split_each_line = each_line.split(\" \")\n",
    "                    split_each_line[2] = int(split_each_line[2])\n",
    "                    split_each_line[3] = int(split_each_line[3])\n",
    "        #            print split_each_line\n",
    "                    blastp_score_split_list.append(split_each_line)    \n",
    "                 \n",
    "    bar = Bar(\"Searching : \"+selected_species_dic[species_of_query]+\"-\"+selected_species_dic[species_of_subject], max = len(forward_best_hit_score_list))\n",
    "    \n",
    "    for forward_best_hit_score_element in forward_best_hit_score_list:\n",
    "        matching_list = []        \n",
    "        backward_best_score = ['-1','-1','-1']\n",
    "        bar.next()\n",
    "        for element in blastp_score_split_list:\n",
    "            if element[1] == forward_best_hit_score_element[1]:                \n",
    "                matching_list.append(element) \n",
    "        \n",
    "        for element in matching_list:\n",
    "            if int(element[2]) > int(backward_best_score[2]):\n",
    "                backward_best_score = element\n",
    " #        with open('./'+selected_species_dic[species_of_query]+\"_\"+selected_species_dic[species_of_subject]+'_subtraction'+\"_\"+str(threshold_score), 'a') as subtraction :\n",
    " #            save_data = int(backward_best_score[2]) - int(forward_best_hit_score_element[2])\n",
    " #            subtraction.write(str(save_data)+\"\\n\")\n",
    "                                                                                                                       \n",
    "        if int(backward_best_score[2]) - int(forward_best_hit_score_element[2]) <= threshold_score :           \n",
    "            with open(Score_file+selected_species_dic[species_of_query]+\"_\"+selected_species_dic[species_of_subject]+\"_oneway_threshold_best_hit_S\"+str(threshold_score), \"a\") as other_oneway_threshold_best_hit:\n",
    "                save_data = forward_best_hit_score_element[0]+\" \"+forward_best_hit_score_element[1]+\" \" +str(int(forward_best_hit_score_element[2]))+\"\\n\"#           \n",
    "                other_oneway_threshold_best_hit.write(save_data)\n",
    "                    \n",
    "    bar.finish()\n",
    "    finish_time_BBH = time.time()\n",
    "    RBH_time = float((finish_time_BBH - start_time_BBH)/60)\n",
    "    print  \"BackwardBestHit of %s-%s took %.2f minutes\" % (selected_species_dic[species_of_query], selected_species_dic[species_of_subject], RBH_time )\n",
    "    return RBH_time\n",
    "\n",
    "def Search_Equal_BBH_Data(target_A):\n",
    "    \"\"\"Search the equal backward best hit data. ex) AAE_AAE_backward_best_hit \"\"\"        \n",
    "    put_data = equal_BBH_data_dic[target_A]\n",
    "    if put_data[1] == 0:\n",
    "        pass\n",
    "    else :\n",
    "        copy_put_data = copy.copy(put_data)\n",
    "        copy_put_data.insert(0, target_A)\n",
    "        results.put(copy_put_data)        \n",
    "        equal_BBH_data_dic[target_A][1] = 0\n",
    " #        print \"---put zero in Search_Equal_BBH_Data----- \", equal_BBH_data_dic[target_A]\n",
    "        for i in second_equal_BBH_data:\n",
    "            if i[2] == 0:\n",
    "                pass\n",
    "            else:\n",
    "                if i[0] == target_A or i[1] == target_A: \n",
    "                    copy_second_put_data = copy.copy(i)\n",
    "                    tasks.put(copy_second_put_data)  # Don't put results as queue. Because the tasks will put copy_second_put_data to results as queue.\n",
    "                    i[2] = 0\n",
    " #                    print \"---put zero in second_equal_BBH_data---\", i\n",
    "    return\n",
    "\n",
    "def Search_Unequal_BBH_Data(target_B):\n",
    "    \"\"\"Search the unequal backward best hit data. ex) AAE_CAC_backward_best_hit\"\"\"\n",
    "    for i in unequal_BBH_data:\n",
    "        if i[2] == 0 :\n",
    "            pass\n",
    "        else:\n",
    "            if target_B[0] == i[0] or target_B[0] == i[1] or target_B[1] == i[0] or target_B[1] == i[1]:\n",
    "                copy_i = copy.copy(i)\n",
    "                tasks.put(copy_i)\n",
    "                unequal_BBH_data[unequal_BBH_data.index(i)][2] = 0\n",
    "    return\n",
    "\n",
    "def Matching_BBH(target):\n",
    "    \"\"\" Match the backward best hit \"\"\" \n",
    "    if target[2] == 0 :\n",
    "        return \n",
    "    \n",
    "    else:\n",
    "        copy_target = copy.copy(target)\n",
    "        Search_Equal_BBH_Data(copy_target[0])\n",
    "        Search_Equal_BBH_Data(copy_target[1])\n",
    "        results.put(copy_target)\n",
    "        unequal_BBH_data[unequal_BBH_data.index(target)][2] = 0\n",
    "\n",
    "    for j in unequal_BBH_data:\n",
    "        if j[2] == 0 :\n",
    "            pass\n",
    "        \n",
    "        else:\n",
    "            if copy_target[0]==j[0] or copy_target[0]==j[1] or copy_target[1]==j[0] or copy_target[1]==j[1]:\n",
    "                copy_j = copy.copy(j)\n",
    " #                print \"targ_get , j = %s %s\" % (copy_target, j)\n",
    "                unequal_BBH_data[unequal_BBH_data.index(j)][2] = 0\n",
    "                tasks.put(copy_j)\n",
    "       \n",
    "    while not tasks.empty():              \n",
    "        get_task = tasks.get()\n",
    "        Search_Equal_BBH_Data(get_task[0])\n",
    "        Search_Equal_BBH_Data(get_task[1])\n",
    "        results.put(get_task)\n",
    "        Search_Unequal_BBH_Data(get_task)\n",
    "\n",
    "def Generating_Matrix_Clustering_Ortholog(element_set, bar):  \n",
    "    \"\"\" Generate the matrix of clustering ortholog. \"\"\"\n",
    "    row_data = []\n",
    "    col_data = []\n",
    "    temp_results = Queue.Queue()  \n",
    "    bar.next()\n",
    "    for element in element_set:    \n",
    "        if row_data.count(element[0]) > 0 : # if element[0] exist, returning the index in the row_data.\n",
    "            row = row_data.index(element[0]) # element[0] is data of row. ['gi|15606057|ref|NP_213434.1|', 'gi|15606057|ref|NP_213434.1|', '3823\\n']\n",
    "   \n",
    "        else :\n",
    "            row = len(row_data) \n",
    "            row_data.append(element[0])\n",
    "            if col_data.count(element[0]) < 1: # if element[0] doesn't exist, appending the element[0] to the col_data.\n",
    "                col_data.append(element[0])                    \n",
    "                            \n",
    "        if col_data.count(element[1]) > 0:\n",
    "            col = col_data.index(element[1]) #element[1] is data of col.\n",
    "\n",
    "        else:\n",
    "            col = len(col_data) \n",
    "            col_data.append(element[1])    \n",
    "            if row_data.count(element[1]) < 1:\n",
    "                col_data.append(element[1])\n",
    "                    \n",
    "        temp_results.put([row,col,element[2]])            \n",
    "                     \n",
    "    score_matrix = numpy.matlib.zeros((len(row_data),len(col_data)), dtype=numpy.float) # create a new matrix of given shape(the size_resuls) and type, filled with zeros.              \n",
    "    while not temp_results.empty():\n",
    "        get_temp_results = temp_results.get()\n",
    "        row = get_temp_results[0]\n",
    "        col = get_temp_results[1]\n",
    "        score_matrix[row,col] = get_temp_results[2]\n",
    "        score_matrix[col,row] = get_temp_results[2]\n",
    " \n",
    "    if len(row_data)*len(col_data) > 4 :  # If the elements of row and col is less than 2, it is excluded.\n",
    "        if len(row_data) > 1000 and cpu_count > 1 : #The big size of matrix(bigger than 1000 X 1000) will be computed by Parallel_Matrix_Multiplication_Using_Numpy function.            \n",
    "            score_matrix = Parallel_MCL(score_matrix)\n",
    "        else :            \n",
    "            score_matrix = MCL(score_matrix)\n",
    "        Clustering(row_data, col_data, score_matrix)    \n",
    "   \n",
    "def Parallel_MCL(score_matrix):    \n",
    "    count = 0\n",
    "    infinitesimal_value = 10**-10\n",
    "    idempotent_value = 1   \n",
    "\n",
    "    while idempotent_value > infinitesimal_value: # > infinitesimal_value \n",
    "        MCL_time_start = time.time()        \n",
    "        pool = multiprocessing.Pool(cpu_count) # create a expansion_matrix  \n",
    "        multiplication_results = pool.map(Parallel_Matrix_Multiplication_Using_Numpy, izip(score_matrix,repeat(score_matrix)))\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        \n",
    "        pool = multiprocessing.Pool(cpu_count) # create a inflation_matrix(part 1)   \n",
    "        power_results = pool.map(Parallel_Matrix_Power_Using_Numpy, multiplication_results)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "            \n",
    "        sum_matrix = 0\n",
    "        for i in power_results: \n",
    "            sum_matrix = i + sum_matrix\n",
    "        \n",
    "        pool = multiprocessing.Pool(cpu_count) # create a inflation_matrix(part 2)   \n",
    "        divide_results = pool.map(Parallel_Matrix_Divide_Using_Numpy, izip(power_results, repeat(sum_matrix)))\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "        score_matrix = numpy.concatenate(divide_results)                                \n",
    "\n",
    "        idempotent_value = abs(numpy.sum(score_matrix) - numpy.sum(multiplication_results)) # identify whether inflation_matrix is idempotent matrix or not.\n",
    "\n",
    "        count += 1\n",
    "        if count > infinite_loop : # It will prevent the infinite loop of MCL algorithm.       \n",
    "            break     \n",
    "        MCL_time_finish = time.time()\n",
    "        if verbose :\n",
    "            print \" MCL time : %f, count : %d, matrix size : %d * %d\" % ((MCL_time_finish - MCL_time_start)/60, count, score_matrix[0].size, score_matrix[0].size)                     \n",
    "    return score_matrix    \n",
    "    \n",
    "def MCL(score_matrix):    \n",
    "    count = 0\n",
    "    infinitesimal_value = 10**-10\n",
    "    idempotent_matrix = numpy.matlib.ones((2,2))    \n",
    "    while idempotent_matrix.sum() > infinitesimal_value: # > infinitesimal_value \n",
    "        MCL_time_start = time.time()        \n",
    "        expansion_matrix = score_matrix ** 2      \n",
    "        score_matrix = numpy.power(expansion_matrix, inflation_factor)\n",
    "        score_matrix_sum =  score_matrix.sum(axis = 0)\n",
    "        score_matrix = numpy.divide(score_matrix, score_matrix_sum) # create a inflation_matrix        \n",
    "        idempotent_matrix =abs(score_matrix - expansion_matrix) # identify whether inflation_matrix is idempotent matrix or not.        \n",
    "        count += 1\n",
    "        if count > infinite_loop : # It will prevent the infinite loop of MCL algorithm.       \n",
    "            break     \n",
    "        MCL_time_finish = time.time()\n",
    "        if verbose :\n",
    "            print \" MCL time : %f, count : %d, matrix size : %d * %d\" % ((MCL_time_finish - MCL_time_start)/60, count, score_matrix[0].size, score_matrix[0].size)            \n",
    "    return score_matrix\n",
    "\n",
    "def Clustering(row_data, col_data, score_matrix):\n",
    "    global cluster_count\n",
    "    global ortholog_count\n",
    "    ortholog_temp_list = []            \n",
    "    for i in range(len(row_data)):\n",
    "        ortholog_list = []\n",
    "        ortholog_sum = 0\n",
    "        ortholog = Queue.Queue() # It is Queue which is put the ortholog.\n",
    "        gene_id_queue = Queue.Queue() # It is Queue which is put the ortholog having changed gene ID \n",
    "        for j in range(len(col_data)):\n",
    "            if 0.1 <= score_matrix[i,j]:            \n",
    "                ortholog.put(col_data[j])\n",
    "                ortholog_list.append(col_data[j])\n",
    "                 \n",
    "        if ortholog.qsize() >= 3: # If the ortholog queue  has the element of ortholog more than 2, it will be printed.\n",
    "            for element in ortholog_list:\n",
    "                try :\n",
    "                    ortholog_sum += ortholog_temp_list.index(element)+1\n",
    "                    \n",
    "                except ValueError:                                                                     \n",
    "                    with open(Cluster_out+\"_geneID_S\"+str(threshold_score)+\"_\"+str(inflation_factor), \"a\") as ortholog_list_save:\n",
    "                        ortholog_print = \"cluster \"+str(cluster_count)+\" :\" \n",
    "                        ortholog_list_save.write(ortholog_print)                                                           \n",
    "                              \n",
    "                        while not ortholog.empty():\n",
    "                            get_ortholog = ortholog.get()\n",
    "                            ortholog_list_save.write(\"\\t\"+get_ortholog)\n",
    "                            \n",
    "                            try:\n",
    "                                get_ortholog_split = get_ortholog.split('_') # get_ortholog --> ECO_170082288,  get_ortholog.split('_') --> ['ECO', '170082288']\n",
    "                                gene_id_queue.put(get_ortholog_split[0]+\"_\"+gene_id_dic[get_ortholog_split[1]])                                \n",
    "                            except KeyError:\n",
    "                                gene_id_queue.put(get_ortholog)  # If the gene_id_dic don't have get_ortholog, it will print the original ID(get_ortholog).\n",
    "                        ortholog_list_save.write(\"\\n\")\n",
    "                    with open(Cluster_out+\"_KO_ID_S\"+str(threshold_score)+\"_\"+str(inflation_factor), \"a\") as ortholog_list_geneID_save:                                 \n",
    "                        ortholog_list_geneID_print = \"cluster \"+str(cluster_count)+\" :\" \n",
    "                        ortholog_list_geneID_save.write(ortholog_list_geneID_print)\n",
    "                        while not gene_id_queue.empty():\n",
    "                            ortholog_list_geneID_save.write(\"\\t\"+gene_id_queue.get())\n",
    "                            ortholog_count += 1                                                   \n",
    "                        cluster_count += 1\n",
    "                        ortholog_list_geneID_save.write(\"\\n\")\n",
    "                    break\n",
    "            ortholog_temp_list = operator.concat(ortholog_temp_list, ortholog_list) \n",
    "\n",
    " \n",
    "\n",
    "def Parallel_Matrix_Multiplication_Using_Numpy(data):\n",
    "    matrix_element , matrix = data     \n",
    "    result = matrix_element * matrix   \n",
    "    return result\n",
    "\n",
    "def Parallel_Matrix_Power_Using_Numpy(matrix_element):\n",
    "    power_matrix_element = numpy.power(matrix_element, inflation_factor)\n",
    "    return power_matrix_element\n",
    "\n",
    "def Parallel_Matrix_Divide_Using_Numpy(data):\n",
    "    matrix_element, sum_data = data\n",
    "    return numpy.divide(matrix_element, sum_data)\n",
    "\n",
    "def Read_Species_List(pr=0):\n",
    "    \"\"\" If pr is 1, it will print \"Species_List\". \"\"\" \n",
    "    read_species =glob.glob(command_options.Species+\"*\")   \n",
    "    selected_species_dic = {}\n",
    "    backward_selected_species_dic = {}\n",
    "    for i, species in enumerate(sorted(read_species), start=1):\n",
    "        selected_species_dic[i] = species.split('/')[-1]\n",
    "        backward_selected_species_dic[species.split('/')[-1]] = i \n",
    "        if pr == 1 :\n",
    "            print str(i)+\".\", species.split('/')[-1]\n",
    "        number = i\n",
    "    return selected_species_dic, backward_selected_species_dic, number\n",
    "        \n",
    "def Del_File(path, file):\n",
    "    del_file =subprocess.Popen([\"rm \"+path+file], stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n",
    "    del_file_stream = del_file.communicate()   \n",
    "    if not del_file_stream[1]:\n",
    "        print \"Done to del \"+path+file\n",
    "    elif del_file_stream[1]:\n",
    "        print del_file_stream[1]\n",
    "        \n",
    "def Check_File(file):\n",
    "    file_list = glob.glob(file+'*')    \n",
    "    if (Cluster_out+\"_geneID_S\"+str(threshold_score)+\"_\"+str(inflation_factor) or Cluster_out+\"_KO_ID_S\"+str(threshold_score)+\"_\"+str(inflation_factor)) in file_list:\n",
    "        print \"Please, set other name of output.\"\n",
    "        sys.exit(2)\n",
    "        \n",
    "def Read_Equal_BBH(path):            \n",
    "    with open(path+\"_oneway_threshold_best_hit_S\"+str(threshold_score), 'r') as equal_RBH:                          \n",
    "        for j in equal_RBH:                    \n",
    "            split_data = j.split()\n",
    "            split_data[2] = int(split_data[2])\n",
    "            equal_BBH_data.append(split_data)                   \n",
    "            equal_BBH_data_dic[split_data[0]] = split_data[1:]\n",
    "    try :\n",
    "        with open(path+\"_second_oneway_threshold_best_hit_S\"+str(threshold_score), 'r') as second_equal_RBH:                \n",
    "            for j in second_equal_RBH:\n",
    "                split_data = j.split()\n",
    "                split_data[2] = int(split_data[2])                        \n",
    "                second_equal_BBH_data.append(split_data)       \n",
    "    except : \n",
    "        pass\n",
    "\n",
    "def Read_Unequal_BBH(path):\n",
    "    with open(path+\"_oneway_threshold_best_hit_S\"+str(threshold_score), 'r') as unequal_RBH:\n",
    "        for j in unequal_RBH:\n",
    "            split_data = j.split()\n",
    "            split_data[2] = int(split_data[2])        \n",
    "            unequal_BBH_data.append(split_data)\n",
    "        \n",
    "print \"Blastp = \", command_options.Blastp\n",
    "print \"Blastp_data = \", command_options.Blastp_data\n",
    "print \"blstp_matrix = \", command_options.blastp_matrix\n",
    "print \"cpu_count = \", command_options.cpu_count\n",
    "print \"genomes\", command_options.genomes\n",
    "print \"infinite_loop = \", command_options.infinite_loop\n",
    "print \"inflation_factor = \", command_options.inflation_factor\n",
    "print \"mode : \", command_options.mode\n",
    "print \"Cluster_out : \", command_options.Cluster_out\n",
    "print \"threshold_score : \", command_options.threshold_score\n",
    "print \"save_raw_blastp_score : \", command_options.save_raw_blastp_score\n",
    "print \"Score_file : \", command_options.Score_file\n",
    "print \"Species : \", command_options.Species\n",
    "print \"verbose : \", command_options.verbose\n",
    "print \n",
    " \n",
    "if not sys.argv[1:]:    \n",
    "    print \"1. BLASTP. \\n2. BLASTP using precalculated data. \\n3. Clustering.\\n\"\n",
    "    mode = raw_input(\">> Select a mode or modes (1 or 2 or 1 3 or 2 3): \")    \n",
    "    selected_species_dic, backward_selected_species_dic, number_i = Read_Species_List(pr=1)\n",
    "    \n",
    "    selected_number= raw_input(\">> Select an genomes to detect orthologs(e.g. 1 2 3 4 5 or 1-5) : \")\n",
    "    \n",
    "    if selected_number.find('-') > 0:\n",
    "        SN=selected_number.split(\"-\")\n",
    "        if int(SN[-1]) > number_i:\n",
    "            print \"\\nWrong typing! Try again!\\n\"\n",
    "            sys.exit(2)\n",
    "        else :\n",
    "            user_selected_number=range(int(SN[0]),int(SN[-1])+1)\n",
    "            for j in user_selected_number:\n",
    "                print selected_species_dic[j],\n",
    "            print \"are selected!!\"\n",
    "        \n",
    "    else :        \n",
    "        user_selected_number = sorted(set([int(read_species) for read_species in selected_number.split()]))                      \n",
    "        if int(user_selected_number[-1]) > number_i:\n",
    "            print \"\\nWrong typing! Try again!\\n\"\n",
    "            sys.exit(2)\n",
    "        else :\n",
    "            for j in user_selected_number:\n",
    "                print selected_species_dic[j],\n",
    "            print \"are selected!!\"       \n",
    "        \n",
    "    blastp_matrix = GetMatrixNumber()\n",
    "\n",
    "    cpu_count = input(\"The processors of CPU are detected. You can use %s processors.\\nIf you input >= 2, The Remark will run a parallel computation for the blastp.\\n\" % multiprocessing.cpu_count()\n",
    "                          + \"Enter the number of process to use in this program (1 ~ %s): \" % multiprocessing.cpu_count())\n",
    "    if \"3\" in mode :\n",
    "        inflation_factor = input(\"Enter the inflation factor to cluster: \")\n",
    "        Cluster_out = raw_input(\"Set the name of clustering output : \")\n",
    "        \n",
    "    \n",
    "elif sys.argv[1:] :\n",
    "    genomes = command_options.genomes        \n",
    "    mode = command_options.mode\n",
    "    cpu_count = command_options.cpu_count\n",
    "    blastp_matrix = command_options.blastp_matrix   \n",
    "    inflation_factor = command_options.inflation_factor\n",
    "    selected_species_dic, backward_selected_species_dic, number_i = Read_Species_List()\n",
    "    user_selected_number = [backward_selected_species_dic[ele] for ele in genomes]\n",
    "    Cluster_out = command_options.Cluster_out   \n",
    "    \n",
    "\n",
    "Species = command_options.Species\n",
    "Blastp = command_options.Blastp\n",
    "Score_file = command_options.Score_file\n",
    "Blastp_data = command_options.Blastp_data\n",
    "save_raw_blastp_score = command_options.save_raw_blastp_score\n",
    "threshold_score = command_options.threshold_score\n",
    "verbose = command_options.verbose\n",
    "infinite_loop = command_options.infinite_loop\n",
    "\n",
    "if \"3\" in mode :\n",
    "    Check_File(Cluster_out)\n",
    "    \n",
    "#Del_File(Score_file, \"*\")\n",
    "   \n",
    "if \"3\" in mode :\n",
    "    Log_file_name = Cluster_out+\"_S\"+str(threshold_score)+\"_\"+str(inflation_factor)+\".log\"\n",
    "elif not \"3\" in mode :\n",
    "    \"if onl;y mode 1 or 2 Passed\"\n",
    "    Log_file_name = 'Log'   \n",
    "\n",
    "with open(Log_file_name, 'w') as log:\n",
    "    \n",
    "        log.write(str(datetime.datetime.now()))\n",
    "        log.write(\"\\nmode :\")\n",
    "        for i in mode :            \n",
    "            log.write(\" \"+i)\n",
    "        log.write(\"\\ngenomes : \")\n",
    "        for i in user_selected_number :\n",
    "            log.write(selected_species_dic[i]+\" \")    \n",
    "        log.write(\"\\ncpu_count : \"+str(cpu_count))\n",
    "        log.write(\"\\nblastp matrix : \"+blastp_matrix)\n",
    "        if \"3\" in mode :\n",
    "            log.write(\"\\ninflation_factor : \"+str(inflation_factor))\n",
    "            log.write(\"\\nCluster out : \"+Cluster_out)\n",
    "        log.write(\"\\nSpecies : \"+Species)\n",
    "        log.write(\"\\nBlastp : \"+Blastp)\n",
    "        log.write(\"\\nScore file : \"+Score_file)  \n",
    "        log.write(\"\\nBlastp_data : \"+Blastp_data)\n",
    "        log.write(\"\\nsave rawblastp score : \"+str(save_raw_blastp_score))\n",
    "        log.write(\"\\n\")   \n",
    "    \n",
    "start_time_OBH = time.time()   \n",
    "if \"1\" in mode :            \n",
    "    backward_best_hit_work_list = Oneway_Threshold_Best_Hit(mode)    \n",
    "    pool = multiprocessing.Pool(cpu_count)    \n",
    "    results = pool.map(Backward_Best_Hit,backward_best_hit_work_list)\n",
    "    pool.close()\n",
    "    pool.join()   \n",
    "    \n",
    "elif \"2\" in mode:\n",
    "    used_precalculated_data_list=[]\n",
    "    new_calculated_data_list=[]\n",
    "    precalculated_data_list = glob.glob(Blastp_data+\"*oneway_threshold_best_hit_S\"+str(threshold_score))\n",
    "    print precalculated_data_list\n",
    "    backward_best_hit_work_list = Oneway_Threshold_Best_Hit(mode)\n",
    "    if not backward_best_hit_work_list == []: # If backward_best_hit_work_list is an empty list, pool instance can't finsh the work.                \n",
    "        pool = multiprocessing.Pool(cpu_count)\n",
    "        results = pool.map(Backward_Best_Hit,backward_best_hit_work_list)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "    else : results = [0,0]\n",
    "    \n",
    "Del_File(\"./\", \"query*\")\n",
    "finish_time_OBH = time.time()\n",
    "blastp_time_log = float(((finish_time_OBH - start_time_OBH)/60))\n",
    "print  \"BLASTP searches + forward best Hit + backwardbest hit took %f minutes\" % blastp_time_log\n",
    "with open(Log_file_name, 'a') as log:\n",
    "    log.write(\"Backward_Best_Hit took \"+str(max(results))+\" minutes\\n\")\n",
    "    log.write(\"BLASTP + Best_Hit + backward_best_hit searches took \"+str(blastp_time_log)+\" minutes\\n\")\n",
    "          \n",
    "if \"3\" in mode :\n",
    "    start_time_clustering = time.time()\n",
    "    ##########################################################################################\n",
    "    #generate matrix and calculate the matrix using MCL algorithm and cluster the ortholog.\"\"\"    \n",
    "    print \"\\n>>>> Start MCL algorithm and Clustering ortholog <<<<\"    \n",
    "    equal_BBH_data = []\n",
    "    unequal_BBH_data = []\n",
    "    equal_BBH_data_dic ={}\n",
    "    second_equal_BBH_data = []\n",
    "    results = Queue.Queue()\n",
    "    tasks = Queue.Queue()\n",
    "    cluster_count = 1\n",
    "    ortholog_count = 0\n",
    "    gene_id_dic = {}    \n",
    "      \n",
    "    with open(\"myva=gb\", \"r\") as id_read:\n",
    "        for i in id_read:\n",
    "            gene_name, gene_id=i.split()\n",
    "            gene_id_dic[gene_id.replace(\"\\n\",\"\")] = gene_name # remove \"\\n\"\n",
    "    \n",
    "    if \"1\" in mode :\n",
    "        for i in user_selected_number:\n",
    "            for k in user_selected_number:       \n",
    "                if k < i :\n",
    "                    pass\n",
    "                elif i == k :\n",
    "                    Read_Equal_BBH(Score_file+selected_species_dic[i]+\"_\"+selected_species_dic[k])\n",
    "                elif i != k :                              \n",
    "                    Read_Unequal_BBH(Score_file+selected_species_dic[i]+\"_\"+selected_species_dic[k])\n",
    "                            \n",
    "    elif \"2\" in mode :   \n",
    "        for used_data in used_precalculated_data_list :\n",
    "            first, second = used_data.split(\"_\")\n",
    "            if first == second :\n",
    "                Read_Equal_BBH(Blastp_data+used_data)                           \n",
    "            elif first != second :\n",
    "                Read_Unequal_BBH(Blastp_data+used_data)   \n",
    "                \n",
    "        for new_data in new_calculated_data_list :\n",
    "            first, second = new_data.split(\"_\")\n",
    "            if first == second :\n",
    "                Read_Equal_BBH(Score_file+new_data)\n",
    "            elif first != second :\n",
    "                Read_Unequal_BBH(Score_file+new_data)          \n",
    "    \n",
    "    matched_BBH_data = []\n",
    "    matched_BBH_element_data_set = []\n",
    "    \n",
    "    for unequal_RBH_element in unequal_BBH_data:   \n",
    "        Matching_BBH(unequal_RBH_element)  \n",
    "        temp_results_list = []\n",
    "        if results._qsize()  != 0: # return the number of results as Queue.      \n",
    "            while not results.empty():\n",
    "                get_results = results.get()\n",
    "                temp_results_list.append(get_results)\n",
    "            matched_BBH_data.append(temp_results_list)\n",
    "        \n",
    "    bar = Bar(\"processing \", max = len(matched_BBH_data))\n",
    "    for data in matched_BBH_data :\n",
    "        Generating_Matrix_Clustering_Ortholog(data, bar) \n",
    "    bar.finish()\n",
    "    finish_time_clustering = time.time()\n",
    "    mcl_time_log = float((finish_time_clustering - start_time_clustering)/60)\n",
    "    remark_time_log = float((finish_time_clustering - start_time_OBH)/60)\n",
    "    print \"MCL algorithm and Ortholog Clustering took %.2f minutes\" % mcl_time_log\n",
    "    print \"owPRemark program took %.2f minutes\" % remark_time_log\n",
    "    \n",
    "    if \"3\" in mode :\n",
    "        with open(Log_file_name, 'a') as log:\n",
    "            log.write(\"Ortholog count : \"+str(ortholog_count)+\",\"+\" Cluster count : \"+str(cluster_count-1)+\"\\n\")    \n",
    "            log.write(\"MCL algorithm and Ortholog Clustering took \"+str(mcl_time_log)+\" minutes\\n\")\n",
    "            log.write(\"Remark program took \"+str(remark_time_log)+ \"minutes\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "log_time = datetime.datetime.now().strftime(\"_%Y_%m_%d_%H_%M_%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_2020_03_30_15_12_40_2020_03_30_15_12_40'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_time+log_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "nbTranslate": {
   "displayLangs": [
    "ko"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "ko",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "209.325px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
